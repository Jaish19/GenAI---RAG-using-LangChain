{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jAwdPxXLYzRx"
      },
      "outputs": [],
      "source": [
        "# https://openrouter.ai/settings/keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "3WlsNSphZdsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bd8248b2-094a-459c-c63f-a9b1a40af5aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain_community)\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-classic, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.80\n",
            "    Uninstalling langchain-core-0.3.80:\n",
            "      Successfully uninstalled langchain-core-0.3.80\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-core-1.1.0 langchain-text-splitters-1.0.0 langchain_community-0.4.1 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "yfkUbrGAbC50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9a91f948-6ff8-4cee-ac31-68cf84a7257f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.4.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: langchain-community<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.11.10)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.4.0-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-experimental\n",
            "Successfully installed langchain-experimental-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "yH1N0y8RbSmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "69fa0cad-2258-47f4-af44-ce2af9de0b67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "4v0uipM7b5lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "495eabb8-e879-41e4-c469-27e2b2ee3a2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jaish19/GenAI---RAG-using-LangChain.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIe8qK4kFlmm",
        "outputId": "e51ada65-5903-4877-a606-4cd3599a92f4",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GenAI---RAG-using-LangChain'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 118 (delta 55), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (118/118), 8.89 MiB | 12.78 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic QA LLM"
      ],
      "metadata": {
        "id": "o476UpnXfVf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_openai"
      ],
      "metadata": {
        "id": "ELAEd29rbyiO",
        "outputId": "df55c871-bf9e-4f35-c38a-7016b94a31de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\") # Replace with your actual OpenRouter API key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Step 2: Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"mistralai/mistral-7b-instruct\",  # or try \"meta-llama/llama-3-8b-instruct\", etc.\n",
        "    temperature=0.7,\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Step 3: Interact with the model (normal chat mode)\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=query)]) # Changed from llm(...) to llm.invoke(...)\n",
        "    print(\"\\nAssistant:\", response.content)\n"
      ],
      "metadata": {
        "id": "ZHYUOORrfR4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "346c73a3-6200-4e65-a71a-296b4a5d16cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangChain - Prompt + LLM"
      ],
      "metadata": {
        "id": "6JwT1rqifIzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ-m04AUaaJe",
        "outputId": "86d89c29-8df8-4cdc-bb58-3e846df3349c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (4.15.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Downloading langchain-1.1.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.1.0 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-huggingface"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaqoHkb4eyoS",
        "outputId": "249dab4b-b2ef-4117-f64f-44821c835318"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.1.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.4.43)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (2.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.11.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-1.1.0-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_classic.chains import LLMChain\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\")  # Replace with your actual OpenRouter API key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Step 2: Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"mistralai/mistral-7b-instruct\",  # or \"meta-llama/llama-3-8b-instruct\"\n",
        "    temperature=0.7,\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Step 3: Define a prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Generate a creative and detailed response for the following request:\\n\\n{user_input}\"\n",
        ")\n",
        "\n",
        "# Step 4: Create an LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Step 5: Interact with the model\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = chain.invoke(input=query)\n",
        "    print(\"\\nAssistant:\", response)\n"
      ],
      "metadata": {
        "id": "-X-s7S_afHa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "72260ad4-208c-46c1-f384-45a4ca1316ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You: What is deep learning\n",
            "\n",
            "Assistant: {'user_input': 'What is deep learning', 'text': ' '}\n",
            "\n",
            "You: List the asian countries\n",
            "\n",
            "Assistant: {'user_input': 'List the asian countries', 'text': \" Here is a list of Asian countries, categorized by subregions for better understanding:\\n\\n1. **Central Asia** (5 countries)\\n   - Kazakhstan\\n   - Kyrgyzstan\\n   - Tajikistan\\n   - Turkmenistan\\n   - Uzbekistan\\n\\n2. **East Asia** (6 countries)\\n   - China\\n   - Japan\\n   - Mongolia\\n   - North Korea\\n   - South Korea\\n   - Taiwan (claimed by China, but de facto independent)\\n\\n3. **South Asia** (7 countries)\\n   - Afghanistan\\n   - Bangladesh\\n   - Bhutan\\n   - India\\n   - Maldives\\n   - Nepal\\n   - Pakistan\\n   - Sri Lanka\\n\\n4. **Southeast Asia** (11 countries)\\n   - Brunei\\n   - Cambodia\\n   - Indonesia\\n   - Laos\\n   - Malaysia\\n   - Myanmar (Burma)\\n   - Philippines\\n   - Singapore\\n   - Thailand\\n   - Timor-Leste (East Timor)\\n   - Vietnam\\n\\n5. **West Asia (Middle East)** (8 countries)\\n   - Armenia\\n   - Azerbaijan (transcontinental, but mostly in Asia)\\n   - Bahrain\\n   - Cyprus (geographically in Asia, but culturally European)\\n   - Georgia (transcontinental, but mostly in Asia)\\n   - Iran\\n   - Iraq\\n   - Israel\\n   - Jordan\\n   - Kuwait\\n   - Lebanon\\n   - Oman\\n   - Palestine (State of Palestine, partially recognized)\\n   - Qatar\\n   - Saudi Arabia\\n   - Syria\\n   - Turkey (transcontinental, but mostly in Asia)\\n   - United Arab Emirates\\n   - Yemen\\n\\n### Notes:\\n- Some countries like Russia, Turkey, and Georgia are transcontinental, but the majority of their land is in Asia.\\n- Taiwan's status is politically sensitive and disputed.\\n- The borders and recognition of some countries (e.g., Palestine, Cyprus) vary depending on political perspectives.\\n\\nWould you like any additional details or a different format?\"}\n",
            "\n",
            "You: What is Indian National Anthem\n",
            "\n",
            "Assistant: {'user_input': 'What is Indian National Anthem', 'text': ' Title: A Melodious Ode: The Indian National Anthem, \"Jana Gana Mana\"\\n\\nIn the vast and diverse landscape of India, the national anthem, \"Jana Gana Mana,\" serves as a melodious ode to the nation\\'s spirit, heritage, and unity. Composed by Rabindranath Tagore, a distinguished Nobel laureate, the anthem is a testament to the rich cultural heritage that permeates through the very core of India.\\n\\nThe anthem, first sung on December 27, 1911, during the opening of the Indian National Congress session at Calcutta (now Kolkata), was officially adopted as the national anthem of India on January 24, 1950, following India\\'s independence.\\n\\n\"Jana Gana Mana\" is sung in Bengali, one of the official languages of India, with a Sanskrit sloka embedded within the first stanza. The anthem is set to music by composer C.V. Ramanathan, and its melody resonates with a blend of Indian classical and Western harmonies, reflecting the unique cultural fusion that is India.\\n\\nThe lyrics of \"Jana Gana Mana\" are both profound and inspirational, calling upon the spirit of the nation to awaken and guide the people of India towards unity, peace, and progress. The anthem is a powerful reminder of India\\'s glorious past, present, and future, and its resonating lyrics continue to inspire millions of Indians every day:\\n\\n1. (Bengali)\\nJana-gana-mana, adhinayak jaya he,\\nBhaja phoebus sona asitam payo jana-gana mana,\\nAdhinayak jaya he, nabin samaya kabira,\\nJaya he!\\n\\n2. (English)\\nThou art the ruler of the minds of all people,\\nDispenser of India\\'s destiny.\\nThou art the well-wisher of the deprived and the distressed,\\nThou art their hope, and their strength, worthy to be invoked.\\nMay thy name always be remembered for the benefits thou hast done to them.\\n\\n3. (Sanskrit)\\nBharat bhagya vidhata,\\nPunjab Sindhu Gujarat Maratha,\\nDravida Utkala Banga,\\nVindhya Himachala Yamuna Ganga,\\nUchchhal Jaladhi Taranga,\\nTava shubha name jage,\\nTava shubha asishka jage,\\nGahe tava jaya gatha.\\nJana-gana mana, adhinayaka jaya he,\\nBharata Bhagya Vidhata.\\nPunjab Sindhu Gujarat Maratha,\\nDravida Utkala Banga,\\nVindhya Himachala Yamuna Ganga,\\nUchchhal Jaladhi Taranga,\\nTava shubha name jage,\\nTava shubha asishka jage,\\nGahe tava jaya gatha.\\n\\n4. (English)\\nThou art the creator of the destiny of India,\\nThou art her protector, maker, preserver.\\nPunjab, Sind, Gujarat, Maratha,\\nDravida, Utkala, Banga,\\nVindhya, Himachala, Yamuna, Ganga,\\nSaraswati, Sindhu, Kaveri,\\nO mighty named rivers,\\nO mighty mountain ranges,\\nO mighty peaks,\\nMay the glorious name of India awaken!\\nMay the auspicious words about India spread far and wide!\\nSing the glory of the name India!\\n\\nThe Indian national anthem, \"Jana Gana Mana,\" is a powerful and poignant reminder of India\\'s shared history, diversity, and unity. Its lyrics continue to inspire and unite the people of India, reminding them of their shared heritage and future as one nation, bound by a common spirit and destiny.'}\n",
            "\n",
            "You: Who's the poet of Silapathikaram\n",
            "\n",
            "Assistant: {'user_input': \"Who's the poet of Silapathikaram\", 'text': ' The Silapathikaram is an ancient Tamil epic poem from South India, dating back to the Sangam period (approximately 3rd century BCE to 4th century CE). The poem is traditionally attributed to Ilango Adigal, who is believed to have been a court poet in the kingdom of Chera.\\n\\nIlango Adigal is not as widely known outside of the Tamil cultural sphere as poets like Homer, Dante, or Shakespeare, but his work, Silapathikaram, is considered a masterpiece of Tamil literature. The epic tells the story of Kovalan and Madhavi, a tragic love tale that is filled with themes of betrayal, love, and redemption.\\n\\nThe Silapathikaram is significant for its poetic style, particularly its use of Kuttuvilai, a complex metrical form that is both rhythmic and melodious. The poem is also renowned for its vivid descriptions and its exploration of human emotions, making it a timeless work that continues to captivate readers and scholars.\\n\\nSo, Ilango Adigal, the poet of Silapathikaram, is a crucial figure in the history of Tamil literature and a testament to the richness and depth of this ancient culture.'}\n",
            "\n",
            "You: How big the Indian Ocean\n",
            "\n",
            "Assistant: {'user_input': 'How big the Indian Ocean', 'text': \" The Indian Ocean is truly a marvel of the natural world, spanning an impressive area of approximately 70,560,000 square kilometers (27,240,000 square miles). It covers about 20% of the total water on Earth and is the third largest of the world's oceanic divisions, following the Pacific Ocean and the Atlantic Ocean.\\n\\nThe Indian Ocean is bordered by the continents of Africa, Asia, and Australia, making it the only tropical ocean entirely in the Southern Hemisphere. It stretches from the Gulf of Aden and the Arabian Sea in the north, to the Antarctic in the south, and from the Indian subcontinent, Southeast Asia, and Australia in the east, to Africa in the west.\\n\\nThe Indian Ocean is home to a rich and diverse ecosystem. It is the only ocean that has a warm-water upwelling system, the Indian Ocean Dipole, which significantly influences the climate patterns of the surrounding continents. This ocean is also home to a vast array of marine life, including some of the world's most endangered species such as the Leatherback sea turtle, the Dugong, and the Whale Shark, as well as numerous coral reefs that provide a critical habitat for thousands of other marine organisms.\\n\\nThe Indian Ocean's size, biodiversity, and strategic importance for global commerce make it a vital and fascinating part of our planet. Its depths remain largely unexplored, offering scientists and explorers opportunities for new discoveries and insights into our world's oceans. The Indian Ocean, in its vastness and complexity, truly encapsulates the beauty and mystery of the Earth's oceans.\"}\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG + LangChain"
      ],
      "metadata": {
        "id": "WZxpiXsdfQM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_classic.chains.retrieval_qa import RetrievalQA # Specific import path\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains import LLMChain  # Specific import path\n",
        "from langchain_classic.chains.combine_documents.stuff import StuffDocumentsChain # Specific import path\n",
        "from langchain_core.prompts import PromptTemplate  # To provide the equipped prompt\n",
        "from langchain_community.document_loaders import PDFPlumberLoader # To read the pdf\n",
        "from langchain_experimental.text_splitter import SemanticChunker # Text chunks\n",
        "from langchain_community.vectorstores import FAISS  # Vector DB\n",
        "from google.colab import userdata\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings # Updated import\n",
        "\n",
        "# Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\")\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"  # Endpoint of the openRouter\n",
        "\n",
        "# Load and process PDF\n",
        "loader = PDFPlumberLoader(\"/content/GenAI---RAG-using-LangChain/neural_network.pdf\")\n",
        "docs = loader.load()\n",
        "print(\"Pages loaded:\", len(docs))\n",
        "\n",
        "\n",
        "# PROCESS 1: CHUNK process\n",
        "# Semantic splitter with custom\n",
        "# With SemanticChunker (in LangChain), you don't define chunk_size directly like in other splitters\n",
        "# (CharacterTextSplitter, RecursiveCharacterTextSplitter, etc.).\n",
        "# Instead, semantic chunking decides where to split based on semantic similarity between segments, not on fixed sizes.\n",
        "splitter = SemanticChunker(\n",
        "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"), # Explicitly pass model_name\n",
        "    breakpoint_threshold_type=\"percentile\", # or standard_deviation\n",
        "    breakpoint_threshold_amount=90, # Value for the threshold (e.g. 95 = 95th percentile distance between embeddings)\n",
        "    buffer_size=1 # (Optional) Adds context from neighboring chunks\n",
        ")\n",
        "\n",
        "# Split document\n",
        "chunks = splitter.split_documents(docs)\n",
        "for chunk in chunks:\n",
        "    print(chunk.page_content)\n",
        "\n",
        "\n",
        "# PROCESS 2: Vector DB\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # Explicitly pass model_name\n",
        "vector = FAISS.from_documents(chunks, embedder)\n",
        "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "'''\n",
        "# Try with some queries for similarity search\n",
        "results = retriever.get_relevant_documents(\"What is the remedy for cold and flu?\")\n",
        "\n",
        "for doc in results:\n",
        "    print(doc.page_content)\n",
        "'''\n",
        "\n",
        "# PROCESS 3: LLM MODEL CHOOSING\n",
        "# Use ChatOpenAI with OpenRouter backend\n",
        "llm = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3-8b-instruct\",   #\"mistralai/mistral-7b-instruct\"\n",
        "    temperature=1.3, # It's a less temp -> so it'll give you some relative information about the content.\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        ")\n",
        "\n",
        "# PROCESS 4: PROMPT CREATION\n",
        "prompt = \"\"\"\n",
        "You are a helpful assistant.\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Answer only using the context and be concise (3–4 sentences).\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "\n",
        "# PROCESS 5: LANGCHAIN CREATION - LLM & PROMPT\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=True)\n",
        "\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "\n",
        "# DOCUMENT CHAIN PROCESS\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        "    callbacks=None,\n",
        ")\n",
        "\n",
        "# PROCESS 6: WRAPPING ALL TOGETHER ALONG WITH RETRIEVER\n",
        "qa = create_retrieval_chain(\n",
        "    combine_docs_chain=combine_documents_chain,\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Example query\n",
        "# result = qa(\"What is the remedy for cold and flu?\")\n",
        "# print(\"Answer:\", result[\"result\"])\n"
      ],
      "metadata": {
        "id": "6DWOwM-JY03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3f5a605c-97f5-4b0e-c572-cb309fdf10f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "meanswecanomitthenet.large_weight_initializer()callabove:\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Plottingtheresults23,weobtain:\n",
            "Inbothcases,weendupwithaclassificationaccuracysomewhatover96percent.\n",
            "Thefinal\n",
            "classificationaccuracyisalmostexactlythesameinthetwocases. Butthenewinitialization\n",
            "techniquebringsustheremuch,muchfaster. Attheendofthefirstepochoftrainingthe\n",
            "oldapproachtoweightinitializationhasaclassificationaccuracyunder87percent,while\n",
            "thenewapproachisalreadyalmost93percent. Whatappearstobegoingonisthatour\n",
            "newapproachtoweightinitializationstartsusoffinamuchbetterregime,whichletsusget\n",
            "goodresultsmuchmorequickly. Thesamephenomenonisalsoseenifweplotresultswith\n",
            "100hiddenneurons:\n",
            "23Theprogramusedtogeneratethisandthenextgraphisweight_initialization.py. \n",
            "(cid:12)\n",
            "98 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "3\n",
            "Inthiscase,thetwocurvesdon’tquitemeet. However,myexperimentssuggestthatwithjust\n",
            "afewmoreepochsoftraining(notshown)theaccuraciesbecomealmostexactlythesame. Soonthebasisoftheseexperimentsitlooksasthoughtheimprovedweightinitialization\n",
            "onlyspeedsuplearning,itdoesn’tchangethefinalperformanceofournetworks. However,in\n",
            "Chapter4we’llseeexamplesofneuralnetworkswherethelong-runbehaviourissignificantly\n",
            "betterwiththe1/ n weightinitialization. Thusit’snotonlythespeedoflearningwhichis\n",
            "(cid:112) in\n",
            "improved,it’ssometimesalsothefinalperformance. The1/ n approachtoweightinitializationhelpsimprovethewayourneuralnets\n",
            "(cid:112) in\n",
            "learn. Othertechniquesforweightinitializationhavealsobeenproposed,manybuildingon\n",
            "thisbasicidea. Iwon’treviewtheotherapproacheshere,since1/ n workswellenoughfor\n",
            "(cid:112) in\n",
            "ourpurposes.\n",
            "Ifyou’reinterestedinlookingfurther,Irecommendlookingatthediscussion\n",
            "onpages14and15ofa2012paperbyYoshuaBengio24,aswellasthereferencestherein. Problem\n",
            "ConnectingregularizationandtheimprovedmethodofweightinitializationL2\n",
            "• regularizationsometimesautomaticallygivesussomethingsimilartothenewap-\n",
            "proachtoweightinitialization. Supposeweareusingtheoldapproachtoweight\n",
            "initialization. Sketchaheuristicargumentthat: (1)supposingλisnottoosmall,\n",
            "thefirstepochsoftrainingwillbedominatedalmostentirelybyweightdecay;(2)\n",
            "providedηλ ntheweightswilldecaybyafactorofexp( ηλ/m)perepoch;and\n",
            "(3)supposing(cid:28)λisnottoolarge,theweightdecaywilltailo−ffwhentheweightsare\n",
            "downtoasizearound1/ n ,wherenisthetotalnumberofweightsinthenetwork. (cid:112) in\n",
            "Arguethattheseconditionsareallsatisfiedintheexamplesgraphedinthissection. 3.4 Handwriting recognition revisited: the code\n",
            "Let’simplementtheideaswe’vediscussedinthischapter. We’lldevelopanewprogram,\n",
            "network2.py,whichisanimprovedversionoftheprogramnetwork.pywedevelopedin\n",
            "Chapter1.\n",
            "Ifyouhaven’tlookedatnetwork.pyinawhilethenyoumayfindithelpfulto\n",
            "spendafewminutesquicklyreadingovertheearlierdiscussion. It’sonly74linesofcode,\n",
            "andiseasilyunderstood. 24PracticalRecommendationsforGradient-BasedTrainingofDeepArchitectures,byYoshuaBengio\n",
            "(2012). \n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 99\n",
            "(cid:12)\n",
            "Aswasthecaseinnetwork.py,thestarofnetwork2.pyistheNetworkclass,which\n",
            "weusetorepresentourneuralnetworks. WeinitializeaninstanceofNetworkwithalistof\n",
            "sizesfortherespectivelayersinthenetwork,andachoiceforthecosttouse,defaultingto\n",
            "thecross-entropy:\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "self.num_layers = len(sizes) 3\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "Thefirstcoupleoflinesofthe__init__methodarethesameasinnetwork.py,andare\n",
            "prettyself-explanatory. Butthenexttwolinesarenew,andweneedtounderstandwhat\n",
            "they’redoingindetail.\n",
            "Let’sstartbyexaminingthedefault_weight_initializermethod. Thismakesuse\n",
            "ofournewandimprovedapproachtoweightinitialization. Aswe’veseen,inthatapproach\n",
            "theweightsinputtoaneuronareinitializedasGaussianrandomvariableswithmean0and\n",
            "standarddeviation1dividedbythesquarerootofthenumberofconnectionsinputtothe\n",
            "neuron. Alsointhismethodwe’llinitializethebiases,usingGaussianrandomvariableswith\n",
            "mean0andstandarddeviation1. Here’sthecode:\n",
            "def default_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "Tounderstandthecode,itmayhelptorecallthatnpistheNumpylibraryfordoinglinear\n",
            "algebra. We’llimportNumpyatthebeginningofourprogram. Also,noticethatwedon’t\n",
            "initialize any biases for the first layer of neurons. We avoid doing this because the first\n",
            "layerisaninputlayer,andsoanybiaseswouldnotbeused. Wedidexactlythesamething\n",
            "innetwork.py. Complementingthedefault_weight_initializerwe’llalsoincludea\n",
            "large_weight_initializermethod. Thismethodinitializestheweightsandbiasesusing\n",
            "the old approach from Chapter 1, with both weights and biases initialized as Gaussian\n",
            "randomvariableswithmean0andstandarddeviation1. Thecodeis,ofcourse,onlyatiny\n",
            "bitdifferentfromthedefault_weight_initializer:\n",
            "def large_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self. sizes[1:])]\n",
            "I’veincludedthelarge_weight_initializermethodmostlyasaconveniencetomake\n",
            "iteasiertocomparetheresultsinthischaptertothoseinChapter1. Ican’tthinkofmany\n",
            "practicalsituationswhereIwouldrecommendusingit!\n",
            "ThesecondnewthinginNetwork’s__init__methodisthatwenowinitializeacost\n",
            "attribute. Tounderstandhowthatworks, let’slookattheclassweusetorepresentthe\n",
            "cross-entropycost25:\n",
            "class CrossEntropyCost(object):\n",
            "25Ifyou’renotfamiliarwithPython’sstaticmethodsyoucanignorethe@staticmethoddecorators,\n",
            "andjusttreatfnanddeltaasordinarymethods.Ifyou’recuriousaboutdetails,all@staticmethod\n",
            "doesistellthePythoninterpreterthatthemethodwhichfollowsdoesn’tdependontheobjectinany\n",
            "way.That’swhyselfisn’tpassedasaparametertothefnanddeltamethods. \n",
            "(cid:12)\n",
            "100 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y)\n",
            "Let’sbreakthisdown. Thefirstthingtoobserveisthateventhoughthecross-entropyis,\n",
            "3\n",
            "mathematicallyspeaking,afunction,we’veimplementeditasaPythonclass,notaPython\n",
            "function. WhyhaveImadethatchoice? Thereasonisthatthecostplaystwodifferentroles\n",
            "inournetwork. Theobviousroleisthatit’sameasureofhowwellanoutputactivation,a,\n",
            "matchesthedesiredoutput,y. ThisroleiscapturedbytheCrossEntropyCost.fnmethod. (Note,bytheway,thatthenp.nan_to_numcallinsideCrossEntropyCost.fnensuresthat\n",
            "Numpydealscorrectlywiththelogofnumbersveryclosetozero.) Butthere’salsoasecond\n",
            "waythecostfunctionentersournetwork. RecallfromChapter2thatwhenrunningthe\n",
            "backpropagationalgorithmweneedtocomputethenetwork’soutputerror,δL. Theformof\n",
            "theoutputerrordependsonthechoiceofcostfunction: differentcostfunction,different\n",
            "formfortheoutputerror. Forthecross-entropytheoutputerroris,aswesawinEquation\n",
            "(3.12),\n",
            "δL =aL y. (3.44)\n",
            "−\n",
            "Forthisreasonwedefineasecondmethod,CrossEntropyCost.delta,whosepurposeisto\n",
            "tellournetworkhowtocomputetheoutputerror. Andthenwebundlethesetwomethodsup\n",
            "intoasingleclasscontainingeverythingournetworksneedtoknowaboutthecostfunction. In a similar way, network2.py also contains a class to represent the quadratic cost\n",
            "function. ThisisincludedforcomparisonwiththeresultsofChapter1,sincegoingforward\n",
            "we’llmostlyusethecrossentropy.\n",
            "Thecodeisjustbelow. TheQuadraticCost.fnmethod\n",
            "isastraightforwardcomputationofthequadraticcostassociatedtotheactualoutput,a,\n",
            "andthedesiredoutput,y. ThevaluereturnedbyQuadraticCost.deltaisbasedonthe\n",
            "expression(2.8)fortheoutputerrorforthequadraticcost,whichwederivedbackinChapter\n",
            "2. class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "We’venowunderstoodthemaindifferencesbetweennetwork2.pyandnetwork.py. It’s\n",
            "all pretty simple stuff.\n",
            "There are a number of smaller changes, which I’ll discuss below,\n",
            "includingtheimplementationofL2regularization. Beforegettingtothat,let’slookatthe\n",
            "completecodefornetwork2.py. Youdon’tneedtoreadallthecodeindetail,butitisworth\n",
            "understandingthebroadstructure,andinparticularreadingthedocumentationstrings,so\n",
            "youunderstandwhateachpieceoftheprogramisdoing. Ofcourse,you’realsowelcometo\n",
            "delveasdeeplyasyouwish!\n",
            "Ifyougetlost,youmaywishtocontinuereadingtheprose\n",
            "below,andreturntothecodelater. Anyway,here’sthecode:\n",
            "\"\"\"network2.py\n",
            "~~~~~~~~~~~~~~\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 101\n",
            "(cid:12)\n",
            "An improved version of network.py, implementing the stochastic\n",
            "gradient descent learning algorithm for a feedforward neural network. Improvements include the addition of the cross-entropy cost function,\n",
            "regularization, and better initialization of network weights.\n",
            "Note\n",
            "that I have focused on making the code simple, easily readable, and\n",
            "easily modifiable.\n",
            "It is not optimized, and omits many desirable\n",
            "features. \"\"\"\n",
            "#### Libraries 3\n",
            "# Standard library\n",
            "import json\n",
            "import random\n",
            "import sys\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "#### Define the quadratic and cross-entropy cost functions\n",
            "class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output ‘‘y‘‘. \"\"\"\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer.\"\"\"\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "class CrossEntropyCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output\n",
            "‘‘y‘‘. Note that np.nan_to_num is used to ensure numerical\n",
            "stability. In particular, if both ‘‘a‘‘ and ‘‘y‘‘ have a 1.0\n",
            "in the same slot, then the expression (1-y)*np.log(1-a)\n",
            "returns nan. The np.nan_to_num ensures that that is converted\n",
            "to the correct value (0.0). \"\"\"\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer. Note that the\n",
            "parameter ‘‘z‘‘ is not used by the method. It is included in\n",
            "the method’s parameters in order to make the interface\n",
            "consistent with the delta method for other cost classes. \"\"\"\n",
            "return (a-y)\n",
            "#### Main Network class\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "\"\"\"The list ‘‘sizes‘‘ contains the number of neurons in the respective\n",
            "\n",
            "(cid:12)\n",
            "102 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "layers of the network. For example, if the list was [2, 3, 1]\n",
            "then it would be a three-layer network, with the first layer\n",
            "containing 2 neurons, the second layer 3 neurons, and the\n",
            "third layer 1 neuron. The biases and weights for the network\n",
            "are initialized randomly, using\n",
            "‘‘self.default_weight_initializer‘‘ (see docstring for that\n",
            "method). 3 \"\"\"\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "def default_weight_initializer(self):\n",
            "\"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
            "and standard deviation 1 over the square root of the number of\n",
            "weights connecting to the same neuron. Initialize the biases\n",
            "using a Gaussian distribution with mean 0 and standard\n",
            "deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we won’t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "def large_weight_initializer(self):\n",
            "\"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
            "and standard deviation 1. Initialize the biases using a\n",
            "Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we won’t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. This weight and bias initializer uses the same approach as in\n",
            "Chapter 1, and is included for purposes of comparison.\n",
            "It\n",
            "will usually be better to use the default weight initializer\n",
            "instead. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)\n",
            "for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "return a\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
            "evaluation_data=None, monitor_evaluation_cost=False,\n",
            "monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
            "monitor_training_accuracy=False):\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 103\n",
            "(cid:12)\n",
            "\"\"\"Train the neural network using mini-batch stochastic gradient\n",
            "descent. The ‘‘training_data‘‘ is a list of tuples ‘‘(x, y)‘‘\n",
            "representing the training inputs and the desired outputs.\n",
            "The\n",
            "other non-optional parameters are self-explanatory, as is the\n",
            "regularization parameter ‘‘lmbda‘‘. The method also accepts\n",
            "‘‘evaluation_data‘‘, usually either the validation or test\n",
            "data. We can monitor the cost and accuracy on either the\n",
            "evaluation data or the training data, by setting the\n",
            "appropriate flags. The method returns a tuple containing four 3\n",
            "lists: the (per-epoch) costs on the evaluation data, the\n",
            "accuracies on the evaluation data, the costs on the training\n",
            "data, and the accuracies on the training data. All values are\n",
            "evaluated at the end of each training epoch. So, for example,\n",
            "if we train for 30 epochs, then the first element of the tuple\n",
            "will be a 30-element list containing the cost on the\n",
            "evaluation data at the end of each epoch. Note that the lists\n",
            "are empty if the corresponding flag is not set. \"\"\"\n",
            "if evaluation_data:\n",
            "n_data = len(evaluation_data)\n",
            "n = len(training_data)\n",
            "evaluation_cost, evaluation_accuracy = [], []\n",
            "training_cost, training_accuracy = [], []\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [\n",
            "training_data[k:k+mini_batch_size]\n",
            "for k in xrange(0, n, mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(\n",
            "mini_batch, eta, lmbda, len(training_data))\n",
            "print \"Epoch %s training complete\" % j\n",
            "if monitor_training_cost:\n",
            "cost = self.total_cost(training_data, lmbda)\n",
            "training_cost.append(cost)\n",
            "print \"Cost on training data: {}\".format(cost)\n",
            "if monitor_training_accuracy:\n",
            "accuracy = self.accuracy(training_data, convert=True)\n",
            "training_accuracy.append(accuracy)\n",
            "print \"Accuracy on training data: {} / {}\".format(accuracy, n)\n",
            "if monitor_evaluation_cost:\n",
            "cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
            "evaluation_cost.append(cost)\n",
            "print \"Cost on evaluation data: {}\".format(cost)\n",
            "if monitor_evaluation_accuracy:\n",
            "accuracy = self.accuracy(evaluation_data)\n",
            "evaluation_accuracy.append(accuracy)\n",
            "print \"Accuracy on evaluation data: {} / {}\".format(self.accuracy(\n",
            "evaluation_data), n_data)\n",
            "print\n",
            "return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
            "def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
            "\"\"\"Update the network’s weights and biases by applying gradient\n",
            "descent using backpropagation to a single mini batch. The\n",
            "‘‘mini_batch‘‘ is a list of tuples ‘‘(x, y)‘‘, ‘‘eta‘‘ is the\n",
            "learning rate, ‘‘lmbda‘‘ is the regularization parameter, and\n",
            "‘‘n‘‘ is the total size of the training data set.\n",
            "\"\"\"\n",
            "\n",
            "(cid:12)\n",
            "104 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "3 self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "def backprop(self, x, y):\n",
            "\"\"\"Return a tuple ‘‘(nabla_b, nabla_w)‘‘ representing the\n",
            "gradient for the cost function C_x. ‘‘nabla_b‘‘ and\n",
            "‘‘nabla_w‘‘ are layer-by-layer lists of numpy arrays, similar\n",
            "to ‘‘self.biases‘‘ and ‘‘self.weights‘‘.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "zs.append(z)\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. It’s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "def accuracy(self, data, convert=False):\n",
            "\"\"\"Return the number of inputs in ‘‘data‘‘ for which the neural\n",
            "network outputs the correct result. The neural network’s\n",
            "output is assumed to be the index of whichever neuron in the\n",
            "final layer has the highest activation.\n",
            "The flag ‘‘convert‘‘ should be set to False if the data set is\n",
            "validation or test data (the usual case), and to True if the\n",
            "data set is the training data. The need for this flag arises\n",
            "due to differences in the way the results ‘‘y‘‘ are\n",
            "represented in the different data sets. In particular, it\n",
            "flags whether we need to convert between the different\n",
            "representations. It may seem strange to use different\n",
            "representations for the different data sets. Why not use the\n",
            "same representation for all three data sets? It’s done for\n",
            "efficiency reasons -- the program usually evaluates the cost\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 105\n",
            "(cid:12)\n",
            "on the training data and the accuracy on other data sets. These are different types of computations, and using different\n",
            "representations speeds things up. More details on the\n",
            "representations can be found in\n",
            "mnist_loader.load_data_wrapper. \"\"\"\n",
            "if convert:\n",
            "results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in 3\n",
            "data]\n",
            "else:\n",
            "results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
            "return sum(int(x == y) for (x, y) in results)\n",
            "def total_cost(self, data, lmbda, convert=False):\n",
            "\"\"\"Return the total cost for the data set ‘‘data‘‘. The flag\n",
            "‘‘convert‘‘ should be set to False if the data set is the\n",
            "training data (the usual case), and to True if the data set is\n",
            "the validation or test data. See comments on the similar (but\n",
            "reversed) convention for the ‘‘accuracy‘‘ method, above. \"\"\"\n",
            "cost = 0.0\n",
            "for x, y in data:\n",
            "a = self.feedforward(x)\n",
            "if convert:\n",
            "y = vectorized_result(y)\n",
            "cost += self.cost.fn(a, y)/len(data)\n",
            "cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
            "return cost\n",
            "def save(self, filename):\n",
            "\"\"\"Save the neural network to the file ‘‘filename‘‘.\"\"\"\n",
            "data = {\"sizes\": self.sizes,\n",
            "\"weights\": [w.tolist() for w in self.weights],\n",
            "\"biases\": [b.tolist() for b in self.biases],\n",
            "\"cost\": str(self.cost.__name__)}\n",
            "f = open(filename, \"w\")\n",
            "json.dump(data, f)\n",
            "f.close()\n",
            "#### Loading a Network\n",
            "def load(filename):\n",
            "\"\"\"Load a neural network from the file ‘‘filename‘‘. Returns an\n",
            "instance of Network. \"\"\"\n",
            "f = open(filename, \"r\")\n",
            "data = json.load(f)\n",
            "f.close()\n",
            "cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
            "net = Network(data[\"sizes\"], cost=cost)\n",
            "net.weights = [np.array(w) for w in data[\"weights\"]]\n",
            "net.biases = [np.array(b) for b in data[\"biases\"]]\n",
            "return net\n",
            "#### Miscellaneous functions\n",
            "def vectorized_result(j):\n",
            "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j’th position\n",
            "and zeroes elsewhere. This is used to convert a digit (0...9)\n",
            "into a corresponding desired output from the neural network.\n",
            "\n",
            "(cid:12)\n",
            "106 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "\"\"\"\n",
            "e = np.zeros((10, 1))\n",
            "e[j] = 1.0\n",
            "return e\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "3\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "OneofthemoreinterestingchangesinthecodeistoincludeL2regularization. Although\n",
            "thisisamajorconceptualchange,it’ssotrivialtoimplementthatit’seasytomissinthe\n",
            "code.\n",
            "Forthemostpartitjustinvolvespassingtheparameterlmbdatovariousmethods,\n",
            "notablytheNetwork.SGDmethod. Therealworkisdoneinasinglelineoftheprogram,the\n",
            "fourth-lastlineoftheNetwork.update_mini_batchmethod. That’swherewemodifythe\n",
            "gradientdescentupdateruletoincludeweightdecay. Butalthoughthemodificationistiny,\n",
            "ithasabigimpactonresults!\n",
            "Thisis,bytheway,commonwhenimplementingnewtechniquesinneuralnetworks. We’vespentthousandsofwordsdiscussingregularization. It’sconceptuallyquitesubtleand\n",
            "difficulttounderstand. Andyetitwastrivialtoaddtoourprogram! Itoccurssurprisingly\n",
            "oftenthatsophisticatedtechniquescanbeimplementedwithsmallchangestocode. Anothersmallbutimportantchangetoourcodeistheadditionofseveraloptionalflags\n",
            "tothestochasticgradientdescentmethod,Network.SGD.Theseflagsmakeitpossibleto\n",
            "monitorthecostandaccuracyeitheronthetraining_dataoronasetofevaluation_data\n",
            "whichcanbepassedtoNetwork.SGD. We’veusedtheseflagsoftenearlierinthechapter,\n",
            "butletmegiveanexampleofhowitworks,justtoremindyou:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True, monitor_evaluation_cost=True,\n",
            "monitor_training_accuracy=True,\n",
            "... monitor_training_cost=True)\n",
            "Here,we’resettingtheevaluation_datatobethevalidation_data. Butwecouldalso\n",
            "havemonitoredperformanceonthetest_dataoranyotherdataset. Wealsohavefour\n",
            "flagstellingustomonitorthecostandaccuracyonboththeevaluation_dataandthe\n",
            "training_data. ThoseflagsareFalsebydefault,butthey’vebeenturnedonhereinorder\n",
            "tomonitorourNetwork’sperformance. Furthermore,network2.py’sNetwork.SGDmethod\n",
            "returnsafour-elementtuplerepresentingtheresultsofthemonitoring. Wecanusethisas\n",
            "follows:\n",
            ">>> evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net\n",
            ".SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=\n",
            "True, monitor_training_accuracy=True, monitor_training_cost=True)\n",
            "So,forexample,evaluation_costwillbea30-elementlistcontainingthecostonthe\n",
            "evaluationdataattheendofeachepoch. Thissortofinformationisextremelyusefulin\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 107\n",
            "(cid:12)\n",
            "understandinganetwork’sbehaviour.\n",
            "Itcan,forexample,beusedtodrawgraphsshowing\n",
            "howthenetworklearnsovertime. Indeed,that’sexactlyhowIconstructedallthegraphs\n",
            "earlierinthechapter. Note,however,thatifanyofthemonitoringflagsarenotset,thenthe\n",
            "correspondingelementinthetuplewillbetheemptylist. OtheradditionstothecodeincludeaNetwork.savemethod,tosaveNetworkobjects\n",
            "todisk,andafunctiontoloadthembackinagainlater. Notethatthesavingandloading\n",
            "isdoneusingJSON,notPython’spickleorcPicklemodules,whicharetheusualwaywe 3\n",
            "saveandloadobjectstoandfromdiskinPython. UsingJSONrequiresmorecodethan\n",
            "pickleorcPicklewould. TounderstandwhyI’veusedJSON,imaginethatatsometimein\n",
            "thefuturewedecidedtochangeourNetworkclasstoallowneuronsotherthansigmoid\n",
            "neurons. Toimplementthatchangewe’dmostlikelychangetheattributesdefinedinthe\n",
            "Network.__init__method. Ifwe’vesimplypickledtheobjectsthatwouldcauseourload\n",
            "functiontofail. UsingJSONtodotheserializationexplicitlymakesiteasytoensurethatold\n",
            "Networkswillstillload. Therearemanyotherminorchangesinthecodefornetwork2.py,butthey’reallsimple\n",
            "variationsonnetwork.py. Thenetresultistoexpandour74-lineprogramtoafarmore\n",
            "capable152lines. Problems\n",
            "ModifythecodeabovetoimplementL1regularization,anduseL1regularizationto\n",
            "• classifyMNISTdigitsusinga30hiddenneuronnetwork. Canyoufindaregularization\n",
            "parameterthatenablesyoutodobetterthanrunningunregularized? TakealookattheNetwork.cost_derivativemethodinnetwork.py.\n",
            "Thatmethod\n",
            "• waswrittenforthequadraticcost. Howwouldyourewritethemethodforthecross-\n",
            "entropycost? Canyouthinkofaproblemthatmightariseinthecross-entropyversion? Innetwork2.pywe’veeliminatedtheNetwork.cost_derivativemethodentirely,\n",
            "insteadincorporatingitsfunctionalityintotheCrossEntropyCost.deltamethod.\n",
            "Howdoesthissolvetheproblemyou’vejustidentified? 3.5 How to choose a neural network’s hyper-parameters? UpuntilnowIhaven’texplainedhowI’vebeenchoosingvaluesforhyper-parameterssuch\n",
            "asthelearningrate,η,theregularizationparameter,λ,andsoon. I’vejustbeensupplying\n",
            "valueswhichworkprettywell. Inpractice,whenyou’reusingneuralnetstoattackaproblem,\n",
            "it can be difficult to find good hyper-parameters. Imagine, for example, that we’ve just\n",
            "beenintroducedtotheMNISTproblem,andhavebegunworkingonit,knowingnothing\n",
            "atallaboutwhathyper-parameterstouse. Let’ssupposethatbygoodfortuneinourfirst\n",
            "experimentswechoosemanyofthehyper-parametersinthesamewayaswasdoneearlier\n",
            "this chapter: 30 hidden neurons, a mini-batch size of 10, training for 30 epochs using\n",
            "thecross-entropy. Butwechoosealearningrateη =10.0andregularizationparameter\n",
            "λ =1000.0. Here’swhatIsawononesuchrun:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 10.0, lmbda = 1000.0,\n",
            "\n",
            "(cid:12)\n",
            "108 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 1030 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 990 / 10000\n",
            "3\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "... Epoch 27 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 983 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 967 / 10000\n",
            "Ourclassificationaccuraciesarenobetterthanchance! Ournetworkisactingasarandom\n",
            "noisegenerator!\n",
            "“Well,that’seasytofix,”youmightsay,“justdecreasethelearningrateandregularization\n",
            "hyper-parameters”. Unfortunately,youdon’taprioriknowthosearethehyper-parameters\n",
            "youneedtoadjust. Maybetherealproblemisthatour30hiddenneuronnetworkwillnever\n",
            "workwell,nomatterhowtheotherhyper-parametersarechosen? Maybewereallyneedat\n",
            "least100hiddenneurons? Or300hiddenneurons? Ormultiplehiddenlayers? Oradifferent\n",
            "approachtoencodingtheoutput? Maybeournetworkislearning,butweneedtotrain\n",
            "formoreepochs? Maybethemini-batchesaretoosmall? Maybewe’ddobetterswitching\n",
            "backtothequadraticcostfunction? Maybeweneedtotryadifferentapproachtoweight\n",
            "initialization? Andsoon,onandonandon. It’seasytofeellostinhyper-parameterspace. Thiscanbeparticularlyfrustratingifyournetworkisverylarge,orusesalotoftraining\n",
            "data,sinceyoumaytrainforhoursordaysorweeks,onlytogetnoresult. Ifthesituation\n",
            "persists,itdamagesyourconfidence. Maybeneuralnetworksarethewrongapproachto\n",
            "yourproblem?\n",
            "Maybeyoushouldquityourjobandtakeupbeekeeping? InthissectionIexplainsomeheuristicswhichcanbeusedtosetthehyper-parameters\n",
            "inaneuralnetwork. Thegoalistohelpyoudevelopaworkflowthatenablesyoutodoa\n",
            "prettygoodjobsettinghyper-parameters. Ofcourse,Iwon’tcovereverythingabouthyper-\n",
            "parameteroptimization. That’sahugesubject, andit’snot, inanycase, aproblemthat\n",
            "isevercompletelysolved,noristhereuniversalagreementamongstpractitionersonthe\n",
            "rightstrategiestouse. There’salwaysonemoretrickyoucantrytoekeoutabitmore\n",
            "performancefromyournetwork. Buttheheuristicsinthissectionshouldgetyoustarted. Broadstrategy:Whenusingneuralnetworkstoattackanewproblemthefirstchallenge\n",
            "istogetanynon-triviallearning,i.e.,forthenetworktoachieveresultsbetterthanchance. Thiscanbesurprisinglydifficult,especiallywhenconfrontinganewclassofproblem.\n",
            "Let’s\n",
            "lookatsomestrategiesyoucanuseifyou’rehavingthiskindoftrouble. Suppose, for example, that you’re attacking MNIST for the first time. You start out\n",
            "enthusiastic,butarealittlediscouragedwhenyourfirstnetworkfailscompletely,asinthe\n",
            "exampleabove.\n",
            "Thewaytogoistostriptheproblemdown. Getridofallthetraining\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 109\n",
            "(cid:12)\n",
            "andvalidationimagesexceptimageswhichare0sor1s. Thentrytotrainanetworkto\n",
            "distinguish0sfrom1s. Notonlyisthataninherentlyeasierproblemthandistinguishingall\n",
            "tendigits,italsoreducestheamountoftrainingdataby80percent,speedinguptrainingby\n",
            "afactorof5. Thatenablesmuchmorerapidexperimentation,andsogivesyoumorerapid\n",
            "insightintohowtobuildagoodnetwork. Youcanfurtherspeedupexperimentationbystrippingyournetworkdowntothesimplest\n",
            "networklikelytodomeaningfullearning. Ifyoubelievea[784, 10]networkcanlikely 3\n",
            "dobetter-than-chanceclassificationofMNISTdigits,thenbeginyourexperimentationwith\n",
            "suchanetwork. It’llbemuchfasterthantraininga[784, 30, 10]network,andyoucan\n",
            "buildbackuptothelatter. Youcangetanotherspeedupinexperimentationbyincreasingthefrequencyofmoni-\n",
            "toring. Innetwork2.pywemonitorperformanceattheendofeachtrainingepoch. With\n",
            "50,000imagesperepoch,thatmeanswaitingalittlewhile–abouttensecondsperepoch,\n",
            "onmylaptop,whentraininga[784,30,10]network–beforegettingfeedbackonhowwell\n",
            "thenetworkislearning. Ofcourse,tensecondsisn’tverylong,butifyouwanttotrialdozens\n",
            "ofhyper-parameterchoicesit’sannoying,andifyouwanttotrialhundredsorthousands\n",
            "ofchoicesitstartstogetdebilitating. Wecangetfeedbackmorequicklybymonitoringthe\n",
            "validationaccuracymoreoften,say,afterevery1,000trainingimages. Furthermore,instead\n",
            "ofusingthefull10,000imagevalidationsettomonitorperformance,wecangetamuch\n",
            "fasterestimateusingjust100validationimages. Allthatmattersisthatthenetworksees\n",
            "enoughimagestodoreallearning,andtogetaprettygoodroughestimateofperformance. Ofcourse,ourprogramnetwork2.pydoesn’tcurrentlydothiskindofmonitoring. Butas\n",
            "akludgetoachieveasimilareffectforthepurposesofillustration, we’llstripdownour\n",
            "trainingdatatojustthefirst1,000MNISTtrainingimages. Let’stryitandseewhathappens.\n",
            "(TokeepthecodebelowsimpleIhaven’timplementedtheideaofusingonly0and1images. Ofcourse,thatcanbedonewithjustalittlemorework.)\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 1000.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "...\n",
            "monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... We’restillgettingpurenoise!\n",
            "Butthere’sabigwin: we’renowgettingfeedbackinafraction\n",
            "ofasecond,ratherthanonceeverytensecondsorso. Thatmeansyoucanmorequickly\n",
            "experimentwithotherchoicesofhyper-parameter,orevenconductexperimentstrialling\n",
            "manydifferentchoicesofhyper-parameternearlysimultaneously. IntheaboveexampleIleftλasλ =1000.0,asweusedearlier. Butsincewechanged\n",
            "thenumberoftrainingexamplesweshouldreallychangeλtokeeptheweightdecaythe\n",
            "same. Thatmeanschangingλto20.0. Ifwedothatthenthisiswhathappens:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "\n",
            "(cid:12)\n",
            "110 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 12 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 14 / 100\n",
            "Epoch 2 training complete\n",
            "3 Accuracy on evaluation data: 25 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 18 / 100\n",
            "... Ahah!\n",
            "Wehaveasignal. Notaterriblygoodsignal,butasignalnonetheless. That’ssomething\n",
            "wecanbuildon,modifyingthehyper-parameterstotrytogetfurtherimprovement. Maybe\n",
            "weguessthatourlearningrateneedstobehigher. (Asyouperhapsrealize,that’sasilly\n",
            "guess,forreasonswe’lldiscussshortly,butpleasebearwithme.) Sototestourguesswetry\n",
            "dialingηupto100.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 100.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... That’snogood!\n",
            "Itsuggeststhatourguesswaswrong, andtheproblemwasn’t thatthe\n",
            "learningratewastoolow. Soinsteadwetrydialingηdowntoη =1.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 1.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 62 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 42 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 43 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 61 / 100\n",
            "... \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 111\n",
            "(cid:12)\n",
            "That’sbetter!\n",
            "Andsowecancontinue,individuallyadjustingeachhyper-parameter,gradually\n",
            "improvingperformance. Oncewe’veexploredtofindanimprovedvalueforη,thenwe\n",
            "moveontofindagoodvalueforλ. Thenexperimentwithamorecomplexarchitecture,\n",
            "sayanetworkwith10hiddenneurons. Thenadjustthevaluesforηandλagain. Then\n",
            "increaseto20hiddenneurons. Andthenadjustotherhyper-parameterssomemore.\n",
            "And\n",
            "soon,ateachstageevaluatingperformanceusingourheld-outvalidationdata,andusing\n",
            "thoseevaluationstofindbetterandbetterhyper-parameters. Aswedoso,ittypicallytakes 3\n",
            "longertowitnesstheimpactduetomodificationsofthehyper-parameters,andsowecan\n",
            "graduallydecreasethefrequencyofmonitoring. Thisalllooksverypromisingasabroadstrategy.\n",
            "However,Iwanttoreturntothatinitial\n",
            "stageoffindinghyper-parametersthatenableanetworktolearnanythingatall. Infact,\n",
            "eventheabovediscussionconveystoopositiveanoutlook. Itcanbeimmenselyfrustrating\n",
            "toworkwithanetworkthat’slearningnothing. Youcantweakhyper-parametersfordays,\n",
            "andstillgetnomeaningfulresponse. AndsoI’dliketore-emphasizethatduringtheearly\n",
            "stagesyoushouldmakesureyoucangetquickfeedbackfromexperiments. Intuitively,it\n",
            "mayseemasthoughsimplifyingtheproblemandthearchitecturewillmerelyslowyoudown. Infact,itspeedsthingsup,sinceyoumuchmorequicklyfindanetworkwithameaningful\n",
            "signal. Onceyou’vegotsuchasignal,youcanoftengetrapidimprovementsbytweakingthe\n",
            "hyper-parameters. Aswithmanythingsinlife,gettingstartedcanbethehardestthingtodo. Okay,that’sthebroadstrategy. Let’snowlookatsomespecificrecommendationsfor\n",
            "settinghyper-parameters. Iwillfocusonthelearningrate,η,theL2regularizationparameter,\n",
            "λ, and the mini-batch size. However, many of the remarks apply also to other hyper-\n",
            "parameters,includingthoseassociatedtonetworkarchitecture,otherformsofregularization,\n",
            "andsomehyper-parameterswe’llmeetlaterinthebook,suchasthemomentumco-efficient. Learningrate: SupposewerunthreeMNISTnetworkswiththreedifferentlearningrates,\n",
            "η =0.025,η =0.25andη =2.5,respectively. We’llsettheotherhyper-parametersasfor\n",
            "theexperimentsinearliersections,runningover30epochs,withamini-batchsizeof10,\n",
            "andwithλ =5.0. We’llalsoreturntousingthefull50,000trainingimages. Here’sagraph\n",
            "showingthebehaviourofthetrainingcostaswetrain26:\n",
            "26Thegraphwasgeneratedbymultiple_eta.py. \n",
            "(cid:12)\n",
            "112 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Withη =0.025thecostdecreasessmoothlyuntilthefinalepoch. Withη =0.25thecost\n",
            "initiallydecreases,butafterabout20epochsitisnearsaturation,andthereaftermostofthe\n",
            "changesaremerelysmallandapparentlyrandomoscillations. Finally,withη =2.5thecost\n",
            "makeslargeoscillationsrightfromthestart. Tounderstandthereasonfortheoscillations,\n",
            "recallthatstochasticgradientdescentissupposedtostepusgraduallydownintoavalleyof\n",
            "thecostfunction,\n",
            "3\n",
            "However,ifηistoolargethenthestepswillbesolargethattheymayactuallyovershootthe\n",
            "minimum,causingthealgorithmtoclimbupoutofthevalleyinstead. That’slikely27what’s\n",
            "causingthecosttooscillatewhenη =2.5. Whenwechooseη=0.25theinitialstepsdotake\n",
            "ustowardaminimumofthecostfunction,andit’sonlyoncewegetnearthatminimumthat\n",
            "westarttosufferfromtheovershootingproblem. Andwhenwechooseη =0.025wedon’t\n",
            "sufferfromthisproblematallduringthefirst30epochs. Ofcourse,choosingηsosmall\n",
            "createsanotherproblem,namely,thatitslowsdownstochasticgradientdescent. Aneven\n",
            "betterapproachwouldbetostartwithη =0.25,trainfor20epochs,andthenswitchto\n",
            "η =0.025. We’lldiscusssuchvariablelearningratescheduleslater. Fornow,though,let’s\n",
            "sticktofiguringouthowtofindasinglegoodvalueforthelearningrate,η. Withthispictureinmind, wecansetηasfollows. First, weestimatethethreshold\n",
            "valueforηatwhichthecostonthetrainingdataimmediatelybeginsdecreasing,insteadof\n",
            "oscillatingorincreasing. Thisestimatedoesn’tneedtobetooaccurate.\n",
            "Youcanestimate\n",
            "theorderofmagnitudebystartingwithη=0.01. Ifthecostdecreasesduringthefirstfew\n",
            "epochs,thenyoushouldsuccessivelytryη =0.1,1.0,...untilyoufindavalueforηwhere\n",
            "thecostoscillatesorincreasesduringthefirstfewepochs. Alternately,ifthecostoscillates\n",
            "orincreasesduringthefirstfewepochswhenη=0.01,thentryη=0.001,0.0001,...until\n",
            "youfindavalueforηwherethecostdecreasesduringthefirstfewepochs. Followingthis\n",
            "procedurewillgiveusanorderofmagnitudeestimateforthethresholdvalueofη. You\n",
            "mayoptionallyrefineyourestimate,topickoutthelargestvalueofηatwhichthecost\n",
            "decreasesduringthefirstfewepochs,sayη=0.5orη=0.2(there’snoneedforthistobe\n",
            "super-accurate). Thisgivesusanestimateforthethresholdvalueofη.\n",
            "Obviously,theactualvalueofηthatyouuseshouldbenolargerthanthethreshold\n",
            "value.\n",
            "Infact,ifthevalueofηistoremainusableovermanyepochsthenyoulikelywantto\n",
            "27Thispictureishelpful,butit’sintendedasanintuition-buildingillustrationofwhatmaygoon,not\n",
            "asacomplete,exhaustiveexplanation. Briefly,amorecompleteexplanationisasfollows: gradient\n",
            "descentusesafirst-orderapproximationtothecostfunctionasaguidetohowtodecreasethecost. Forlargeη,higher-ordertermsinthecostfunctionbecomemoreimportant,andmaydominatethe\n",
            "behaviour,causinggradientdescenttobreakdown.Thisisespeciallylikelyasweapproachminimaand\n",
            "quasi-minimaofthecostfunction,sincenearsuchpointsthegradientbecomessmall,makingiteasier\n",
            "forhigher-ordertermstodominatebehaviour. \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 113\n",
            "(cid:12)\n",
            "useavalueforηthatissmaller,say,afactoroftwobelowthethreshold.\n",
            "Suchachoicewill\n",
            "typicallyallowyoutotrainformanyepochs,withoutcausingtoomuchofaslowdownin\n",
            "learning. InthecaseoftheMNISTdata,followingthisstrategyleadstoanestimateof0.1forthe\n",
            "orderofmagnitudeofthethresholdvalueofη. Aftersomemorerefinement,weobtaina\n",
            "thresholdvalueη=0.5. Followingtheprescriptionabove,thissuggestsusingη=0.25asour\n",
            "valueforthelearningrate. Infact,Ifoundthatusingη=0.5workedwellenoughover30 3\n",
            "epochsthatforthemostpartIdidn’tworryaboutusingalowervalueofη. Thisallseemsquitestraightforward. However,usingthetrainingcosttopickηappears\n",
            "tocontradictwhatIsaidearlierinthissection,namely,thatwe’dpickhyper-parameters\n",
            "byevaluatingperformanceusingourheld-outvalidationdata. Infact,we’llusevalidation\n",
            "accuracy to pick the regularization hyper-parameter, the mini-batch size, and network\n",
            "parameterssuchasthenumberoflayersandhiddenneurons,andsoon. Whydothings\n",
            "differentlyforthelearningrate?\n",
            "Frankly,thischoiceismypersonalaestheticpreference,\n",
            "andisperhapssomewhatidiosyncratic. Thereasoningisthattheotherhyper-parameters\n",
            "areintendedtoimprovethefinalclassificationaccuracyonthetestset,andsoitmakes\n",
            "sensetoselectthemonthebasisofvalidationaccuracy. However,thelearningrateisonly\n",
            "incidentallymeanttoimpactthefinalclassificationaccuracy. Itsprimarypurposeisreallyto\n",
            "controlthestepsizeingradientdescent,andmonitoringthetrainingcostisthebestway\n",
            "todetectifthestepsizeistoobig. Withthatsaid,thisisapersonalaestheticpreference. Earlyonduringlearningthetrainingcostusuallyonlydecreasesifthevalidationaccuracy\n",
            "improves,andsoinpracticeit’sunlikelytomakemuchdifferencewhichcriterionyouuse. Useearly stoppingto determinethenumber oftraining epochs: Aswediscussed\n",
            "earlierinthechapter,earlystoppingmeansthatattheendofeachepochweshouldcompute\n",
            "theclassificationaccuracyonthevalidationdata. Whenthatstopsimproving,terminate.\n",
            "Thismakessettingthenumberofepochsverysimple. Inparticular,itmeansthatwedon’t\n",
            "needtoworryaboutexplicitlyfiguringouthowthenumberofepochsdependsontheother\n",
            "hyper-parameters. Instead,that’stakencareofautomatically. Furthermore,earlystopping\n",
            "alsoautomaticallypreventsusfromoverfitting. Thisis,ofcourse,agoodthing,althoughin\n",
            "theearlystagesofexperimentationitcanbehelpfultoturnoffearlystopping,soyoucan\n",
            "seeanysignsofoverfitting,anduseittoinformyourapproachtoregularization. To implement early stopping we need to say more precisely what it means that the\n",
            "classificationaccuracyhasstoppedimproving. Aswe’veseen,theaccuracycanjumparound\n",
            "quiteabit,evenwhentheoveralltrendistoimprove.\n",
            "Ifwestopthefirsttimetheaccuracy\n",
            "decreasesthenwe’llalmostcertainlystopwhentherearemoreimprovementstobehad. A\n",
            "betterruleistoterminateifthebestclassificationaccuracydoesn’timproveforquitesome\n",
            "time. Suppose,forexample,thatwe’redoingMNIST.Thenwemightelecttoterminateifthe\n",
            "classificationaccuracyhasn’timprovedduringthelasttenepochs. Thisensuresthatwedon’t\n",
            "stoptoosoon,inresponsetobadluckintraining,butalsothatwe’renotwaitingaround\n",
            "foreverforanimprovementthatnevercomes. This no-improvement-in-ten rule is good for initial exploration of MNIST. However,\n",
            "networkscansometimesplateaunearaparticularclassificationaccuracyforquitesometime,\n",
            "onlytothenbeginimprovingagain. Ifyou’retryingtogetreallygoodperformance,theno-\n",
            "improvement-in-tenrulemaybetooaggressiveaboutstopping. Inthatcase,Isuggestusing\n",
            "theno-improvement-in-tenruleforinitialexperimentation,andgraduallyadoptingmore\n",
            "lenientrules,asyoubetterunderstandthewayyournetworktrains: no-improvement-in-\n",
            "twenty,no-improvement-in-fifty,andsoon. Ofcourse,thisintroducesanewhyper-parameter\n",
            "\n",
            "(cid:12)\n",
            "114 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "tooptimize! Inpractice,however,it’susuallyeasytosetthishyper-parametertogetpretty\n",
            "goodresults. Similarly,forproblemsotherthanMNIST,theno-improvement-in-tenrule\n",
            "maybemuchtooaggressiveornotnearlyaggressiveenough,dependingonthedetailsof\n",
            "theproblem. However,withalittleexperimentationit’susuallyeasytofindaprettygood\n",
            "strategyforearlystopping. Wehaven’tusedearlystoppinginourMNISTexperimentstodate. Thereasonisthat\n",
            "3 we’vebeendoingalotofcomparisonsbetweendifferentapproachestolearning. Forsuch\n",
            "comparisonsit’shelpfultousethesamenumberofepochsineachcase. However,it’swell\n",
            "worthmodifyingnetwork2.pytoimplementearlystopping:\n",
            "Problem\n",
            "Modifynetwork2.pysothatitimplementsearlystoppingusingano-improvement-\n",
            "• in-nepochsstrategy,wherenisaparameterthatcanbeset. Canyouthinkofaruleforearlystoppingotherthanno-improvement-in-n? Ideally,the\n",
            "• ruleshouldcompromisebetweengettinghighvalidationaccuraciesandnottraining\n",
            "toolong. Addyourruletonetwork2.py,andrunthreeexperimentscomparingthe\n",
            "validationaccuraciesandnumberofepochsoftrainingtono-improvement-in-10. Learningrateschedule: We’vebeenholdingthelearningrateηconstant. However,it’s\n",
            "oftenadvantageoustovarythelearningrate. Earlyonduringthelearningprocessit’slikely\n",
            "thattheweightsarebadlywrong. Andsoit’sbesttousealargelearningratethatcauses\n",
            "theweightstochangequickly. Later, wecanreducethelearningrateaswemakemore\n",
            "fine-tunedadjustmentstoourweights. Howshouldwesetourlearningrateschedule?\n",
            "Manyapproachesarepossible. One\n",
            "naturalapproachistousethesamebasicideaasearlystopping. Theideaistoholdthe\n",
            "learningrateconstantuntilthevalidationaccuracystartstogetworse. Thendecreasethe\n",
            "learningratebysomeamount,sayafactoroftwoorten. Werepeatthismanytimes,until,\n",
            "say,thelearningrateisafactorof1,024(or1,000)timeslowerthantheinitialvalue. Then\n",
            "weterminate. Avariablelearningschedulecanimproveperformance,butitalsoopensupaworldof\n",
            "possiblechoicesforthelearningschedule. Thosechoicescanbeaheadache–youcanspend\n",
            "forevertryingtooptimizeyourlearningschedule. Forfirstexperimentsmysuggestionisto\n",
            "useasingle,constantvalueforthelearningrate.\n",
            "That’llgetyouagoodfirstapproximation.\n",
            "Later,ifyouwanttoobtainthebestperformancefromyournetwork,it’sworthexperimenting\n",
            "withalearningschedule,alongthelinesI’vedescribed28. Exercise\n",
            "Modify network2.py so that it implements a learning schedule that: halves the\n",
            "• learningrateeachtimethevalidationaccuracysatisfiestheno-improvement-in-10\n",
            "rule;andterminateswhenthelearningratehasdroppedto1/128ofitsoriginalvalue. Theregularizationparameter,λ: Isuggeststartinginitiallywithnoregularization(λ\n",
            "=\n",
            "0.0),anddeterminingavalueforη,asabove.\n",
            "Usingthatchoiceofη,wecanthenusethe\n",
            "validationdatatoselectagoodvalueforλ. Startbytriallingλ =1.029,andthenincreaseor\n",
            "decreasebyfactorsof10,asneededtoimproveperformanceonthevalidationdata. Once\n",
            "you’vefoundagoodorderofmagnitude,youcanfinetuneyourvalueofλ. Thatdone,you\n",
            "shouldreturnandre-optimizeηagain. 28Areadablerecentpaperwhichdemonstratesthebenefitsofvariablelearningratesinattacking\n",
            "MNISTisDeep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCire¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJürgenSchmidhuber(2010). 29Idon’thaveagoodprincipledjustificationforusingthisasastartingvalue.Ifanyoneknowsofa\n",
            "goodprincipleddiscussionofwheretostartwithλ,I’dappreciatehearingit(mn@michaelnielsen.org). \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 115\n",
            "(cid:12)\n",
            "Exercise\n",
            "It’stemptingtousegradientdescenttotrytolearngoodvaluesforhyper-parameters\n",
            "• suchasλandη. Canyouthinkofanobstacletousinggradientdescenttodetermine\n",
            "λ? Canyouthinkofanobstacletousinggradientdescenttodetermineη? HowIselectedhyper-parametersearlierinthisbook: Ifyouusetherecommendations\n",
            "inthissectionyou’llfindthatyougetvaluesforηandλwhichdon’talwaysexactlymatch 3\n",
            "thevaluesI’veusedearlierinthebook. Thereasonisthatthebookhasnarrativeconstraints\n",
            "thathavesometimesmadeitimpracticaltooptimizethehyper-parameters. Thinkofallthe\n",
            "comparisonswe’vemadeofdifferentapproachestolearning,e.g.,comparingthequadratic\n",
            "andcross-entropycostfunctions,comparingtheoldandnewmethodsofweightinitialization,\n",
            "runningwithandwithoutregularization,andsoon. Tomakesuchcomparisonsmeaningful,\n",
            "I’veusuallytriedtokeephyper-parametersconstantacrosstheapproachesbeingcompared\n",
            "(ortoscaletheminanappropriateway). Ofcourse,there’snoreasonforthesamehyper-\n",
            "parameterstobeoptimalforallthedifferentapproachestolearning,sothehyper-parameters\n",
            "I’veusedaresomethingofacompromise. Asanalternativetothiscompromise,Icouldhavetriedtooptimizetheheckoutofthe\n",
            "hyper-parametersforeverysingleapproachtolearning. Inprinciplethat’dbeabetter,fairer\n",
            "approach,sincethenwe’dseethebestfromeveryapproachtolearning. However,we’ve\n",
            "madedozensofcomparisonsalongtheselines,andinpracticeIfoundittoocomputationally\n",
            "expensive. That’swhyI’veadoptedthecompromiseofusingprettygood(butnotnecessarily\n",
            "optimal)choicesforthehyper-parameters. Mini-batchsize: Howshouldwesetthemini-batchsize? Toanswerthisquestion,let’s\n",
            "firstsupposethatwe’redoingonlinelearning,i.e.,thatwe’reusingamini-batchsizeof1. Theobviousworryaboutonlinelearningisthatusingmini-batcheswhichcontainjust\n",
            "asingletrainingexamplewillcausesignificanterrorsinourestimateofthegradient. In\n",
            "fact,though,theerrorsturnouttonotbesuchaproblem. Thereasonisthattheindividual\n",
            "gradientestimatesdon’tneedtobesuper-accurate. Allweneedisanestimateaccurate\n",
            "enoughthatourcostfunctiontendstokeepdecreasing. It’sasthoughyouaretryingtoget\n",
            "totheNorthMagneticPole,buthaveawonkycompassthat’s10–20degreesoffeachtime\n",
            "youlookatit. Providedyoustoptocheckthecompassfrequently,andthecompassgetsthe\n",
            "directionrightonaverage,you’llendupattheNorthMagneticPolejustfine. Basedonthisargument,itsoundsasthoughweshoulduseonlinelearning.\n",
            "Infact,the\n",
            "situationturnsouttobemorecomplicatedthanthat.\n",
            "InaprobleminthelastchapterI\n",
            "pointedoutthatit’spossibletousematrixtechniquestocomputethegradientupdatefor\n",
            "allexamplesinamini-batchsimultaneously,ratherthanloopingoverthem. Dependingon\n",
            "thedetailsofyourhardwareandlinearalgebralibrarythiscanmakeitquiteabitfaster\n",
            "tocomputethegradientestimateforamini-batchof(forexample)size100,ratherthan\n",
            "computing the mini-batch gradient estimate by looping over the 100 training examples\n",
            "separately. Itmighttake(say)only50timesaslong,ratherthan100timesaslong.\n",
            "Now,atfirstitseemsasthoughthisdoesn’thelpusthatmuch. Withourmini-batchof\n",
            "size100thelearningrulefortheweightslookslike:\n",
            "1 (cid:88)\n",
            "w\n",
            "→\n",
            "w (cid:48)=w\n",
            "−\n",
            "η\n",
            "100 x ∇\n",
            "C\n",
            "x\n",
            ", (3.45)\n",
            "\n",
            "(cid:12)\n",
            "116 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "wherethesumisovertrainingexamplesinthemini-batch. Thisisversus\n",
            "w w (cid:48)=w η C\n",
            "x\n",
            "(3.46)\n",
            "→ − ∇\n",
            "foronlinelearning. Evenifitonlytakes50timesaslongtodothemini-batchupdate,it\n",
            "stillseemslikelytobebettertodoonlinelearning,becausewe’dbeupdatingsomuchmore\n",
            "3 frequently. Suppose,however,thatinthemini-batchcaseweincreasethelearningratebya\n",
            "factor100,sotheupdaterulebecomes\n",
            "(cid:88)\n",
            "w w (cid:48)=w η C\n",
            "x\n",
            ". (3.47)\n",
            "→ − x ∇\n",
            "That’salotlikedoing100separateinstancesofonlinelearningwithalearningrateofη. Butitonlytakes50timesaslongasdoingasingleinstanceofonlinelearning.\n",
            "Ofcourse,it’s\n",
            "nottrulythesameas100instancesofonlinelearning,sinceinthemini-batchthe C ’sare\n",
            "x\n",
            "allevaluatedforthesamesetofweights,asopposedtothecumulativelearningth∇atoccurs\n",
            "intheonlinecase. Still,itseemsdistinctlypossiblethatusingthelargermini-batchwould\n",
            "speedthingsup. Withthesefactorsinmind,choosingthebestmini-batchsizeisacompromise. Toosmall,\n",
            "andyoudon’tgettotakefulladvantageofthebenefitsofgoodmatrixlibrariesoptimizedfor\n",
            "fasthardware.\n",
            "Toolargeandyou’resimplynotupdatingyourweightsoftenenough. What\n",
            "youneedistochooseacompromisevaluewhichmaximizesthespeedoflearning.Fortunately,\n",
            "thechoiceofmini-batchsizeatwhichthespeedismaximizedisrelativelyindependent\n",
            "oftheotherhyper-parameters(apartfromtheoverallarchitecture),soyoudon’tneedto\n",
            "haveoptimizedthosehyper-parametersinordertofindagoodmini-batchsize. Theway\n",
            "togoisthereforetousesomeacceptable(butnotnecessarilyoptimal)valuesfortheother\n",
            "hyper-parameters,andthentrialanumberofdifferentmini-batchsizes,scalingηasabove. Plotthevalidationaccuracyversustime(asin,realelapsedtime,notepoch!),andchoose\n",
            "whichevermini-batchsizegivesyouthemostrapidimprovementinperformance. Withthe\n",
            "mini-batchsizechosenyoucanthenproceedtooptimizetheotherhyper-parameters. Ofcourse,asyou’venodoubtrealized,Ihaven’tdonethisoptimizationinourwork. Indeed,ourimplementationdoesn’tusethefasterapproachtomini-batchupdatesatall. I’ve\n",
            "simplyusedamini-batchsizeof10withoutcommentorexplanationinnearlyallexamples. Becauseofthis,wecouldhavespeduplearningbyreducingthemini-batchsize. Ihaven’t\n",
            "donethis,inpartbecauseIwantedtoillustratetheuseofmini-batchesbeyondsize1,andin\n",
            "partbecausemypreliminaryexperimentssuggestedthespeedupwouldberathermodest. In\n",
            "practicalimplementations,however,wewouldmostcertainlyimplementthefasterapproach\n",
            "tomini-batchupdates,andthenmakeanefforttooptimizethemini-batchsize,inorderto\n",
            "maximizeouroverallspeed. Automatedtechniques: I’vebeendescribingtheseheuristicsasthoughyou’reoptimiz-\n",
            "ingyourhyper-parametersbyhand. Hand-optimizationisagoodwaytobuildupafeelfor\n",
            "howneuralnetworksbehave. However,andunsurprisingly,agreatdealofworkhasbeen\n",
            "doneonautomatingtheprocess. Acommontechniqueisgridsearch,whichsystematically\n",
            "searchesthroughagridinhyper-parameterspace. Areviewofboththeachievementsand\n",
            "thelimitationsofgridsearch(withsuggestionsforeasily-implementedalternatives)maybe\n",
            "foundina2012paper30byJamesBergstraandYoshuaBengio. Manymoresophisticated\n",
            "30Randomsearchforhyper-parameteroptimization,byJamesBergstraandYoshuaBengio(2012).\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 117\n",
            "(cid:12)\n",
            "approacheshavealsobeenproposed. Iwon’treviewallthatworkhere,butdowanttomen-\n",
            "tionaparticularlypromising2012paperwhichusedaBayesianapproachtoautomatically\n",
            "optimizehyper-parameters31. Thecodefromthepaperispubliclyavailable,andhasbeen\n",
            "usedwithsomesuccessbyotherresearchers. Summingup: Followingtherules-of-thumbI’vedescribedwon’tgiveyoutheabsolute\n",
            "bestpossibleresultsfromyourneuralnetwork. Butitwilllikelygiveyouagoodstartanda\n",
            "basisforfurtherimprovements. Inparticular,I’vediscussedthehyper-parameterslargely 3\n",
            "independently. Inpractice,therearerelationshipsbetweenthehyper-parameters. Youmay\n",
            "experimentwithη,feelthatyou’vegotitjustright,thenstarttooptimizeforλ,onlytofind\n",
            "thatit’smessingupyouroptimizationforη. Inpractice,ithelpstobouncebackwardand\n",
            "forward,graduallyclosingingoodvalues. Aboveall,keepinmindthattheheuristicsI’ve\n",
            "describedarerulesofthumb,notrulescastinstone. Youshouldbeonthelookoutforsigns\n",
            "thatthingsaren’tworking,andbewillingtoexperiment. Inparticular,thismeanscarefully\n",
            "monitoringyournetwork’sbehaviour,especiallythevalidationaccuracy. The difficulty of choosing hyper-parameters is exacerbated by the fact that the lore\n",
            "abouthowtochoosehyper-parametersiswidelyspread,acrossmanyresearchpapersand\n",
            "softwareprograms,andoftenisonlyavailableinsidetheheadsofindividualpractitioners. Therearemany,manypaperssettingout(sometimescontradictory)recommendationsfor\n",
            "howtoproceed.\n",
            "However,thereareafewparticularlyusefulpapersthatsynthesizeand\n",
            "distilloutmuchofthislore. YoshuaBengiohasa2012paper32 thatgivessomepractical\n",
            "recommendationsforusingbackpropagationandgradientdescenttotrainneuralnetworks,\n",
            "includingdeepneuralnets. BengiodiscussesmanyissuesinmuchmoredetailthanIhave,\n",
            "includinghowtodomoresystematichyper-parametersearches.\n",
            "Anothergoodpaperisa\n",
            "1998paper33 byYannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller.\n",
            "Both\n",
            "thesepapersappearinanextremelyuseful2012bookthatcollectsmanytrickscommonly\n",
            "usedinneuralnets34. Thebookisexpensive,butmanyofthearticleshavebeenplaced\n",
            "onlinebytheirrespectiveauthorswith,onepresumes,theblessingofthepublisher,andmay\n",
            "belocatedusingasearchengine. Onethingthatbecomesclearasyoureadthesearticlesand,especially,asyouengagein\n",
            "yourownexperiments,isthathyper-parameteroptimizationisnotaproblemthatisever\n",
            "completelysolved. There’salwaysanothertrickyoucantrytoimproveperformance. There\n",
            "isasayingcommonamongwritersthatbooksareneverfinished,onlyabandoned. Thesame\n",
            "isalsotrueofneuralnetworkoptimization: thespaceofhyper-parametersissolargethat\n",
            "oneneverreallyfinishesoptimizing,oneonlyabandonsthenetworktoposterity. Soyour\n",
            "goalshouldbetodevelopaworkflowthatenablesyoutoquicklydoaprettygoodjobon\n",
            "theoptimization,whileleavingyoutheflexibilitytotrymoredetailedoptimizations,ifthat’s\n",
            "important. Thechallengeofsettinghyper-parametershasledsomepeopletocomplainthatneural\n",
            "networksrequirealotofworkwhencomparedwithothermachinelearningtechniques. I’ve\n",
            "heardmanyvariationsonthefollowingcomplaint: “Yes,awell-tunedneuralnetworkmay\n",
            "getthebestperformanceontheproblem. Ontheotherhand,Icantryarandomforest[or\n",
            "31PracticalBayesianoptimizationofmachinelearningalgorithms,byJasperSnoek,HugoLarochelle,\n",
            "andRyanAdams. 32Practicalrecommendationsforgradient-basedtrainingofdeeparchitectures,byYoshuaBengio\n",
            "(2012). 33EfficientBackProp,byYannLeCun,LÃl’onBottou,GenevieveOrrandKlaus-RobertMüller(1998)\n",
            "34NeuralNetworks:TricksoftheTrade,editedbyGrégoireMontavon,GenevièveOrr,andKlaus-Robert\n",
            "Müller.\n",
            "\n",
            "(cid:12)\n",
            "118 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "SVMor... insertyourownfavoritetechnique]anditjustworks.\n",
            "Idon’thavetimetofigure\n",
            "outjusttherightneuralnetwork.” Ofcourse,fromapracticalpointofviewit’sgoodto\n",
            "haveeasy-to-applytechniques. Thisisparticularlytruewhenyou’rejustgettingstartedona\n",
            "problem,anditmaynotbeobviouswhethermachinelearningcanhelpsolvetheproblemat\n",
            "all. Ontheotherhand,ifgettingoptimalperformanceisimportant,thenyoumayneedto\n",
            "tryapproachesthatrequiremorespecialistknowledge. Whileitwouldbeniceifmachine\n",
            "3 learningwerealwayseasy,thereisnoapriorireasonitshouldbetriviallysimple. 3.6 Other techniques\n",
            "Eachtechniquedevelopedinthischapterisvaluabletoknowinitsownright,butthat’s\n",
            "nottheonlyreasonI’veexplainedthem. Thelargerpointistofamiliarizeyouwithsomeof\n",
            "theproblemswhichcanoccurinneuralnetworks,andwithastyleofanalysiswhichcan\n",
            "helpovercomethoseproblems. Inasense,we’vebeenlearninghowtothinkaboutneural\n",
            "nets. OvertheremainderofthischapterIbrieflysketchahandfulofothertechniques.\n",
            "These\n",
            "sketchesarelessin-depththantheearlierdiscussions,butshouldconveysomefeelingfor\n",
            "thediversityoftechniquesavailableforuseinneuralnetworks. 3.6.1 Variationsonstochasticgradientdescent\n",
            "StochasticgradientdescentbybackpropagationhasserveduswellinattackingtheMNIST\n",
            "digitclassificationproblem. However,therearemanyotherapproachestooptimizingthecost\n",
            "function,andsometimesthoseotherapproachesofferperformancesuperiortomini-batch\n",
            "stochasticgradientdescent. InthissectionIsketchtwosuchapproaches,theHessianand\n",
            "momentumtechniques. Hessiantechnique: Tobeginourdiscussionithelpstoputneuralnetworksasidefora\n",
            "bit. Instead,we’rejustgoingtoconsidertheabstractproblemofminimizingacostfunction\n",
            "C whichisafunctionofmanyvariables,w=w\n",
            "1\n",
            ",w\n",
            "2\n",
            ",...,soC=C(w). ByTaylor’stheorem,\n",
            "thecostfunctioncanbeapproximatednearapointwby\n",
            "(cid:88) ∂C 1(cid:88) ∂2C\n",
            "C(w+ ∆w)=C(w)+ ∂w ∆w j+ 2 ∆w j∂w ∂w ∆w k+... (3.48)\n",
            "j j jk j k\n",
            "Wecanrewritethismorecompactlyas\n",
            "1\n",
            "C(w+ ∆w)=C(w)+ C ∆w+ ∆wTH∆w+..., (3.49)\n",
            "∇ · 2\n",
            "where Cistheusualgradientvector,andHisamatrixknownastheHessianmatrix,whose\n",
            "jk-the∇ntryis∂2C/∂w ∂w . Supposeweapproximate C bydiscardingthehigher-order\n",
            "j k\n",
            "termsrepresentedby...above,\n",
            "1\n",
            "C(w+ ∆w) C(w)+ C ∆w+ ∆wTH∆w. (3.50)\n",
            "≈ ∇ · 2\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 119\n",
            "(cid:12)\n",
            "Usingcalculuswecanshowthattheexpressionontheright-handsidecanbeminimized35\n",
            "bychoosing\n",
            "∆w= H\n",
            "−\n",
            "1 C. (3.51)\n",
            "− ∇\n",
            "Provided(3.50)isagoodapproximateexpressionforthecostfunction,thenwe’dexpect\n",
            "thatmovingfromthepointwtow+ ∆w=w H\n",
            "−\n",
            "1 C shouldsignificantlydecreasethecost 3\n",
            "function. Thatsuggestsapossiblealgorithm−formi∇nimizingthecost:\n",
            "Chooseastartingpoint,w. • Updatewtoanewpointw (cid:48)=w H\n",
            "−\n",
            "1 C,wheretheHessianHand Carecomputed\n",
            "• atw. − ∇ ∇\n",
            "Update w\n",
            "(cid:48)\n",
            "toanewpoint w\n",
            "(cid:48)(cid:48)\n",
            "=w\n",
            "(cid:48)\n",
            "H\n",
            "(cid:48)−\n",
            "1\n",
            "(cid:48)\n",
            "C,wheretheHessian H\n",
            "(cid:48)\n",
            "and\n",
            "(cid:48)\n",
            "C are\n",
            "• computedatw. − ∇ ∇\n",
            "(cid:48)\n",
            "... •\n",
            "Inpractice,(3.50)isonlyanapproximation,andit’sbettertotakesmallersteps. Wedothis\n",
            "byrepeatedlychangingwbyanamount∆w= ηH\n",
            "−\n",
            "1 C,whereηisknownasthelearning\n",
            "rate. − ∇\n",
            "ThisapproachtominimizingacostfunctionisknownastheHessiantechniqueorHessian\n",
            "optimization. TherearetheoreticalandempiricalresultsshowingthatHessianmethods\n",
            "convergeonaminimuminfewerstepsthanstandardgradientdescent. Inparticular,by\n",
            "incorporating information about second-order changes in the cost function it’s possible\n",
            "for the Hessian approach to avoid many pathologies that can occur in gradient descent. Furthermore,thereareversionsofthebackpropagationalgorithmwhichcanbeusedto\n",
            "computetheHessian. If Hessian optimization is so great, why aren’t we using it in our neural networks? Unfortunately,whileithasmanydesirableproperties,ithasoneveryundesirableproperty:\n",
            "it’sverydifficulttoapplyinpractice.PartoftheproblemisthesheersizeoftheHessianmatrix. Supposeyouhaveaneuralnetworkwith107weightsandbiases. Thenthecorresponding\n",
            "Hessianmatrixwillcontain107 107 =1014entries. That’salotofentries!\n",
            "Andthatmakes\n",
            "computingH 1 C extremelyd×ifficultinpractice.\n",
            "However,thatdoesn’tmeanthatit’snot\n",
            "−\n",
            "useful to under∇stand. In fact, there are many variations on gradient descent which are\n",
            "inspiredbyHessianoptimization,butwhichavoidtheproblemwithoverly-largematrices. Let’stakealookatonesuchtechnique,momentum-basedgradientdescent. Momentum-basedgradientdescent: Intuitively,theadvantageHessianoptimization\n",
            "hasisthatitincorporatesnotjustinformationaboutthegradient,butalsoinformationabout\n",
            "how the gradient is changing. Momentum-based gradient descent is based on a similar\n",
            "intuition,butavoidslargematricesofsecondderivatives. Tounderstandthemomentum\n",
            "technique,thinkbacktoouroriginalpictureofgradientdescent1.5,inwhichweconsidered\n",
            "aballrollingdownintoavalley. Atthetime,weobservedthatgradientdescentis,despite\n",
            "itsname,onlylooselysimilartoaballfallingtothebottomofavalley. Themomentum\n",
            "techniquemodifiesgradientdescentintwowaysthatmakeitmoresimilartothephysical\n",
            "picture. First,itintroducesanotionof“velocity”fortheparameterswe’retryingtooptimize. Thegradientactstochangethevelocity,not(directly)the“position”,inmuchthesame\n",
            "wayasphysicalforceschangethevelocity,andonlyindirectlyaffectposition. Second,the\n",
            "35Strictlyspeaking,forthistobeaminimum,andnotmerelyanextremum,weneedtoassumethat\n",
            "theHessianmatrixispositivedefinite. Intuitively,thismeansthatthefunctionC lookslikeavalley\n",
            "locally,notamountainorasaddle.\n",
            "\n",
            "(cid:12)\n",
            "120 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "momentummethodintroducesakindoffrictionterm,whichtendstograduallyreducethe\n",
            "velocity. Let’s give a more precise mathematical description. We introduce velocity variables\n",
            "v = v 1 ,v 2 ,..., one for each corresponding w j variable36. Then we replace the gradient\n",
            "descentupdaterulew w (cid:48)=w η C by\n",
            "→ − ∇\n",
            "3 v v (cid:48)= µv η C (3.52)\n",
            "→ − ∇\n",
            "w w (cid:48)=w+v\n",
            "(cid:48)\n",
            ". (3.53)\n",
            "→\n",
            "Intheseequations,µisahyper-parameterwhichcontrolstheamountofdampingorfriction\n",
            "inthesystem.\n",
            "Tounderstandthemeaningoftheequationsit’shelpfultofirstconsiderthe\n",
            "casewhereµ =1,whichcorrespondstonofriction. Whenthat’sthecase,inspectionof\n",
            "theequationsshowsthatthe“force” C isnowmodifyingthevelocity,v,andthevelocity\n",
            "iscontrollingtherateofchangeofw∇. Intuitively,webuildupthevelocitybyrepeatedly\n",
            "addinggradienttermstoit. Thatmeansthatifthegradientisin(roughly)thesamedirection\n",
            "throughseveralroundsoflearning,wecanbuildupquiteabitofsteammovinginthat\n",
            "direction. Think,forexample,ofwhathappensifwe’removingstraightdownaslope:\n",
            "Witheachstepthevelocitygetslargerdowntheslope,sowemovemoreandmorequickly\n",
            "tothebottomofthevalley. Thiscanenablethemomentumtechniquetoworkmuchfaster\n",
            "thanstandardgradientdescent. Ofcourse,aproblemisthatoncewereachthebottomof\n",
            "thevalleywewillovershoot. Or,ifthegradientshouldchangerapidly,thenwecouldfind\n",
            "ourselvesmovinginthewrongdirection. That’sthereasonfortheµhyper-parameterin\n",
            "(3.52). Isaidearlierthatµcontrolstheamountoffrictioninthesystem;tobealittlemore\n",
            "precise,youshouldthinkof1 µastheamountoffrictioninthesystem. Whenµ =1,as\n",
            "we’veseen,thereisnofriction−,andthevelocityiscompletelydrivenbythegradient C. Bycontrast,whenµ =0there’salotoffriction,thevelocitycan’tbuildup,andEquati∇ons\n",
            "(3.52)and(3.53)reducetotheusualequationforgradientdescent,w w (cid:48)=w η C. In\n",
            "practice,usingavalueofµintermediatebetween0and1cangiveusm→uchofthe−be∇nefitof\n",
            "beingabletobuildupspeed,butwithoutcausingovershooting. Wecanchoosesuchavalue\n",
            "forµusingtheheld-outvalidationdata,inmuchthesamewayasweselectηandλ.\n",
            "I’veavoidednamingthehyper-parameterµuptonow. Thereasonisthatthestandard\n",
            "name for µ is badly chosen: it’s called the momentum co-efficient. This is potentially\n",
            "36Inaneuralnetthew variableswould,ofcourse,includeallweightsandbiases.\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 121\n",
            "(cid:12)\n",
            "confusing,sinceµisnotatallthesameasthenotionofmomentumfromphysics. Rather,it\n",
            "ismuchmorecloselyrelatedtofriction.\n",
            "However,thetermmomentumco-efficientiswidely\n",
            "used,sowewillcontinuetouseit. Anicethingaboutthemomentumtechniqueisthatittakesalmostnoworktomod-\n",
            "ify an implementation of gradient descent to incorporate momentum. We can still use\n",
            "backpropagationtocomputethegradients,justasbefore,anduseideassuchassampling\n",
            "stochasticallychosenmini-batches. Inthisway,wecangetsomeoftheadvantagesofthe 3\n",
            "Hessiantechnique,usinginformationabouthowthegradientischanging. Butit’sdone\n",
            "withoutthedisadvantages,andwithonlyminormodificationstoourcode. Inpractice,the\n",
            "momentumtechniqueiscommonlyused,andoftenspeedsuplearning. Exercise\n",
            "Whatwouldgowrongifweusedµ>1inthemomentumtechnique?\n",
            "• Whatwouldgowrongifweusedµ<0inthemomentumtechnique? •\n",
            "Problem\n",
            "Addmomentum-basedstochasticgradientdescenttonetwork2.py. •\n",
            "Otherapproachestominimizingthecostfunction: Manyotherapproachestominimizingthe\n",
            "costfunctionhavebeendeveloped,andthereisn’tuniversalagreementonwhichisthebest\n",
            "approach. Asyougodeeperintoneuralnetworksit’sworthdiggingintotheothertechniques,\n",
            "understandinghowtheywork,theirstrengthsandweaknesses,andhowtoapplythemin\n",
            "practice. ApaperImentionedearlier37introducesandcomparesseveralofthesetechniques,\n",
            "includingconjugategradientdescentandtheBFGSmethod(seealsothecloselyrelated\n",
            "limited-memoryBFGSmethod,knownasL-BFGS).Anothertechniquewhichhasrecently\n",
            "shownpromisingresults38isNesterov’sacceleratedgradienttechnique,whichimproveson\n",
            "themomentumtechnique. However,formanyproblems,plainstochasticgradientdescent\n",
            "workswell,especiallyifmomentumisused,andsowe’llsticktostochasticgradientdescent\n",
            "throughtheremainderofthisbook. Othermodelsofartificialneuron\n",
            "Uptonowwe’vebuiltourneuralnetworksusingsigmoidneurons. Inprinciple,anetwork\n",
            "builtfromsigmoidneuronscancomputeanyfunction. Inpractice,however,networksbuilt\n",
            "usingothermodelneuronssometimesoutperformsigmoidnetworks. Dependingonthe\n",
            "application,networksbasedonsuchalternatemodelsmaylearnfaster,generalizebetterto\n",
            "testdata,orperhapsdoboth. Letmementionacoupleofalternatemodelneurons,togive\n",
            "youtheflavorofsomevariationsincommonuse. Perhapsthesimplestvariationisthetanh(pronounced“tanch”)neuron,whichreplaces\n",
            "thesigmoidfunctionbythehyperbolictangentfunction. Theoutputofatanhneuronwith\n",
            "input x,weightvectorw,andbias bisgivenby\n",
            "tanh(w x+b), (3.54)\n",
            "·\n",
            "wheretanhis,ofcourse,thehyperbolictangentfunction. Itturnsoutthatthisisveryclosely\n",
            "37EfficientBackProp,byYannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller(1998).\n",
            "38See,forexample,Ontheimportanceofinitializationandmomentumindeeplearning,byIlya\n",
            "Sutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "122 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "relatedtothesigmoidneuron. Toseethis,recallthatthetanhfunctionisdefinedby\n",
            "ez e z\n",
            "tanh(z)\n",
            "−\n",
            "− . (3.55)\n",
            "≡\n",
            "ez+e z\n",
            "−\n",
            "Withalittlealgebraitcaneasilybeverifiedthat\n",
            "3\n",
            "σ (z)=\n",
            "1+tanh(z/2)\n",
            ", (3.56)\n",
            "2\n",
            "thatis,tanhisjustarescaledversionofthesigmoidfunction. Wecanalsoseegraphically\n",
            "thatthetanhfunctionhasthesameshapeasthesigmoidfunction,\n",
            "tanhfunction\n",
            "1\n",
            "0.5\n",
            "4 2 2 4\n",
            "− −\n",
            "0.5\n",
            "−\n",
            "1\n",
            "−\n",
            "Onedifferencebetweentanhneuronsandsigmoidneuronsisthattheoutputfromtanh\n",
            "neuronsrangesfrom 1to1,not0to1. Thismeansthatifyou’regoingtobuildanetwork\n",
            "basedontanhneuron−syoumayneedtonormalizeyouroutputs(and,dependingonthe\n",
            "detailsoftheapplication,possiblyyourinputs)alittledifferentlythaninsigmoidnetworks. Similartosigmoidneurons,anetworkoftanhneuronscan,inprinciple,computeany\n",
            "function39mappinginputstotherange 1to1. Furthermore,ideassuchasbackpropagation\n",
            "andstochasticgradientdescentareas−easilyappliedtoanetworkoftanhneuronsastoa\n",
            "networkofsigmoidneurons. Exercise\n",
            "ProvetheidentityinEquation(3.56). Whic•htypeofneuronshouldyouuseinyournetworks,thetanhorsigmoid?\n",
            "Apriorithe\n",
            "answerisnotobvious,toputitmildly! However,therearetheoreticalargumentsandsome\n",
            "empiricalevidencetosuggestthatthetanhsometimesperformsbetter40. Letmebriefly\n",
            "giveyoutheflavorofoneofthetheoreticalargumentsfortanhneurons. Supposewe’re\n",
            "usingsigmoidneurons, soallactivationsinournetworkarepositive. Let’sconsiderthe\n",
            "weightswl\n",
            "j\n",
            "+\n",
            "k\n",
            "1inputtothe j-thneuroninthe(l+1)-thlayer. Therulesforbackpropagation\n",
            "tellusthattheassociatedgradientwillbealδl+1. Becausetheactivationsarepositivethe\n",
            "k j\n",
            "39Therearesometechnicalcaveatstothisstatementforbothtanhandsigmoidneurons,aswellas\n",
            "fortherectifiedlinearneuronsdiscussedbelow.However,informallyit’susuallyfinetothinkofneural\n",
            "networksasbeingabletoapproximateanyfunctiontoarbitraryaccuracy. 40See,forexample,EfficientBackProp,byYannLeCun,LéonBottou,GenevieveOrrandKlaus-Robert\n",
            "Müller(1998),andUnderstandingthedifficultyoftrainingdeepfeedforwardnetworks,byXavierGlorot\n",
            "andYoshuaBengio(2010).\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 123\n",
            "(cid:12)\n",
            "signofthisgradientwillbethesameasthesignofδl+1. Whatthismeansisthatifδl+1\n",
            "j j\n",
            "ispositivethenalltheweightswl+1willdecreaseduringgradientdescent,whileifδl+1is\n",
            "jk j\n",
            "negativethenalltheweightswl+1 willincreaseduringgradientdescent. Inotherwords,\n",
            "jk\n",
            "allweightstothesameneuronmusteitherincreasetogetherordecreasetogether. That’s\n",
            "aproblem,sincesomeoftheweightsmayneedtoincreasewhileothersneedtodecrease. Thatcanonlyhappenifsomeoftheinputactivationshavedifferentsigns. Thatsuggests\n",
            "replacingthesigmoidbyanactivationfunction,suchastanh,whichallowsbothpositiveand 3\n",
            "negativeactivations. Indeed,becausetanhissymmetricaboutzero,tanh( z)= tanh(z),\n",
            "wemightevenexpectthat, roughlyspeaking, theactivationsinhiddenl−ayers−wouldbe\n",
            "equallybalancedbetweenpositiveandnegative. Thatwouldhelpensurethatthereisno\n",
            "systematicbiasfortheweightupdatestobeonewayortheother.\n",
            "Howseriouslyshouldwetakethisargument? Whiletheargumentissuggestive,it’sa\n",
            "heuristic,notarigorousproofthattanhneuronsoutperformsigmoidneurons. Perhapsthere\n",
            "areotherpropertiesofthesigmoidneuronwhichcompensateforthisproblem? Indeed,\n",
            "formanytasksthetanhisfoundempiricallytoprovideonlyasmallornoimprovementin\n",
            "performanceoversigmoidneurons. Unfortunately,wedon’tyethavehard-and-fastrulesto\n",
            "knowwhichneurontypeswilllearnfastest,orgivethebestgeneralizationperformance,for\n",
            "anyparticularapplication. Anothervariationonthesigmoidneuronistherectifiedlinearneuronorrectifiedlinear\n",
            "unit. Theoutputofarectifiedlinearunitwithinput x,weightvectorw,andbias bisgiven\n",
            "by\n",
            "max(0,w x+b). (3.57)\n",
            "·\n",
            "Graphically,therectifyingfunctionmax(0,z)lookslikethis:\n",
            "max(0,z)\n",
            "4\n",
            "2\n",
            "z\n",
            "0\n",
            "4 2 0 2 4\n",
            "− −\n",
            "2\n",
            "−\n",
            "4\n",
            "−\n",
            "Obviouslysuchneuronsarequitedifferentfrombothsigmoidandtanhneurons. However,\n",
            "likethesigmoidandtanhneurons,rectifiedlinearunitscanbeusedtocomputeanyfunction,\n",
            "andtheycanbetrainedusingideassuchasbackpropagationandstochasticgradientdescent. Whenshouldyouuserectifiedlinearunitsinsteadofsigmoidortanhneurons?\n",
            "Some\n",
            "\n",
            "(cid:12)\n",
            "124 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "recentworkonimagerecognition41hasfoundconsiderablebenefitinusingrectifiedlinear\n",
            "unitsthroughmuchofthenetwork. However,aswithtanhneurons,wedonotyethavea\n",
            "reallydeepunderstandingofwhen,exactly,rectifiedlinearunitsarepreferable,norwhy. To\n",
            "giveyoutheflavorofsomeoftheissues,recallthatsigmoidneuronsstoplearningwhen\n",
            "theysaturate,i.e.,whentheiroutputisneareither0or1. Aswe’veseenrepeatedlyinthis\n",
            "chapter,theproblemisthatσ termsreducethegradient,andthatslowsdownlearning. (cid:48)\n",
            "3 Tanhneuronssufferfromasimilarproblemwhentheysaturate. Bycontrast,increasing\n",
            "theweightedinputtoarectifiedlinearunitwillnevercauseittosaturate,andsothere\n",
            "isnocorrespondinglearningslowdown. Ontheotherhand,whentheweightedinputto\n",
            "arectifiedlinearunitisnegative,thegradientvanishes,andsotheneuronstopslearning\n",
            "entirely. Thesearejusttwoofthemanyissuesthatmakeitnon-trivialtounderstandwhen\n",
            "andwhyrectifiedlinearunitsperformbetterthansigmoidortanhneurons. I’vepaintedapictureofuncertaintyhere,stressingthatwedonotyethaveasolidtheory\n",
            "ofhowactivationfunctionsshouldbechosen. Indeed,theproblemishardereventhanI\n",
            "havedescribed, forthereareinfinitelymanypossibleactivationfunctions. Whichisthe\n",
            "bestforanygivenproblem? Whichwillresultinanetworkwhichlearnsfastest? Which\n",
            "willgivethehighesttestaccuracies? Iamsurprisedhowlittlereallydeepandsystematic\n",
            "investigationhasbeendoneofthesequestions. Ideally,we’dhaveatheorywhichtellsus,in\n",
            "detail,howtochoose(andperhapsmodify-on-the-fly)ouractivationfunctions. Ontheother\n",
            "hand,weshouldn’tletthelackofafulltheorystopus!\n",
            "Wehavepowerfultoolsalreadyat\n",
            "hand,andcanmakealotofprogresswiththosetools. Throughtheremainderofthisbook\n",
            "I’llcontinuetousesigmoidneuronsasourgo-toneuron,sincethey’repowerfulandprovide\n",
            "concreteillustrationsofthecoreideasaboutneuralnets. Butkeepinthebackofyourmind\n",
            "thatthesesameideascanbeappliedtoothertypesofneuron,andthattherearesometimes\n",
            "advantagesindoingso. Onstoriesinneuralnetworks\n",
            "Question:Howdoyouapproachutilizingandresearchingmachinelearning\n",
            "techniques that are supported almost entirely empirically, as opposed to\n",
            "mathematically? Alsoinwhatsituationshaveyounoticedsomeofthese\n",
            "techniquesfail?\n",
            "Answer: You have to realize that our theoretical tools are very weak. Sometimes,wehavegoodmathematicalintuitionsforwhyaparticular\n",
            "techniqueshouldwork. Sometimesourintuitionendsupbeingwrong[...]\n",
            "Thequestionsbecome: howwelldoesmymethodworkonthisparticular\n",
            "problem,andhowlargeisthesetofproblemsonwhichitworkswell. —QuestionandanswerwithneuralnetworksresearcherYannLeCun\n",
            "Once,attendingaconferenceonthefoundationsofquantummechanics,Inoticedwhat\n",
            "seemedtomeamostcuriousverbalhabit: whentalksfinished,questionsfromtheaudience\n",
            "41See,forexample,WhatistheBestMulti-StageArchitectureforObjectRecognition?,byKevinJarrett,\n",
            "KorayKavukcuoglu,Marc’AurelioRanzatoandYannLeCun(2009),,byXavierGlorot,AntoineBordes,\n",
            "andYoshuaBengio(2011),andImageNetClassificationwithDeepConvolutionalNeuralNetworks,by\n",
            "AlexKrizhevsky,IlyaSutskever,andGeoffreyHinton(2012).Notethatthesepapersfillinimportant\n",
            "detailsabouthowtosetuptheoutputlayer,costfunction,andregularizationinnetworksusingrectified\n",
            "linearunits. I’veglossedoverallthesedetailsinthisbriefaccount.\n",
            "Thepapersalsodiscussinmore\n",
            "detailthebenefitsanddrawbacksofusingrectifiedlinearunits.AnotherinformativepaperisRectified\n",
            "LinearUnitsImproveRestrictedBoltzmannMachines,byVinodNairandGeoffreyHinton(2010),which\n",
            "demonstratesthebenefitsofusingrectifiedlinearunitsinasomewhatdifferentapproachtoneural\n",
            "networks. \n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 125\n",
            "(cid:12)\n",
            "oftenbeganwith“I’mverysympathetictoyourpointofview,but[...]”. Quantumfoundations\n",
            "wasnotmyusualfield,andInoticedthisstyleofquestioningbecauseatotherscientific\n",
            "conferencesI’drarelyorneverheardaquestionerexpresstheirsympathyforthepointof\n",
            "viewofthespeaker. Atthetime,Ithoughttheprevalenceofthequestionsuggestedthat\n",
            "littlegenuineprogresswasbeingmadeinquantumfoundations,andpeopleweremerely\n",
            "spinningtheirwheels. Later,Irealizedthatassessmentwastooharsh. Thespeakerswere\n",
            "wrestlingwithsomeofthehardestproblemshumanmindshaveeverconfronted. Ofcourse 3\n",
            "progresswasslow! Buttherewasstillvalueinhearingupdatesonhowpeoplewerethinking,\n",
            "eveniftheydidn’talwayshaveunarguablenewprogresstoreport.\n",
            "Youmayhavenoticedaverbalticsimilarto“I’mverysympathetic[...]” inthecurrent\n",
            "book. Toexplainwhatwe’reseeingI’veoftenfallenbackonsaying“Heuristically,[...]”,or\n",
            "“Roughlyspeaking,[...]”,followingupwithastorytoexplainsomephenomenonorother. Thesestoriesareplausible,buttheempiricalevidenceI’vepresentedhasoftenbeenpretty\n",
            "thin. Ifyoulookthroughtheresearchliteratureyou’llseethatstoriesinasimilarstyleappear\n",
            "inmanyresearchpapersonneuralnets,oftenwiththinsupportingevidence. Whatshould\n",
            "wethinkaboutsuchstories? Inmanypartsofscience–especiallythosepartsthatdealwithsimplephenomena–it’s\n",
            "possibletoobtainverysolid,veryreliableevidenceforquitegeneralhypotheses. Butin\n",
            "neuralnetworkstherearelargenumbersofparametersandhyper-parameters,andextremely\n",
            "complexinteractionsbetweenthem. Insuchextraordinarilycomplexsystemsit’sexceedingly\n",
            "difficulttoestablishreliablegeneralstatements. Understandingneuralnetworksintheirfull\n",
            "generalityisaproblemthat,likequantumfoundations,teststhelimitsofthehumanmind. Instead,weoftenmakedowithevidencefororagainstafewspecificinstancesofageneral\n",
            "statement. Asaresultthosestatementssometimeslaterneedtobemodifiedorabandoned,\n",
            "whennewevidencecomestolight.\n",
            "Onewayofviewingthissituationisthatanyheuristicstoryaboutneuralnetworkscarries\n",
            "withitanimpliedchallenge. Forexample,considerthestatementIquotedearlier,explaining\n",
            "whydropoutworks42: “Thistechniquereducescomplexco-adaptationsofneurons,sincea\n",
            "neuroncannotrelyonthepresenceofparticularotherneurons. Itis,therefore,forcedto\n",
            "learnmorerobustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsets\n",
            "oftheotherneurons.” Thisisarich,provocativestatement,andonecouldbuildafruitful\n",
            "researchprogramentirelyaroundunpackingthestatement,figuringoutwhatinitistrue,\n",
            "whatisfalse,whatneedsvariationandrefinement. Indeed,thereisnowasmallindustryof\n",
            "researcherswhoareinvestigatingdropout(andmanyvariations),tryingtounderstandhow\n",
            "itworks,andwhatitslimitsare. Andsoitgoeswithmanyoftheheuristicswe’vediscussed. Eachheuristicisnotjusta(potential)explanation,it’salsoachallengetoinvestigateand\n",
            "understandinmoredetail. Of course, there is not time for any single person to investigate all these heuristic\n",
            "explanationsindepth. It’sgoingtotakedecades(orlonger)forthecommunityofneural\n",
            "networksresearcherstodevelopareallypowerful,evidence-basedtheoryofhowneural\n",
            "networkslearn. Doesthismeanyoushouldrejectheuristicexplanationsasunrigorous,and\n",
            "notsufficientlyevidence-based?\n",
            "No! Infact,weneedsuchheuristicstoinspireandguide\n",
            "ourthinking. It’slikethegreatageofexploration: theearlyexplorerssometimesexplored\n",
            "(andmadenewdiscoveries)onthebasisofbeliefswhichwerewronginimportantways.\n",
            "Later,thosemistakeswerecorrectedaswefilledinourknowledgeofgeography. Whenyou\n",
            "42FromImageNetClassificationwithDeepConvolutionalNeuralNetworks,byAlexKrizhevsky,Ilya\n",
            "Sutskever,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "126 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "understandsomethingpoorly–astheexplorersunderstoodgeography,andasweunderstand\n",
            "neuralnetstoday–it’smoreimportanttoexploreboldlythanitistoberigorouslycorrect\n",
            "ineverystepofyourthinking. Andsoyoushouldviewthesestoriesasausefulguideto\n",
            "howtothinkaboutneuralnets,whileretainingahealthyawarenessofthelimitationsof\n",
            "suchstories,andcarefullykeepingtrackofjusthowstrongtheevidenceisforanygivenline\n",
            "ofreasoning. Putanotherway,weneedgoodstoriestohelpmotivateandinspireus,and\n",
            "3 rigorousin-depthinvestigationinordertouncovertherealfactsofthematter.\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 127\n",
            "(cid:12)\n",
            "4444\n",
            "A visual proof that neural nets\n",
            "can compute any function\n",
            "4\n",
            "Oneofthemoststrikingfactsaboutneuralnetworksisthattheycancomputeanyfunction\n",
            "atall. Thatis,supposesomeonehandsyousomecomplicated,wigglyfunction, f(x):\n",
            "f(x)\n",
            "Nomatterwhatthefunction,thereisguaranteedtobeaneuralnetworksothatforevery\n",
            "possibleinput, x,thevalue f(x)(orsomecloseapproximation)isoutputfromthenetwork,\n",
            "e.g.:\n",
            "\n",
            "(cid:12)\n",
            "128 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4 Thisresultholdsevenifthefunctionhasmanyinputs, f = f(x 1 ,...,x m),andmanyoutputs. Forinstance,here’sanetworkcomputingafunctionwithm=3inputsandn=2outputs:\n",
            "Thisresulttellsusthatneuralnetworkshaveakindofuniversality.\n",
            "Nomatterwhatfunction\n",
            "wewanttocompute,weknowthatthereisaneuralnetworkwhichcandothejob. What’smore,thisuniversalitytheoremholdsevenifwerestrictournetworkstohave\n",
            "justasinglelayerintermediatebetweentheinputandtheoutputneurons–aso-calledsingle\n",
            "hiddenlayer. Soevenverysimplenetworkarchitecturescanbeextremelypowerful. Theuniversalitytheoremiswellknownbypeoplewhouseneuralnetworks. Butwhyit’s\n",
            "trueisnotsowidelyunderstood. Mostoftheexplanationsavailablearequitetechnical. For\n",
            "instance,oneoftheoriginalpapersprovingtheresult1didsousingtheHahn-Banachtheorem,\n",
            "theRieszRepresentationtheorem,andsomeFourieranalysis. Ifyou’reamathematicianthe\n",
            "argumentisnotdifficulttofollow,butit’snotsoeasyformostpeople. That’sapity,since\n",
            "theunderlyingreasonsforuniversalityaresimpleandbeautiful. InthischapterIgiveasimpleandmostlyvisualexplanationoftheuniversalitytheorem.\n",
            "We’llgostepbystepthroughtheunderlyingideas. You’llunderstandwhyit’struethatneural\n",
            "networkscancomputeanyfunction. You’llunderstandsomeofthelimitationsoftheresult.\n",
            "Andyou’llunderstandhowtheresultrelatestodeepneuralnetworks. Tofollowthematerialinthechapter, youdonotneedtohavereadearlierchapters\n",
            "inthisbook. Instead, thechapterisstructuredtobeenjoyableasaself-containedessay. Providedyouhavejustalittlebasicfamiliaritywithneuralnetworks,youshouldbeableto\n",
            "followtheexplanation. Iwill,however,provideoccasionallinkstoearliermaterial,tohelp\n",
            "fillinanygapsinyourknowledge. 1Approximationbysuperpositionsofasigmoidalfunction,byGeorgeCybenko(1989).Theresult\n",
            "wasverymuchintheairatthetime,andseveralgroupsprovedcloselyrelatedresults. Cybenko’s\n",
            "papercontainsausefuldiscussionofmuchofthatwork.AnotherimportantearlypaperisMultilayer\n",
            "feedforwardnetworksareuniversalapproximators,byKurtHornik,MaxwellStinchcombe,andHalbert\n",
            "White(1989).ThispaperusestheStone-Weierstrasstheoremtoarriveatsimilarresults. \n",
            "(cid:12)\n",
            "4.1. Twocaveats (cid:12) 129\n",
            "(cid:12)\n",
            "Universality theorems are a commonplace in computer science, so much so that we\n",
            "sometimesforgethowastonishingtheyare. Butit’sworthremindingourselves: theabilityto\n",
            "computeanarbitraryfunctionistrulyremarkable.\n",
            "Almostanyprocessyoucanimaginecan\n",
            "bethoughtofasfunctioncomputation. Considertheproblemofnamingapieceofmusic\n",
            "basedonashortsampleofthepiece. Thatcanbethoughtofascomputingafunction.\n",
            "Or\n",
            "considertheproblemoftranslatingaChinesetextintoEnglish. Again,thatcanbethought\n",
            "of as computing a function2. Or consider the problem of taking an mp4 movie file and\n",
            "generatingadescriptionoftheplotofthemovie,andadiscussionofthequalityoftheacting. Again,thatcanbethoughtofasakindoffunctioncomputation3Universalitymeansthat,in\n",
            "principle,neuralnetworkscandoallthesethingsandmanymore. 4\n",
            "Ofcourse,justbecauseweknowaneuralnetworkexiststhatcan(say)translateChinese\n",
            "text into English, that doesn’t mean we have good techniques for constructing or even\n",
            "recognizingsuchanetwork. Thislimitationappliesalsototraditionaluniversalitytheorems\n",
            "formodelssuchasBooleancircuits. But,aswe’veseenearlierinthebook,neuralnetworks\n",
            "havepowerfulalgorithmsforlearningfunctions. Thatcombinationoflearningalgorithms+\n",
            "universalityisanattractivemix. Uptonow,thebookhasfocusedonthelearningalgorithms.\n",
            "Inthischapter,wefocusonuniversality,andwhatitmeans. 4.1 Two caveats\n",
            "Beforeexplainingwhytheuniversalitytheoremistrue,Iwanttomentiontwocaveatstothe\n",
            "informalstatement“aneuralnetworkcancomputeanyfunction”. First,thisdoesn’tmeanthatanetworkcanbeusedtoexactlycomputeanyfunction. Rather,wecangetanapproximationthatisasgoodaswewant. Byincreasingthenumberof\n",
            "hiddenneuronswecanimprovetheapproximation. Forinstance,earlier(see4)Iillustrated\n",
            "anetworkcomputingsomefunction f(x)usingthreehiddenneurons. Formostfunctions\n",
            "onlyalow-qualityapproximationwillbepossibleusingthreehiddenneurons. Byincreasing\n",
            "thenumberofhiddenneurons(say,tofive)wecantypicallygetabetterapproximation:\n",
            "Andwecandostillbetterbyfurtherincreasingthenumberofhiddenneurons. 2Actually,computingoneofmanyfunctions,sincethereareoftenmanyacceptabletranslationsofa\n",
            "givenpieceoftext. 3Dittotheremarkabouttranslationandtherebeingmanypossiblefunctions.. \n",
            "(cid:12)\n",
            "130 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Tomakethisstatementmoreprecise,supposewe’regivenafunction f(x)whichwe’d\n",
            "liketocomputetowithinsomedesiredaccuracyε>0. Theguaranteeisthatbyusing\n",
            "enoughhiddenneuronswecanalwaysfindaneuralnetworkwhoseoutput g(x)satisfies\n",
            "g(x) f(x) <ε,forallinputsx. Inotherwords,theapproximationwillbegoodtowithin\n",
            "|thede−sireda|ccuracyforeverypossibleinput. Thesecondcaveatisthattheclassoffunctionswhichcanbeapproximatedintheway\n",
            "describedarethecontinuousfunctions. Ifafunctionisdiscontinuous,i.e.,makessudden,\n",
            "sharpjumps,thenitwon’tingeneralbepossibletoapproximateusinganeuralnet. This\n",
            "isnotsurprising,sinceourneuralnetworkscomputecontinuousfunctionsoftheirinput. 4 However,evenifthefunctionwe’dreallyliketocomputeisdiscontinuous,it’softenthe\n",
            "casethatacontinuousapproximationisgoodenough. Ifthat’sso,thenwecanuseaneural\n",
            "network.\n",
            "Inpractice,thisisnotusuallyanimportantlimitation. Summingup,amoreprecisestatementoftheuniversalitytheoremisthatneuralnetworks\n",
            "withasinglehiddenlayercanbeusedtoapproximateanycontinuousfunctiontoanydesired\n",
            "precision. Inthischapterwe’llactuallyproveaslightlyweakerversionofthisresult,using\n",
            "twohiddenlayersinsteadofone.\n",
            "IntheproblemsI’llbrieflyoutlinehowtheexplanation\n",
            "can,withafewtweaks,beadaptedtogiveaproofwhichusesonlyasinglehiddenlayer. 4.2 Universality with one input and one output\n",
            "Tounderstandwhytheuniversalitytheoremistrue, let’sstartbyunderstandinghowto\n",
            "construct a neural network which approximates a function with just one input and one\n",
            "output:\n",
            "f(x)\n",
            "It turns out that this is the core of the problem of universality. Once we’ve understood\n",
            "thisspecialcaseit’sactuallyprettyeasytoextendtofunctionswithmanyinputsandmany\n",
            "outputs. Tobuildinsightintohowtoconstructanetworktocompute f,let’sstartwithanetwork\n",
            "containingjustasinglehiddenlayer,withtwohiddenneurons,andanoutputlayercontaining\n",
            "asingleoutputneuron:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 131\n",
            "(cid:12)\n",
            "4\n",
            "Togetafeelforhowcomponentsinthenetworkwork,let’sfocusonthetophiddenneuron. Inthediagrambelow,clickontheweight,w,anddragthemousealittlewaystotherightto\n",
            "increasew. Youcanimmediatelyseehowthefunctioncomputedbythetophiddenneuron\n",
            "changes:\n",
            "Outputfromneuron\n",
            "1 w=7,b= 2\n",
            "w=7,b= −3\n",
            "w=7,b= −4\n",
            "w=7,b= −5\n",
            "w=7,b= −6\n",
            "−\n",
            "0 1\n",
            "Outputfromneuron\n",
            "1 w=9,b= 4\n",
            "w=8,b= −4\n",
            "w=7,b= −4\n",
            "w=6,b= −4\n",
            "w=5,b= −4\n",
            "−\n",
            "0 1\n",
            "Aswelearntearlierinthebook,what’sbeingcomputedbythehiddenneuronisσ (wx+b),\n",
            "whereσ (z) 1/ (1+e\n",
            "−\n",
            "z )isthesigmoidfunction. Uptonow,we’vemadefrequentuseofthis\n",
            "algebraicfor≡m.\n",
            "Butfortheproofofuniversalitywewillobtainmoreinsightbyignoringthe\n",
            "algebraentirely,andinsteadmanipulatingandobservingtheshapeshowninthegraph. This\n",
            "\n",
            "(cid:12)\n",
            "132 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "won’tjustgiveusabetterfeelforwhat’sgoingon,itwillalsogiveusaproof4ofuniversality\n",
            "thatappliestoactivationfunctionsotherthanthesigmoidfunction. Wecansimplifyour\n",
            "analysisquiteabitbyincreasingtheweightsomuchthattheoutputreallyisastepfunction,\n",
            "toaverygoodapproximation. BelowI’veplottedtheoutputfromthetophiddenneuron\n",
            "whentheweightisw=999. Outputfromtophiddenneuron\n",
            "4\n",
            "1\n",
            "x\n",
            "1\n",
            "It’sactuallyquiteabiteasiertoworkwithstepfunctionsthangeneralsigmoidfunctions. Thereasonisthatintheoutputlayerweaddupcontributionsfromallthehiddenneurons. It’seasytoanalyzethesumofabunchofstepfunctions,butrathermoredifficulttoreason\n",
            "aboutwhathappenswhenyouaddupabunchofsigmoidshapedcurves. Andsoitmakes\n",
            "thingsmucheasiertoassumethatourhiddenneuronsareoutputtingstepfunctions. More\n",
            "concretely,wedothisbyfixingtheweightwtobesomeverylargevalue,andthensettingthe\n",
            "positionofthestepbymodifyingthebias. Ofcourse,treatingtheoutputasastepfunction\n",
            "isanapproximation,butit’saverygoodapproximation,andfornowwe’lltreatitasexact. I’llcomebacklatertodiscusstheimpactofdeviationsfromthisapproximation. Atwhatvalueof x doesthestepoccur? Putanotherway,howdoesthepositionofthe\n",
            "stepdependupontheweightandbias? Toanswerthisquestion,trymodifyingtheweightandbiasinthediagramabove(you\n",
            "mayneedtoscrollbackabit). Canyoufigureouthowthepositionofthestepdependsonw\n",
            "and b?\n",
            "Withalittleworkyoushouldbeabletoconvinceyourselfthatthepositionofthe\n",
            "stepisproportionalto b,andinverselyproportionaltow. Infact,thestepisatpositions= b/w,asyoucanseebymodifyingtheweightand\n",
            "biasinthefollowingdiagram: −\n",
            "4Strictlyspeaking,thevisualapproachI’mtakingisn’twhat’straditionallythoughtofasaproof.But\n",
            "Ibelievethevisualapproachgivesmoreinsightintowhytheresultistruethanatraditionalproof.And,\n",
            "ofcourse,thatkindofinsightistherealpurposebehindaproof.Occasionally,therewillbesmallgapsin\n",
            "thereasoningIpresent:placeswhereImakeavisualargumentthatisplausible,butnotquiterigorous. Ifthisbothersyou,thenconsideritachallengetofillinthemissingsteps.Butdon’tlosesightofthe\n",
            "realpurpose:tounderstandwhytheuniversalitytheoremistrue.\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 133\n",
            "(cid:12)\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "b/w=0.4 1\n",
            "4\n",
            "−\n",
            "Itwillgreatlysimplifyourlivestodescribehiddenneuronsusingjustasingleparameter,s,\n",
            "whichisthestepposition,s= b/w. Trymodifyingsinthefollowingdiagram,inorderto\n",
            "getusedtothenewparameter−ization:\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "s 1\n",
            "Asnotedabove,we’veimplicitlysettheweightwontheinputtobesomelargevalue–big\n",
            "enoughthatthestepfunctionisaverygoodapproximation. Wecaneasilyconvertaneuron\n",
            "parameterizedinthiswaybackintotheconventionalmodel,bychoosingthebias b= ws. −\n",
            "Uptonowwe’vebeenfocusingontheoutputfromjustthetophiddenneuron.\n",
            "Let’stake\n",
            "alookatthebehavioroftheentirenetwork. Inparticular,we’llsupposethehiddenneurons\n",
            "arecomputingstepfunctionsparameterizedbysteppointss (topneuron)ands (bottom\n",
            "1 2\n",
            "neuron). Andthey’llhaverespectiveoutputweightsw andw . Here’sthenetwork:\n",
            "1 2\n",
            "Weightedoutputfromhiddenlayer\n",
            "w 1+w\n",
            "2\n",
            "1\n",
            "w\n",
            "1\n",
            "x\n",
            "s 1 s 2 1\n",
            "What’sbeingplottedontherightistheweightedoutputw\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a\n",
            "2\n",
            "fromthehiddenlayer. \n",
            "(cid:12)\n",
            "134 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Here, a and a aretheoutputsfromthetopandbottomhiddenneurons, respectively5. 1 2\n",
            "Theseoutputsaredenotedwithasbecausethey’reoftenknownastheneurons’activations. Tryincreasinganddecreasingthesteppoints ofthetophiddenneuron. Getafeel\n",
            "1\n",
            "forhowthischangestheweightedoutputfromthehiddenlayer. It’sparticularlyworth\n",
            "understandingwhathappenswhens goespasts . You’llseethatthegraphchangesshape\n",
            "1 2\n",
            "whenthishappens,sincewehavemovedfromasituationwherethetophiddenneuron\n",
            "isthefirsttobeactivatedtoasituationwherethebottomhiddenneuronisthefirsttobe\n",
            "activated. Similarly,trymanipulatingthesteppoints ofthebottomhiddenneuron,andgetafeel\n",
            "2\n",
            "forhowthischangesthecombinedoutputfromthehiddenneurons. 4\n",
            "Tryincreasinganddecreasingeachoftheoutputweights. Noticehowthisrescalesthe\n",
            "contributionfromtherespectivehiddenneurons. Whathappenswhenoneoftheweightsis\n",
            "zero?\n",
            "Finally,trysettingw tobe0.8andw tobe-0.8. Yougeta“bump”function,which\n",
            "1 2\n",
            "startsatpoints ,endsatpoints ,andhasheight0.8. Forinstance,theweightedoutput\n",
            "1 2\n",
            "mightlooklikethis:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "w 1= w 2\n",
            "−\n",
            "x\n",
            "s 1 s 2 1\n",
            "Ofcourse,wecanrescalethebumptohaveanyheightatall. Let’suseasingleparameter,h,\n",
            "todenotetheheight. ToreduceclutterI’llalsoremovethe“s 1=...”and“w 1=...”notations. Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "x\n",
            "s 1 s 2 1\n",
            "1\n",
            "−\n",
            "Trychangingthevalueofhupanddown,toseehowtheheightofthebumpchanges. Try\n",
            "changingtheheightsoit’snegative,andobservewhathappens.\n",
            "Andtrychangingthestep\n",
            "pointstoseehowthatchangestheshapeofthebump. 5Note,bytheway,thattheoutputfromthewholenetworkisσ (w\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a 2+b),wherebisthe\n",
            "biasontheoutputneuron.Obviously,thisisn’tthesameastheweightedoutputfromthehiddenlayer,\n",
            "whichiswhatwe’replottinghere.We’regoingtofocusontheweightedoutputfromthehiddenlayer\n",
            "rightnow,andonlylaterwillwethinkabouthowthatrelatestotheoutputfromthewholenetwork. \n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 135\n",
            "(cid:12)\n",
            "You’llnotice,bytheway,thatwe’reusingourneuronsinawaythatcanbethought\n",
            "ofnotjustingraphicalterms,butinmoreconventionalprogrammingterms,asakindof\n",
            "if-then-elsestatement,e.g.:\n",
            "if input >= step point:\n",
            "add 1 to the weighted output\n",
            "else:\n",
            "add 0 to the weighted output\n",
            "4\n",
            "ForthemostpartI’mgoingtostickwiththegraphicalpointofview. Butinwhatfollowsyou\n",
            "maysometimesfindithelpfultoswitchpointsofview,andthinkaboutthingsintermsof\n",
            "if-then-else. Wecanuseourbump-makingtricktogettwobumps,bygluingtwopairsofhidden\n",
            "neuronstogetherintothesamenetwork:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "2\n",
            "0\n",
            "1\n",
            "−h\n",
            "1\n",
            "0 s1 s1s2 s21\n",
            "1 2 1 2\n",
            "I’vesuppressedtheweightshere,simplywritingthehvaluesforeachpairofhiddenneurons. Tryincreasinganddecreasingbothhvalues,andobservehowitchangesthegraph.\n",
            "Move\n",
            "thebumpsaroundbychangingthesteppoints. Moregenerally,wecanusethisideatogetasmanypeaksaswewant,ofanyheight. In\n",
            "particular,wecandividetheinterval[0,1]upintoalargenumber,N,ofsubintervals,and\n",
            "useN pairsofhiddenneuronstosetuppeaksofanydesiredheight. Let’sseehowthisworks\n",
            "forN=5. That’squiteafewneurons,soI’mgoingtopackthingsinabit. Apologiesforthe\n",
            "complexityofthediagram: Icouldhidethecomplexitybyabstractingawayfurther,butI\n",
            "thinkit’sworthputtingupwithalittlecomplexity,forthesakeofgettingamoreconcrete\n",
            "feelforhowthesenetworkswork. \n",
            "(cid:12)\n",
            "136 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Weightedoutput\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "1\n",
            "−\n",
            "Youcanseethattherearefivepairsofhiddenneurons. Thesteppointsfortherespective\n",
            "pairsofneuronsare0,1/5,then1/5,2/5,andsoon,outto4/5,5/5. Thesevaluesarefixed\n",
            "–theymakeitsowegetfiveevenlyspacedbumpsonthegraph.\n",
            "Eachpairofneuronshasavalueofhassociatedtoit. Remember,theconnectionsoutput\n",
            "fromtheneuronshaveweightshand h(notmarked). Clickononeofthehvalues,and\n",
            "dragthemousetotherightorlefttoc−hangethevalue. Asyoudoso,watchthefunction\n",
            "change. Bychangingtheoutputweightswe’reactuallydesigningthefunction! Contrariwise,tryclickingonthegraph,anddraggingupordowntochangetheheight\n",
            "ofanyofthebumpfunctions. Asyouchangetheheights,youcanseethecorresponding\n",
            "changeinhvalues. And,althoughit’snotshown,thereisalsoachangeinthecorresponding\n",
            "outputweights,whichare+hand h. −\n",
            "Inotherwords,wecandirectlymanipulatethefunctionappearinginthegraphonthe\n",
            "right,andseethatreflectedinthehvaluesontheleft. Afunthingtodoistoholdthemouse\n",
            "buttondownanddragthemousefromonesideofthegraphtotheother.\n",
            "Asyoudothisyou\n",
            "drawoutafunction,andgettowatchtheparametersintheneuralnetworkadapt. Timeforachallenge. Let’sthinkbacktothefunctionIplottedatthebeginningofthechapter:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 137\n",
            "(cid:12)\n",
            "f(x)\n",
            "4\n",
            "Ididn’tsayitatthetime,butwhatIplottedisactuallythefunction\n",
            "f(x)=0.2+0.4x2 +0.3xsin(15x)+0.05cos(50x), (4.1)\n",
            "plottedover x from0to1,andwiththe y axistakingvaluesfrom0to1. That’sobviouslynotatrivialfunction.\n",
            "You’regoingtofigureouthowtocomputeitusinganeuralnetwork. (cid:80)\n",
            "Inournetworksabovewe’vebeenanalyzingtheweightedcombination w a output\n",
            "j j j\n",
            "fromthehiddenneurons. Wenowknowhowtogetalotofcontroloverthisquantity. But,\n",
            "asInotedearlier,thisquantityisnotwhat’soutputfromthenetwork. What’soutputfrom\n",
            "thenetworkisσ ( (cid:80) j w j a j+b)where bisthebiasontheoutputneuron. Istheresomeway\n",
            "wecanachievecontrolovertheactualoutputfromthenetwork? Thesolutionistodesignaneuralnetworkwhosehiddenlayerhasaweightedoutput\n",
            "givenbyσ\n",
            "−\n",
            "1 f(x),whereσ\n",
            "−\n",
            "1isjusttheinverseoftheσfunction. Thatis,wewantthe\n",
            "weightedoutp◦utfromthehiddenlayertobe:\n",
            "2\n",
            "σ\n",
            "−\n",
            "1 f(x)\n",
            "◦\n",
            "1\n",
            "1\n",
            "1\n",
            "−\n",
            "2\n",
            "−\n",
            "Ifwecandothis,thentheoutputfromthenetworkasawholewillbeagoodapproximation\n",
            "to f(x) 6. Yourchallenge,then,istodesignaneuralnetworktoapproximatethegoalfunction\n",
            "shownjustabove. Tolearnasmuchaspossible,Iwantyoutosolvetheproblemtwice. The\n",
            "firsttime,pleaseclickonthegraph,directlyadjustingtheheightsofthedifferentbump\n",
            "6NotethatIhavesetthebiasontheoutputneuronto0.\n",
            "\n",
            "(cid:12)\n",
            "138 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "functions. Youshouldfinditfairlyeasytogetagoodmatchtothegoalfunction. How\n",
            "wellyou’redoingismeasuredbytheaveragedeviationbetweenthegoalfunctionandthe\n",
            "functionthenetworkisactuallycomputing. Yourchallengeistodrivetheaveragedeviation\n",
            "aslowaspossible. Youcompletethechallengewhenyoudrivetheaveragedeviationto0.40\n",
            "orbelow7\n",
            "4\n",
            "2\n",
            "Weightedoutput\n",
            "σ =0.38\n",
            "1\n",
            "1\n",
            "1\n",
            "−\n",
            "2\n",
            "−\n",
            "You’venowfiguredoutalltheelementsnecessaryforthenetworktoapproximatelycompute\n",
            "thefunction f(x)!\n",
            "It’sonlyacoarseapproximation,butwecouldeasilydomuchbetter,\n",
            "merelybyincreasingthenumberofpairsofhiddenneurons,allowingmorebumps. In particular, it’s easy to convert all the data we have found back into the standard\n",
            "parametrizationusedforneuralnetworks.\n",
            "Letmejustrecapquicklyhowthatworks. Thefirstlayerofweightsallhavesomelarge,constantvalue,sayw=1000. Thebiasesonthehiddenneuronsarejust b= ws. So,forinstance,forthesecond\n",
            "hiddenneurons=0.2becomes b= 1000 0.2= −200. Thefinallayerofweightsarede−termin×edbyth−ehvalues. So,forinstance,thevalue\n",
            "you’vechosenaboveforthefirsth,h= 0.6,meansthattheoutputweightsfromthetop\n",
            "twohiddenneuronsare 0.6and0.6,re−spectively. Andsoon,fortheentirelayerofoutput\n",
            "weights. −\n",
            "Finally,thebiasontheoutputneuronis0. That’severything: wenowhaveacompletedescriptionofaneuralnetworkwhichdoes\n",
            "aprettygoodjobcomputingouroriginalgoalfunction. Andweunderstandhowtoimprove\n",
            "thequalityoftheapproximationbyimprovingthenumberofhiddenneurons. What’smore,therewasnothingspecialaboutouroriginalgoalfunction, f(x)=0.2+\n",
            "0.4x2 +0.3sin(15x)+0.05cos(50x). Wecouldhaveusedthisprocedureforanycontinuous\n",
            "functionfrom[0,1]to[0,1]. Inessence,we’reusingoursingle-layerneuralnetworksto\n",
            "7Thisparagraphreferstointeractiveelement,availableonline.Thegraphshowsthefinalresultof\n",
            "manualminimizationofaveragedeviation. \n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 139\n",
            "(cid:12)\n",
            "buildalookuptableforthefunction. Andwe’llbeabletobuildonthisideatoprovidea\n",
            "generalproofofuniversality. 4.3 Many input variables\n",
            "Let’sextendourresultstothecaseofmanyinputvariables. Thissoundscomplicated,but\n",
            "alltheideasweneedcanbeunderstoodinthecaseofjusttwoinputs.\n",
            "Solet’saddressthe\n",
            "two-inputcase. We’llstartbyconsideringwhathappenswhenwehavetwoinputstoaneuron:\n",
            "Here,wehaveinputs x and y,withcorrespondingweightsw andw ,andabias bonthe\n",
            "1 2\n",
            "neuron. Let’ssettheweightw to0,andthenplayaroundwiththefirstweight,w ,andthe\n",
            "2 1\n",
            "bias, b,toseehowtheyaffecttheoutputfromtheneuron:\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 8\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 2\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=11,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w 1=5,b= 5\n",
            "−\n",
            "\n",
            "(cid:12)\n",
            "140 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Asyoucansee,withw 2=0theinput y makesnodifferencetotheoutputfromtheneuron. It’sasthough x istheonlyinput. Giventhis,whatdoyouthinkhappenswhenweincreasetheweightw\n",
            "1\n",
            "tow 1=100,\n",
            "withw remaining0? Ifyoudon’timmediatelyseetheanswer,ponderthequestionfora\n",
            "2\n",
            "bit,andseeifyoucanfigureoutwhathappens.\n",
            "Thentryitoutandseeifyou’reright. I’ve\n",
            "shownwhathappensinthefollowingmovie:\n",
            "Justasinourearlierdiscussion,astheinputweightgetslargertheoutputapproachesa\n",
            "stepfunction. Thedifferenceisthatnowthestepfunctionisinthreedimensions. Alsoas\n",
            "before,wecanmovethelocationofthesteppointaroundbymodifyingthebias.\n",
            "Theactual\n",
            "locationofthesteppointiss b/w . x 1\n",
            "Let’sredotheaboveusing≡the−positionofthestepastheparameter:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.7\n",
            "Here,weassumetheweightonthe x inputhassomelargevalue–I’veusedw 1=1000–\n",
            "andtheweightw 2=0. Thenumberontheneuronisthesteppoint,andthelittle x above\n",
            "thenumberremindsusthatthestepisinthe x direction. Ofcourse,it’salsopossibleto\n",
            "getastepfunctioninthe y direction,bymakingtheweightonthe y inputverylarge(say,\n",
            "w 2=1000),andtheweightonthe x equalto0,i.e.,w 1=0:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s y=0.7\n",
            "Thenumberontheneuronisagainthesteppoint,andinthiscasethelittle y abovethe\n",
            "numberremindsusthatthestepisinthe y direction. Icouldhaveexplicitlymarkedthe\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 141\n",
            "(cid:12)\n",
            "weightsonthe x and y inputs,butdecidednotto,sinceitwouldmakethediagramrather\n",
            "cluttered. Butdokeepinmindthatthelittle y markerimplicitlytellsusthatthe y weightis\n",
            "large,andthe x weightis0. Wecanusethestepfunctionswe’vejustconstructedtocomputeathree-dimensional\n",
            "bumpfunction. Todothis,weusetwoneurons,eachcomputingastepfunctioninthe x\n",
            "direction. Thenwecombinethosestepfunctionswithweighthand h,respectively,where\n",
            "histhedesiredheightofthebump. It’sallillustratedinthefollowin−gdiagram:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.7,h=1\n",
            "Trychangingthevalueoftheheight,h. Observehowitrelatestotheweightsinthenetwork.\n",
            "Andseehowitchangestheheightofthebumpfunctionontheright. Also,trychangingthesteppoint0.30associatedtothetophiddenneuron. Witnesshow\n",
            "itchangestheshapeofthebump. Whathappenswhenyoumoveitpastthesteppoint0.70\n",
            "associatedtothebottomhiddenneuron? We’vefiguredouthowtomakeabumpfunctioninthe x direction. Ofcourse,wecan\n",
            "easilymakeabumpfunctioninthe ydirection,byusingtwostepfunctionsinthe ydirection.\n",
            "Recallthatwedothisbymakingtheweightlargeonthe y input,andtheweight0onthe x\n",
            "input. Here’stheresult:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.4,s2 y=0.6,h=0.8\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.3,s2 y=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "Thislooksnearlyidenticaltotheearliernetwork! Theonlythingexplicitlyshownaschanging\n",
            "isthatthere’snowlittle y markersonourhiddenneurons. Thatremindsusthatthey’re\n",
            "\n",
            "(cid:12)\n",
            "142 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "producing y stepfunctions,not x stepfunctions,andsotheweightisverylargeonthe y\n",
            "input,andzeroonthex input,notviceversa. Asbefore,Idecidednottoshowthisexplicitly,\n",
            "inordertoavoidclutter. Let’sconsiderwhathappenswhenweadduptwobumpfunctions,\n",
            "oneinthe x direction,theotherinthe y direction,bothofheighth:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=0.6\n",
            "TosimplifythediagramI’vedroppedtheconnectionswithzeroweight. Fornow,I’veleftin\n",
            "thelittlexand ymarkersonthehiddenneurons,toremindyouinwhatdirectionsthebump\n",
            "functionsarebeingcomputed.\n",
            "We’lldropeventhosemarkerslater,sincethey’reimplied\n",
            "bytheinputvariable. Tryvaryingtheparameterh. Asyoucansee,thiscausestheoutput\n",
            "weightstochange,andalsotheheightsofboththe x and y bumpfunctions. Whatwe’ve\n",
            "builtlooksalittlelikeatowerfunction:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "Towerfunction\n",
            "Ifwecouldbuildsuchtowerfunctions,thenwecouldusethemtoapproximatearbitrary\n",
            "functions,justbyaddingupmanytowersofdifferentheights,andindifferentlocations:\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 143\n",
            "(cid:12)\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Ofcourse,wehaven’tyetfiguredouthowtobuildatowerfunction.Whatwehaveconstructed\n",
            "lookslikeacentraltower,ofheight2h,withasurroundingplateau,ofheighth. Butwecanmakeatowerfunction. Rememberthatearlierwesawneuronscanbeused\n",
            "toimplementatypeof’inlineif-then-elsestatement:\n",
            "if input >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "Thatwasforaneuronwithjustasingleinput. Whatwewantistoapplyasimilarideato\n",
            "thecombinedoutputfromthehiddenneurons:\n",
            "if combined output from hidden neurons >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "If we choose the threshold appropriately — say, a value of 3h/2, which is sandwiched\n",
            "betweentheheightoftheplateauandtheheightofthecentraltower–wecouldsquashthe\n",
            "plateaudowntozero,andleavejustthetowerstanding. Canyouseehowtodothis?\n",
            "Tryexperimentingwiththefollowingnetworktofigureit\n",
            "out. Notethatwe’renowplottingtheoutputfromtheentirenetwork,notjusttheweighted\n",
            "outputfromthehiddenlayer. Thismeansweaddabiastermtotheweightedoutputfromthe\n",
            "hiddenlayer,andapplythesigmafunction. Canyoufindvaluesforhand bwhichproduce\n",
            "atower? Thisisabittricky,soifyouthinkaboutthisforawhileandremainstuck,here’s\n",
            "twohints: (1)Togettheoutputneurontoshowtherightkindofif-then-elsebehaviour,\n",
            "weneedtheinputweights(allhor h)tobelarge;and(2)thevalueof bdeterminesthe\n",
            "scaleoftheif-then-elsethreshold−. \n",
            "(cid:12)\n",
            "144 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=0.3,b= 0.5\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=1.0,b= 2.0\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=3.0,b= 5.0\n",
            "−\n",
            "Withourinitialparameters,theoutputlookslikeaflattenedversionoftheearlierdiagram,\n",
            "withitstowerandplateau. Togetthedesiredbehaviour,weincreasetheparameterhuntil\n",
            "itbecomeslarge.\n",
            "Thatgivestheif-then-elsethresholdingbehaviour. Second, togetthe\n",
            "thresholdright,we’llchoose b 3h/2. Tryit,andseehowitworks! Here’swhatitlookslike,w≈he−nweuseh=10:\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 5\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 7\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 12\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 15\n",
            "−\n",
            "Evenforthisrelativelymodestvalueofh,wegetaprettygoodtowerfunction. And,of\n",
            "course,wecanmakeitasgoodaswewantbyincreasinghstillfurther,andkeepingthebias\n",
            "as b= 3h/2. Let−’strygluingtwosuchnetworkstogether,inordertocomputetwodifferenttower\n",
            "functions. To make the respective roles of the two sub-networks clear I’ve put them in\n",
            "separateboxes,below: eachboxcomputesatowerfunction,usingthetechniquedescribed\n",
            "above. Thegraphontherightshowstheweightedoutputfromthesecondhiddenlayer,that\n",
            "is,it’saweightedcombinationoftowerfunctions. 1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.8,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.2,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w1=0.2,w2=0.9\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 145\n",
            "(cid:12)\n",
            "Inparticular,youcanseethatbymodifyingtheweightsinthefinallayeryoucanchangethe\n",
            "heightoftheoutputtowers. Thesameideacanbeusedtocomputeasmanytowersaswelike.\n",
            "Wecanalsomake\n",
            "themasthinaswelike,andwhateverheightwelike. Asaresult,wecanensurethatthe\n",
            "weightedoutputfromthesecondhiddenlayerapproximatesanydesiredfunctionoftwo\n",
            "variables:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Inparticular,bymakingtheweightedoutputfromthesecondhiddenlayeragoodapproxi-\n",
            "mationtoσ 1 f,weensuretheoutputfromournetworkwillbeagoodapproximationto\n",
            "−\n",
            "anydesiredfun◦ction, f. Whataboutfunctionsofmorethantwovariables? Let’strythreevariables x ,x ,x . Thefollowingnetworkcanbeusedtocomputea\n",
            "1 2 3\n",
            "towerfunctioninfourdimensions:\n",
            "Here,the x ,x ,x denoteinputstothenetwork. Thes ,t andsoonaresteppointsfor\n",
            "1 2 3 1 1\n",
            "neurons–thatis,alltheweightsinthefirstlayerarelarge,andthebiasesaresettogivethe\n",
            "steppointss\n",
            "1\n",
            ",t\n",
            "1\n",
            ",s\n",
            "2\n",
            ",.... Theweightsinthesecondlayeralternate+h, h,wherehissome\n",
            "verylargenumber.\n",
            "Andtheoutputbiasis 5h/2. −\n",
            "Thisnetworkcomputesafunctionwh−ichis1providedthreeconditionsaremet: x is\n",
            "1\n",
            "betweens andt ; x isbetweens andt ;and x isbetweens andt . Thenetworkis0\n",
            "1 1 2 2 2 3 3 3\n",
            "\n",
            "(cid:12)\n",
            "146 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "everywhereelse. Thatis,it’sakindoftowerwhichis1inalittleregionofinputspace,and\n",
            "0everywhereelse. Bygluingtogethermanysuchnetworkswecangetasmanytowersaswewant,and\n",
            "soapproximateanarbitraryfunctionofthreevariables. Exactlythesameideaworksinm\n",
            "dimensions. Theonlychangeneededistomaketheoutputbias( m+1/2)h,inorderto\n",
            "gettherightkindofsandwichingbehaviortoleveltheplateau. −\n",
            "Okay,sowenowknowhowtouseneuralnetworkstoapproximateareal-valuedfunction\n",
            "ofmanyvariables. Whataboutvector-valuedfunctions f(x 1 ,...,x m) Rn? Ofcourse,sucha\n",
            "functioncanberegardedasjustnseparatereal-valuedfunctions,f1 (x\n",
            "1\n",
            "∈,...,x m),f2 (x\n",
            "1\n",
            ",...,x m),\n",
            "andsoon. Sowecreateanetworkapproximating f1,anothernetworkfor f2,andsoon.\n",
            "4\n",
            "Andthenwesimplyglueallthenetworkstogether.\n",
            "Sothat’salsoeasytocopewith. Problem\n",
            "We’veseenhowtousenetworkswithtwohiddenlayerstoapproximateanarbitrary\n",
            "• function. Canyoufindaproofshowingthatit’spossiblewithjustasinglehidden\n",
            "layer? Asahint,tryworkinginthecaseofjusttwoinputvariables,andshowingthat:\n",
            "(a)it’spossibletogetstepfunctionsnotjustinthexor ydirections,butinanarbitrary\n",
            "direction;(b)byaddingupmanyoftheconstructionsfrompart(a)it’spossibleto\n",
            "approximateatowerfunctionwhichiscircularinshape, ratherthanrectangular;\n",
            "(c)usingthesecirculartowers,it’spossibletoapproximateanarbitraryfunction. To\n",
            "dopart(c)itmayhelptouseideasfromabitlaterinthischapter. 4.4 Extension beyond sigmoid neurons\n",
            "We’veprovedthatnetworksmadeupofsigmoidneuronscancomputeanyfunction. Recall\n",
            "thatinasigmoidneurontheinputsx\n",
            "1\n",
            ",x\n",
            "2\n",
            ",...resultintheoutputσ( (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b),wherew\n",
            "j\n",
            "aretheweights, bisthebias,andσisthesigmoidfunction:\n",
            "Whatifweconsideradifferenttypeofneuron,oneusingsomeotheractivationfunction,\n",
            "s(z):\n",
            "\n",
            "(cid:12)\n",
            "4.4. Extensionbeyondsigmoidneurons (cid:12) 147\n",
            "(cid:12)\n",
            "Thatis,we’llassumethatifourneuronshaveinputs x ,x ,...,weightsw ,w ,...andbias\n",
            "1 2 1 2\n",
            "(cid:80)\n",
            "b,thentheoutputiss(\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b). Wecanusethisactivationfunctiontogetastepfunction,justaswedidwiththesigmoid. Tryrampinguptheweightinthefollowing,saytow=100:\n",
            "Outputfromneuron\n",
            "1 w=100,b= 3\n",
            "w=15,b= −3\n",
            "w=8,b= −3\n",
            "4\n",
            "w=6,b= −3\n",
            "w=4,b= −3\n",
            "−\n",
            "0 1\n",
            "1 w=6,b= 5\n",
            "w=6,b= −3\n",
            "w=6,b= −1\n",
            "−\n",
            "0 1\n",
            "Justaswiththesigmoid,thiscausestheactivationfunctiontocontract,andultimatelyit\n",
            "becomesaverygoodapproximationtoastepfunction. Trychangingthebias,andyou’llsee\n",
            "thatwecansetthepositionofthesteptobewhereverwechoose.\n",
            "Andsowecanuseallthe\n",
            "sametricksasbeforetocomputeanydesiredfunction. Whatpropertiesdoess(z)needtosatisfyinorderforthistowork? Wedoneedtoassume\n",
            "thats(z)iswell-definedasz andz . Thesetwolimitsarethetwovaluestaken\n",
            "onbyourstepfunction. We→als∞oneedto→as∞sumethattheselimitsaredifferentfromone\n",
            "another.\n",
            "Iftheyweren’t,there’dbenostep,simplyaflatgraph! Butprovidedtheactivation\n",
            "functions(z)satisfiestheseproperties,neuronsbasedonsuchanactivationfunctionare\n",
            "universalforcomputation. Problems\n",
            "Earlierinthebookwemetanothertypeofneuronknownasarectifiedlinearunit. • Explainwhysuchneuronsdon’tsatisfytheconditionsjustgivenforuniversality. Finda\n",
            "proofofuniversalityshowingthatrectifiedlinearunitsareuniversalforcomputation. Supposeweconsiderlinearneurons,i.e.,neuronswiththeactivationfunctions(z)=z. • Explainwhylinearneuronsdon’tsatisfytheconditionsjustgivenforuniversality.Show\n",
            "thatsuchneuronscan’tbeusedtodouniversalcomputation. \n",
            "(cid:12)\n",
            "148 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4.5 Fixing up the step functions\n",
            "Uptonow,we’vebeenassumingthatourneuronscanproducestepfunctionsexactly. That’s\n",
            "aprettygoodapproximation,butitisonlyanapproximation.\n",
            "Infact,therewillbeanarrow\n",
            "windowoffailure,illustratedinthefollowinggraph,inwhichthefunctionbehavesvery\n",
            "differentlyfromastepfunction:\n",
            "4\n",
            "InthesewindowsoffailuretheexplanationI’vegivenforuniversalitywillfail. Now,it’snotaterriblefailure. Bymakingtheweightsinputtotheneuronsbigenough\n",
            "we can make these windows of failure as small as we like. Certainly, we can make the\n",
            "windowmuchnarrowerthanI’veshownabove–narrower,indeed,thanoureyecouldsee. Soperhapswemightnotworrytoomuchaboutthisproblem.\n",
            "Nonetheless,it’dbenicetohavesomewayofaddressingtheproblem. Infact,theproblemturnsouttobeeasytofix. Let’slookatthefixforneuralnetworks\n",
            "computingfunctionswithjustoneinputandoneoutput. Thesameideasworkalsotoaddress\n",
            "theproblemwhentherearemoreinputsandoutputs. Inparticular,supposewewantournetworktocomputesomefunction, f. Asbefore,we\n",
            "dothisbytryingtodesignournetworksothattheweightedoutputfromourhiddenlayerof\n",
            "neuronsisσ\n",
            "−\n",
            "1 f(x):\n",
            "◦\n",
            "\n",
            "(cid:12)\n",
            "4.5. Fixingupthestepfunctions (cid:12) 149\n",
            "(cid:12)\n",
            "Ifweweretodothisusingthetechniquedescribedearlier,we’dusethehiddenneuronsto\n",
            "produceasequenceofbumpfunctions:\n",
            "4\n",
            "Again,I’veexaggeratedthesizeofthewindowsoffailure,inordertomakethemeasierto\n",
            "see. Itshouldbeprettyclearthatifweaddallthesebumpfunctionsupwe’llendupwitha\n",
            "reasonableapproximationtoσ\n",
            "−\n",
            "1 f(x),exceptwithinthewindowsoffailure. Supposethatinsteadofusing◦theapproximationjustdescribed,weuseasetofhidden\n",
            "neuronstocomputeanapproximationtohalfouroriginalgoalfunction,i.e.,toσ\n",
            "−\n",
            "1 f(x) /2. Ofcourse,thislooksjustlikeascaleddownversionofthelastgraph: ◦\n",
            "Andsupposeweuseanothersetofhiddenneuronstocomputeanapproximationtoσ 1\n",
            "−\n",
            "f(x) /2,butwiththebasesofthebumpsshiftedbyhalfthewidthofabump: ◦\n",
            "Nowwehavetwodifferentapproximationstoσ\n",
            "−\n",
            "1 f(x) /2. Ifweaddupthetwoapproxi-\n",
            "mationswe’llgetanoverallapproximationtoσ\n",
            "−\n",
            "1◦f(x).\n",
            "Thatoverallapproximationwill\n",
            "◦\n",
            "\n",
            "(cid:12)\n",
            "150 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "stillhavefailuresinsmallwindows. Buttheproblemwillbemuchlessthanbefore.\n",
            "The\n",
            "reasonisthatpointsinafailurewindowforoneapproximationwon’tbeinafailurewindow\n",
            "fortheother. Andsotheapproximationwillbeafactorroughly2betterinthosewindows. Wecoulddoevenbetterbyaddingupalargenumber,M,ofoverlappingapproximations\n",
            "tothefunctionσ\n",
            "−\n",
            "1 f(x) /M. Providedthewindowsoffailurearenarrowenough,apoint\n",
            "willonlyeverbeino◦newindowoffailure.\n",
            "Andprovidedwe’reusingalargeenoughnumber\n",
            "M ofoverlappingapproximations,theresultwillbeanexcellentoverallapproximation. Conclusion\n",
            "4\n",
            "Theexplanationforuniversalitywe’vediscussediscertainlynotapracticalprescriptionfor\n",
            "howtocomputeusingneuralnetworks! Inthis,it’smuchlikeproofsofuniversalityforNAND\n",
            "gatesandthelike. Forthisreason,I’vefocusedmostlyontryingtomaketheconstruction\n",
            "clearandeasytofollow,andnotonoptimizingthedetailsoftheconstruction. However,you\n",
            "mayfinditafunandinstructiveexercisetoseeifyoucanimprovetheconstruction. Althoughtheresultisn’tdirectlyusefulinconstructingnetworks,it’simportantbecause\n",
            "ittakesoffthetablethequestionofwhetheranyparticularfunctioniscomputableusinga\n",
            "neuralnetwork. Theanswertothatquestionisalways“yes”. Sotherightquestiontoaskis\n",
            "notwhetheranyparticularfunctioniscomputable,butratherwhat’sagoodwaytocompute\n",
            "thefunction. Theuniversalityconstructionwe’vedevelopedusesjusttwohiddenlayerstocomputean\n",
            "arbitraryfunction. Furthermore,aswe’vediscussed,it’spossibletogetthesameresultwith\n",
            "justasinglehiddenlayer. Giventhis,youmightwonderwhywewouldeverbeinterested\n",
            "indeepnetworks,i.e.,networkswithmanyhiddenlayers. Can’twesimplyreplacethose\n",
            "networkswithshallow,singlehiddenlayernetworks? Whileinprinciplethat’spossible,therearegoodpracticalreasonstousedeepnetworks.\n",
            "AsarguedinChapter1,deepnetworkshaveahierarchicalstructurewhichmakesthem\n",
            "particularlywelladaptedtolearnthehierarchiesofknowledgethatseemtobeusefulin\n",
            "solvingreal-worldproblems. Putmoreconcretely,whenattackingproblemssuchasimage\n",
            "recognition,ithelpstouseasystemthatunderstandsnotjustindividualpixels,butalso\n",
            "increasinglymorecomplexconcepts: fromedgestosimplegeometricshapes,alltheway\n",
            "upthroughcomplex,multi-objectscenes. Inlaterchapters,we’llseeevidencesuggesting\n",
            "thatdeepnetworksdoabetterjobthanshallownetworksatlearningsuchhierarchiesof\n",
            "knowledge. Tosumup: universalitytellsusthatneuralnetworkscancomputeanyfunction;\n",
            "andempiricalevidencesuggeststhatdeepnetworksarethenetworksbestadaptedtolearn\n",
            "thefunctionsusefulinsolvingmanyreal-worldproblems. 7Chapteracknowledgments:ThankstoJenDoddandChrisOlahformanydiscussionsaboutuniver-\n",
            "salityinneuralnetworks.Mythanks,inparticular,toChrisforsuggestingtheuseofalookuptableto\n",
            "proveuniversality.Theinteractivevisualformofthechapterisinspiredbytheworkofpeoplesuchas\n",
            "MikeBostock,AmitPatel,BretVictor,andStevenWittens. \n",
            "(cid:12)\n",
            "(cid:12) 151\n",
            "(cid:12)\n",
            "5555\n",
            "Why are deep neural networks\n",
            "hard to train? 5\n",
            "Imagineyou’reanengineerwhohasbeenaskedtodesignacomputerfromscratch.\n",
            "Oneday\n",
            "you’reworkingawayinyouroffice,designinglogicalcircuits,settingoutANDgates,ORgates,\n",
            "andsoon,whenyourbosswalksinwithbadnews. Thecustomerhasjustaddedasurprising\n",
            "designrequirement: thecircuitfortheentirecomputermustbejusttwolayersdeep:\n",
            "You’redumbfounded,andtellyourboss: “Thecustomeriscrazy!”\n",
            "Yourbossreplies: “Ithinkthey’recrazy,too. Butwhatthecustomerwants,theyget.”\n",
            "Infact,there’salimitedsenseinwhichthecustomerisn’tcrazy. Supposeyou’reallowed\n",
            "touseaspeciallogicalgatewhichletsyouANDtogetherasmanyinputsasyouwant. And\n",
            "you’realsoallowedamany-inputNANDgate,thatis,agatewhichcanANDmultipleinputs\n",
            "andthennegatetheoutput. Withthesespecialgatesitturnsouttobepossibletocompute\n",
            "anyfunctionatallusingacircuitthat’sjusttwolayersdeep. Butjustbecausesomethingispossibledoesn’tmakeitagoodidea. Inpractice,when\n",
            "solvingcircuitdesignproblems(ormostanykindofalgorithmicproblem),weusuallystart\n",
            "\n",
            "(cid:12)\n",
            "152 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "byfiguringouthowtosolvesub-problems,andthengraduallyintegratethesolutions. In\n",
            "otherwords,webuilduptoasolutionthroughmultiplelayersofabstraction. Forinstance,supposewe’redesigningalogicalcircuittomultiplytwonumbers. Chances\n",
            "arewewanttobuilditupoutofsub-circuitsdoingoperationslikeaddingtwonumbers. Thesub-circuitsforaddingtwonumberswill,inturn,bebuiltupoutofsub-sub-circuitsfor\n",
            "addingtwobits. Veryroughlyspeakingourcircuitwilllooklike:\n",
            "5\n",
            "Thatis,ourfinalcircuitcontainsatleastthreelayersofcircuitelements. Infact,it’llprobably\n",
            "containmorethanthreelayers,aswebreakthesub-tasksdownintosmallerunitsthanI’ve\n",
            "described. Butyougetthegeneralidea.\n",
            "Sodeepcircuitsmaketheprocessofdesigneasier.\n",
            "Butthey’renotjusthelpfulfordesign. Thereare,infact,mathematicalproofsshowingthatforsomefunctionsveryshallowcircuits\n",
            "requireexponentiallymorecircuitelementstocomputethandodeepcircuits. Forinstance,\n",
            "afamousseriesofpapersintheearly1980s1showedthatcomputingtheparityofasetof\n",
            "bitsrequiresexponentiallymanygates,ifdonewithashallowcircuit. Ontheotherhand,if\n",
            "youusedeepercircuitsit’seasytocomputetheparityusingasmallcircuit: youjustcompute\n",
            "theparityofpairsofbits,thenusethoseresultstocomputetheparityofpairsofpairsofbits,\n",
            "andsoon,buildingupquicklytotheoverallparity. Deepcircuitsthuscanbeintrinsically\n",
            "muchmorepowerfulthanshallowcircuits. Uptonow,thisbookhasapproachedneuralnetworkslikethecrazycustomer. Almostall\n",
            "thenetworkswe’veworkedwithhavejustasinglehiddenlayerofneurons(plustheinput\n",
            "andoutputlayers):\n",
            "1Thehistoryissomewhatcomplex,soIwon’tgivedetailedreferences.SeeJohanHåstad’s2012paper\n",
            "Onthecorrelationofparityandsmall-depthcircuitsforanaccountoftheearlyhistoryandreferences. \n",
            "(cid:12)\n",
            "(cid:12) 153\n",
            "(cid:12)\n",
            "5\n",
            "Thesesimplenetworkshavebeenremarkablyuseful: inearlierchaptersweusednetworks\n",
            "likethistoclassifyhandwrittendigitswithbetterthan98percentaccuracy! Nonetheless,\n",
            "intuitivelywe’dexpectnetworkswithmanymorehiddenlayerstobemorepowerful:\n",
            "Suchnetworkscouldusetheintermediatelayerstobuildupmultiplelayersofabstraction,\n",
            "justaswedoinBooleancircuits. Forinstance,ifwe’redoingvisualpatternrecognition,\n",
            "thentheneuronsinthefirstlayermightlearntorecognizeedges,theneuronsinthesecond\n",
            "layercouldlearntorecognizemorecomplexshapes,saytriangleorrectangles,builtupfrom\n",
            "edges. Thethirdlayerwouldthenrecognizestillmorecomplexshapes.\n",
            "Andsoon. These\n",
            "multiplelayersofabstractionseemlikelytogivedeepnetworksacompellingadvantagein\n",
            "learningtosolvecomplexpatternrecognitionproblems. Moreover,justasinthecaseof\n",
            "circuits,therearetheoreticalresultssuggestingthatdeepnetworksareintrinsicallymore\n",
            "powerfulthanshallownetworks2. 2ForcertainproblemsandnetworkarchitecturesthisisprovedinOnthenumberofresponseregionsof\n",
            "deepfeedforwardnetworkswithpiece-wiselinearactivations,byRazvanPascanu,GuidoMontúfar,and\n",
            "YoshuaBengio(2014).Seealsothemoreinformaldiscussioninsection2ofLearningdeeparchitectures\n",
            "forAI,byYoshuaBengio(2009). \n",
            "(cid:12)\n",
            "154 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "Howcanwetrainsuchdeepnetworks? Inthischapter,we’lltrytrainingdeepnetworks\n",
            "usingourworkhorselearningalgorithm–stochasticgradientdescentbybackpropagation. Butwe’llrunintotrouble,withourdeepnetworksnotperformingmuch(ifatall)better\n",
            "thanshallownetworks. Thatfailureseemssurprisinginthelightofthediscussionabove. Ratherthangiveupon\n",
            "deepnetworks,we’lldigdownandtrytounderstandwhat’smakingourdeepnetworkshard\n",
            "totrain. Whenwelookclosely,we’lldiscoverthatthedifferentlayersinourdeepnetwork\n",
            "arelearningatvastlydifferentspeeds. Inparticular,whenlaterlayersinthenetworkare\n",
            "learningwell,earlylayersoftengetstuckduringtraining,learningalmostnothingatall. This\n",
            "stucknessisn’tsimplyduetobadluck. Rather,we’lldiscovertherearefundamentalreasons\n",
            "thelearningslowdownoccurs,connectedtoouruseofgradient-basedlearningtechniques. Aswedelveintotheproblemmoredeeply,we’lllearnthattheoppositephenomenon\n",
            "canalsooccur: theearlylayersmaybelearningwell,butlaterlayerscanbecomestuck. In\n",
            "5\n",
            "fact,we’llfindthatthere’sanintrinsicinstabilityassociatedtolearningbygradientdescent\n",
            "indeep,many-layerneuralnetworks. Thisinstabilitytendstoresultineithertheearlyor\n",
            "thelaterlayersgettingstuckduringtraining. Thisallsoundslikebadnews. Butbydelvingintothesedifficulties,wecanbegintogain\n",
            "insightintowhat’srequiredtotraindeepnetworkseffectively. Andsotheseinvestigations\n",
            "aregoodpreparationforthenextchapter,wherewe’llusedeeplearningtoattackimage\n",
            "recognitionproblems. 5.1 The vanishing gradient problem\n",
            "So,whatgoeswrongwhenwetrytotrainadeepnetwork? Toanswerthatquestion,let’sfirstrevisitthecaseofanetworkwithjustasinglehidden\n",
            "layer. Asperusual,we’llusetheMNISTdigitclassificationproblemasourplaygroundfor\n",
            "learningandexperimentation3. Ifyouwish,youcanfollowalongbytrainingnetworksonyourcomputer.\n",
            "Itisalso,of\n",
            "course,finetojustreadalong. Ifyoudowishtofollowlive,thenyou’llneedPython2.7,\n",
            "Numpy,andacopyofthecode,whichyoucangetbycloningtherelevantrepositoryfrom\n",
            "thecommandline:\n",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
            "Ifyoudon’tusegitthenyoucandownloadthedataandcodehere. You’llneedtochange\n",
            "intothesrcsubdirectory.\n",
            "Then,fromaPythonshellweloadtheMNISTdata:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            "Wesetupournetwork:\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            "3IintroducedtheMNISTproblemanddatahere1.5andhere1.6. \n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 155\n",
            "(cid:12)\n",
            "Thisnetworkhas784neuronsintheinputlayer,correspondingtothe28 28=784pixels\n",
            "intheinputimage. Weuse30hiddenneurons,aswellas10outputneuro×ns,corresponding\n",
            "tothe10possibleclassificationsfortheMNISTdigits(‘0’,‘1’,‘2’,...,‘9’). Let’strytrainingournetworkfor30completeepochs,usingmini-batchesof10training\n",
            "examplesatatime,alearningrateη =0.1,andregularizationparameterλ =5.0. Aswe\n",
            "trainwe’llmonitortheclassificationaccuracyonthevalidation_data4:\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Wegetaclassificationaccuracyof96.48percent(orthereabouts–it’llvaryabitfromrunto\n",
            "run),comparabletoourearlierresultswithasimilarconfiguration. Now,let’saddanotherhiddenlayer,alsowith30neuronsinit,andtrytrainingwiththe\n",
            "samehyper-parameters:\n",
            "5\n",
            ">>> net = network2.Network([784, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thisgivesanimprovedclassificationaccuracy,96.90percent. That’sencouraging: alittle\n",
            "moredepthishelping. Let’saddanother30-neuronhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thatdoesn’thelpatall. Infact,theresultdropsbackdownto96.57percent,closetoour\n",
            "originalshallownetwork. Andsupposeweinsertonefurtherhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Theclassificationaccuracydropsagain,to96.53percent. That’sprobablynotastatistically\n",
            "significantdrop,butit’snotencouraging,either.\n",
            "Thisbehaviourseemsstrange. Intuitively,extrahiddenlayersoughttomakethenetwork\n",
            "able to learn more complex classification functions, and thus do a better job classifying. Certainly,thingsshouldn’tgetworse,sincetheextralayerscan,intheworstcase,simplydo\n",
            "nothing5. Butthat’snotwhat’sgoingon.\n",
            "So what is going on? Let’s assume that the extra hidden layers really could help in\n",
            "principle,andtheproblemisthatourlearningalgorithmisn’tfindingtherightweightsand\n",
            "biases. We’dliketofigureoutwhat’sgoingwronginourlearningalgorithm,andhowtodo\n",
            "better. Togetsomeinsightintowhat’sgoingwrong, let’svisualizehowthenetworklearns. Below,I’veplottedpartofa[784,30,30,10]network,i.e.,anetworkwithtwohiddenlayers,\n",
            "each containing 30 hidden neurons. Each neuron in the diagram has a little bar on it,\n",
            "4Notethatthenetworksislikelytotakesomeminutestotrain,dependingonthespeedofyour\n",
            "machine.Soifyou’rerunningthecodeyoumaywishtocontinuereadingandreturnlater,notwaitfor\n",
            "thecodetofinishexecuting. 5Seethislaterproblem5.2tounderstandhowtobuildahiddenlayerthatdoesnothing.\n",
            "\n",
            "(cid:12)\n",
            "156 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "representinghowquicklythatneuronischangingasthenetworklearns.\n",
            "Abigbarmeans\n",
            "theneuron’sweightsandbiasarechangingrapidly,whileasmallbarmeanstheweights\n",
            "andbiasarechangingslowly. Moreprecisely,thebarsdenotethegradient∂C/∂bforeach\n",
            "neuron,i.e.,therateofchangeofthecostwithrespecttotheneuron’sbias. BackinChapter\n",
            "2wesawthatthisgradientquantitycontrollednotjusthowrapidlythebiaschangesduring\n",
            "learning,butalsohowrapidlytheweightsinputtotheneuronchange,too. Don’tworryif\n",
            "youdon’trecallthedetails: thethingtokeepinmindissimplythatthesebarsshowhow\n",
            "quicklyeachneuron’sweightsandbiasarechangingasthenetworklearns. Tokeepthediagramsimple,I’veshownjustthetopsixneuronsinthetwohiddenlayers. I’veomittedtheinputneurons,sincethey’vegotnoweightsorbiasestolearn. I’vealso\n",
            "omittedtheoutputneurons,sincewe’redoinglayer-wisecomparisons,anditmakesmost\n",
            "sensetocomparelayerswiththesamenumberofneurons. Theresultsareplottedatthe\n",
            "verybeginningoftraining,i.e.,immediatelyafterthenetworkisinitialized. Heretheyare6:\n",
            "5\n",
            "Thenetworkwasinitializedrandomly,andsoit’snotsurprisingthatthere’salotofvariation\n",
            "inhowrapidlytheneuronslearn. Still,onethingthatjumpsoutisthatthebarsinthesecond\n",
            "hiddenlayeraremostlymuchlargerthanthebarsinthefirsthiddenlayer. Asaresult,the\n",
            "neuronsinthesecondhiddenlayerwilllearnquiteabitfasterthantheneuronsinthefirst\n",
            "hiddenlayer. Isthismerelyacoincidence,oraretheneuronsinthesecondhiddenlayer\n",
            "likelytolearnfasterthanneuronsinthefirsthiddenlayeringeneral? Todeterminewhetherthisisthecase,ithelpstohaveaglobalwayofcomparingthe\n",
            "speedoflearninginthefirstandsecondhiddenlayers. Todothis,let’sdenotethegradient\n",
            "asδl j= ∂C/∂bl j ,i.e.,thegradientforthe j-thneuroninthel-thlayer7Wecanthinkofthe\n",
            "gradientδ1asavectorwhoseentriesdeterminehowquicklythefirsthiddenlayerlearns,\n",
            "andδ2 asavectorwhoseentriesdeterminehowquicklythesecondhiddenlayerlearns. 6Thedataplottedisgeneratedusingtheprogramgenerate_gradient.py.Thesameprogramis\n",
            "alsousedtogeneratetheresultsquotedlaterinthissection. 7BackinChapter2wereferredtothisastheerror,butherewe’lladopttheinformalterm“gradient”. Isay“informal”becauseofcoursethisdoesn’texplicitlyincludethepartialderivativesofthecostwith\n",
            "respecttotheweights,∂C/∂w.\n",
            "\n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 157\n",
            "(cid:12)\n",
            "We’llthenusethelengthsofthesevectorsas(rough!) globalmeasuresofthespeedatwhich\n",
            "thelayersarelearning. So,forinstance,thelength δ1 measuresthespeedatwhichthe\n",
            "firsthiddenlayerislearning,whilethelength δ2 m(cid:107)eas(cid:107)uresthespeedatwhichthesecond\n",
            "hiddenlayerislearning.\n",
            "(cid:107) (cid:107)\n",
            "Withthesedefinitions, andinthesameconfigurationaswasplottedabove, wefind\n",
            "δ1 =0.07...and δ2 =0.31....\n",
            "Sothisconfirmsourearliersuspicion: theneuronsin\n",
            "(cid:107)the(cid:107)secondhiddenla(cid:107)yer(cid:107)reallyarelearningmuchfasterthantheneuronsinthefirsthidden\n",
            "layer. What happens if we add more hidden layers? If we have three hidden layers, in a\n",
            "[784,30,30,30,10]network, thentherespectivespeedsoflearningturnouttobe0.012,\n",
            "0.060,and0.283. Again,earlierhiddenlayersarelearningmuchslowerthanlaterhidden\n",
            "layers. Supposeweaddyetanotherlayerwith30hiddenneurons. Inthatcase,therespective\n",
            "speedsoflearningare0.003,0.017,0.070,and0.285. Thepatternholds: earlylayerslearn\n",
            "5\n",
            "slowerthanlaterlayers. We’vebeenlookingatthespeedoflearningatthestartoftraining,thatis,justafterthe\n",
            "networksareinitialized.\n",
            "Howdoesthespeedoflearningchangeaswetrainournetworks? Let’sreturntolookatthenetworkwithjusttwohiddenlayers. Thespeedoflearningchanges\n",
            "asfollows:\n",
            "Togeneratetheseresults, Iusedbatchgradientdescentwithjust1,000trainingimages,\n",
            "trainedover500epochs. Thisisabitdifferentthanthewayweusuallytrain–I’veusedno\n",
            "mini-batches,andjust1,000trainingimages,ratherthanthefull50,000imagetrainingset. I’mnottryingtodoanythingsneaky,orpullthewooloveryoureyes,butitturnsoutthat\n",
            "usingmini-batchstochasticgradientdescentgivesmuchnoisier(albeitverysimilar,when\n",
            "youaverageawaythenoise)results. UsingtheparametersI’vechosenisaneasywayof\n",
            "smoothingtheresultsout,sowecanseewhat’sgoingon. Inanycase,asyoucanseethetwolayersstartoutlearningatverydifferentspeeds(as\n",
            "wealreadyknow). Thespeedinbothlayersthendropsveryquickly,beforerebounding. But\n",
            "throughitall,thefirsthiddenlayerlearnsmuchmoreslowlythanthesecondhiddenlayer. Whataboutmorecomplexnetworks? Here’stheresultsofasimilarexperiment,butthis\n",
            "timewiththreehiddenlayers(a[784,30,30,30,10]network):\n",
            "\n",
            "(cid:12)\n",
            "158 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Finally,let’sadd\n",
            "afourthhiddenlayer(a[784,30,30,30,30,10]network),andseewhathappenswhenwe\n",
            "train:\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Inthiscase,\n",
            "thefirsthiddenlayerislearningroughly100timesslowerthanthefinalhiddenlayer.\n",
            "No\n",
            "wonderwewerehavingtroubletrainingthesenetworksearlier! Wehavehereanimportantobservation: inatleastsomedeepneuralnetworks,the\n",
            "gradienttendstogetsmalleraswemovebackwardthroughthehiddenlayers. Thismeans\n",
            "thatneuronsintheearlierlayerslearnmuchmoreslowlythanneuronsinlaterlayers. And\n",
            "while we’ve seen this in just a single network, there are fundamental reasons why this\n",
            "happensinmanyneuralnetworks. Thephenomenonisknownasthevanishinggradient\n",
            "\n",
            "(cid:12)\n",
            "5.2. What’scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 159\n",
            "(cid:12)\n",
            "problem8. Whydoesthevanishinggradientproblemoccur? Aretherewayswecanavoidit?\n",
            "And\n",
            "howshouldwedealwithitintrainingdeepneuralnetworks? Infact,we’lllearnshortly\n",
            "thatit’snotinevitable,althoughthealternativeisnotveryattractive,either: sometimesthe\n",
            "gradientgetsmuchlargerinearlierlayers! Thisistheexplodinggradientproblem,andit’s\n",
            "notmuchbetternewsthanthevanishinggradientproblem. Moregenerally,itturnsout\n",
            "thatthegradientindeepneuralnetworksisunstable,tendingtoeitherexplodeorvanish\n",
            "inearlierlayers. Thisinstabilityisafundamentalproblemforgradient-basedlearningin\n",
            "deepneuralnetworks. It’ssomethingweneedtounderstand,and,ifpossible,takestepsto\n",
            "address. Oneresponsetovanishing(orunstable)gradientsistowonderifthey’rereallysuch\n",
            "aproblem. Momentarilysteppingawayfromneuralnets,imagineweweretryingtonu-\n",
            "mericallyminimizeafunction f(x)ofasinglevariable. Wouldn’titbegoodnewsifthe\n",
            "5\n",
            "derivative f (cid:48)(x)wassmall? Wouldn’tthatmeanwewerealreadynearanextremum? Ina\n",
            "similarway,mightthesmallgradientinearlylayersofadeepnetworkmeanthatwedon’t\n",
            "needtodomuchadjustmentoftheweightsandbiases? Ofcourse,thisisn’tthecase. Recallthatwerandomlyinitializedtheweightandbiases\n",
            "inthenetwork. Itisextremelyunlikelyourinitialweightsandbiaseswilldoagoodjobat\n",
            "whateveritiswewantournetworktodo. Tobeconcrete,considerthefirstlayerofweights\n",
            "ina[784,30,30,30,10]networkfortheMNISTproblem. Therandominitializationmeans\n",
            "thefirstlayerthrowsawaymostinformationabouttheinputimage. Eveniflaterlayershave\n",
            "beenextensivelytrained,theywillstillfinditextremelydifficulttoidentifytheinputimage,\n",
            "simplybecausetheydon’thaveenoughinformation. Andsoitcan’tpossiblybethecasethat\n",
            "notmuchlearningneedstobedoneinthefirstlayer.\n",
            "Ifwe’regoingtotraindeepnetworks,\n",
            "weneedtofigureouthowtoaddressthevanishinggradientproblem. 5.2 What’s causing the vanishing gradient problem? Unstable\n",
            "gradients in deep neural nets\n",
            "Togetinsightintowhythevanishinggradientproblemoccurs,let’sconsiderthesimplest\n",
            "deepneuralnetwork: onewithjustasingleneuronineachlayer. Here’sanetworkwith\n",
            "threehiddenlayers:\n",
            "Here,w ,w ,...aretheweights, b ,b ,...arethebiases,andC issomecostfunction. Just\n",
            "1 2 1 2\n",
            "toremindyouhowthisworks,theoutputa\n",
            "j\n",
            "fromthe j-thneuronisσ (z j),whereσisthe\n",
            "usualsigmoidactivationfunction,andz j=w\n",
            "j\n",
            "a\n",
            "j\n",
            "1+b\n",
            "j\n",
            "istheweightedinputtotheneuron. I’vedrawnthecostC attheendtoemphasizet−hatthecostisafunctionofthenetwork’s\n",
            "output,a : iftheactualoutputfromthenetworkisclosetothedesiredoutput,thenthecost\n",
            "4\n",
            "willbelow,whileifit’sfaraway,thecostwillbehigh. 8SeeGradientflowinrecurrentnets: thedifficultyoflearninglong-termdependencies,bySepp\n",
            "Hochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber(2001).Thispaperstudiedrecurrent\n",
            "neuralnets,buttheessentialphenomenonisthesameasinthefeedforwardnetworkswearestudying. SeealsoSeppHochreiter’searlierDiplomaThesis,UntersuchungenzudynamischenneuronalenNetzen\n",
            "(1991,inGerman).\n",
            "\n",
            "(cid:12)\n",
            "160 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "We’regoingtostudythegradient∂C/∂b associatedtothefirsthiddenneuron. We’ll\n",
            "1\n",
            "figureoutanexpressionfor∂C/∂b ,andbystudyingthatexpressionwe’llunderstandwhy\n",
            "1\n",
            "thevanishinggradientproblemoccurs. I’llstartbysimplyshowingyoutheexpressionfor∂C/∂b .\n",
            "Itlooksforbidding, but\n",
            "1\n",
            "it’sactuallygotasimplestructure,whichI’lldescribeinamoment. Here’stheexpression\n",
            "(ignorethenetwork,fornow,andnotethatσ isjustthederivativeoftheσfunction):\n",
            "(cid:48)\n",
            "Thestructureintheexpressionisasfollows: thereisaσ (cid:48)(z j)termintheproductforeach\n",
            "5 neuroninthenetwork;aweightw termforeachweightinthenetwork;andafinal∂C/∂a\n",
            "j 4\n",
            "term,correspondingtothecostfunctionattheend. NoticethatI’veplacedeachterminthe\n",
            "expressionabovethecorrespondingpartofthenetwork.\n",
            "Sothenetworkitselfisamnemonic\n",
            "fortheexpression. You’rewelcometotakethisexpressionforgranted,andskiptothediscussionofhowit\n",
            "relatestothevanishinggradientproblem. There’snoharmindoingthis,sincetheexpression\n",
            "is a special case of our earlier discussion of backpropagation. But there’s also a simple\n",
            "explanationofwhytheexpressionistrue,andsoit’sfun(andperhapsenlightening)totake\n",
            "alookatthatexplanation. Imaginewemakeasmallchange∆b inthebias b .\n",
            "Thatwillsetoffacascadingseries\n",
            "1 1\n",
            "ofchangesintherestofthenetwork. First,itcausesachange∆a intheoutputfromthe\n",
            "1\n",
            "firsthiddenneuron. That,inturn,willcauseachange∆z intheweightedinputtothe\n",
            "2\n",
            "secondhiddenneuron. Thenachange∆a intheoutputfromthesecondhiddenneuron. 2\n",
            "Andsoon,allthewaythroughtoachange∆C inthecostattheoutput. Wehave\n",
            "∂C ∆C\n",
            ". (5.1)\n",
            "∂b\n",
            "1 ≈\n",
            "∆b\n",
            "1\n",
            "Thissuggeststhatwecanfigureoutanexpressionforthegradient∂C/∂b bycarefully\n",
            "1\n",
            "trackingtheeffectofeachstepinthiscascade. Todothis,let’sthinkabouthow∆b causestheoutputa fromthefirsthiddenneuron\n",
            "1 1\n",
            "tochange. Wehavea 1= σ (z 1)= σ (w 1 a 0+b 1),so\n",
            "∆a 1 ≈ ∂σ (w ∂ 1 a b 0 1 +b 1)∆b 1= σ (cid:48)(z 1) ∆b 1 . (5.2)\n",
            "Thatσ (cid:48)(z 1)termshouldlookfamiliar: it’sthefirstterminourclaimedexpressionforthe\n",
            "gradient∂C/∂b . Intuitively,thistermconvertsachange∆b inthebiasintoachange∆a\n",
            "1 1 1\n",
            "intheoutputactivation. Thatchange∆a inturncausesachangeintheweightedinput\n",
            "1\n",
            "z 2=w\n",
            "2\n",
            "a 1+b\n",
            "2\n",
            "tothesecondhiddenneuron:\n",
            "∂z\n",
            "∆z\n",
            "2\n",
            "≈\n",
            "∂a\n",
            "2\n",
            "1\n",
            "∆a 1=w\n",
            "2\n",
            "∆a\n",
            "1\n",
            ". (5.3)\n",
            "Combiningourexpressionsfor∆z and∆a ,weseehowthechangeinthebiasb propagates\n",
            "2 1 1\n",
            "\n",
            "(cid:12)\n",
            "5.2. What’scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 161\n",
            "(cid:12)\n",
            "alongthenetworktoaffectz :\n",
            "2\n",
            "∆z\n",
            "2\n",
            "σ (cid:48)(z 1)w\n",
            "2\n",
            "∆b\n",
            "1\n",
            ". (5.4)\n",
            "≈\n",
            "Again,thatshouldlookfamiliar: we’venowgotthefirsttwotermsinourclaimedexpression\n",
            "forthegradient∂C/∂b .\n",
            "1\n",
            "Wecankeepgoinginthisfashion,trackingthewaychangespropagatethroughtherest\n",
            "ofthenetwork. Ateachneuronwepickupaσ (cid:48)(z j)term,andthrougheachweightwepick\n",
            "upaw term. Theendresultisanexpressionrelatingthefinalchange∆C incosttothe\n",
            "j\n",
            "initialchange∆b inthebias:\n",
            "1\n",
            "∂C\n",
            "∆C\n",
            "≈\n",
            "σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)...σ (cid:48)(z 4)∂a\n",
            "4\n",
            "∆b 1 . (5.5)\n",
            "5\n",
            "Dividingby∆b wedoindeedgetthedesiredexpressionforthegradient:\n",
            "1\n",
            "∂C ∂C\n",
            "∂b = σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)...σ (cid:48)(z 4)∂a . (5.6)\n",
            "1 4\n",
            "Whythevanishinggradientproblemoccurs: Tounderstandwhythevanishinggradi-\n",
            "entproblemoccurs,let’sexplicitlywriteouttheentireexpressionforthegradient:\n",
            "∂C ∂C\n",
            "∂b = σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)w 3 σ (cid:48)(z 3)w 4 σ (cid:48)(z 4)∂a . (5.7)\n",
            "1 4\n",
            "Exceptingtheverylastterm,thisexpressionisaproductoftermsoftheformw\n",
            "j\n",
            "σ (cid:48)(z j). To\n",
            "understandhoweachofthosetermsbehave,let’slookataplotofthefunctionσ:\n",
            "(cid:48)\n",
            "Derivativeofsigmoidfunction\n",
            "0.2\n",
            "0.1\n",
            "0\n",
            "4 2 0 2 4\n",
            "− −\n",
            "Thederivativereachesamaximumatσ (cid:48)(0)=1/4. Now,ifweuseourstandardapproachto\n",
            "initializingtheweightsinthenetwork,thenwe’llchoosetheweightsusingaGaussianwith\n",
            "mean0andstandarddeviation1. Sotheweightswillusuallysatisfy w <1. Puttingthese\n",
            "j\n",
            "observationstogether,weseethatthetermsw j σ (cid:48)(z j)willusuallysa|tis|fy w j σ (cid:48)(z j) <1/4. Andwhenwetakeaproductofmanysuchterms,theproductwilltend|toexpone|ntially\n",
            "decrease: themoreterms,thesmallertheproductwillbe.\n",
            "Thisisstartingtosmelllikea\n",
            "possibleexplanationforthevanishinggradientproblem. To make this all a bit more explicit, let’s compare the expression for ∂C/∂b to an\n",
            "1\n",
            "expressionforthegradientwithrespecttoalaterbias,say∂C/∂b . Ofcourse,wehaven’t\n",
            "3\n",
            "\n",
            "(cid:12)\n",
            "162 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "explicitlyworkedoutanexpressionfor∂C/∂b ,butitfollowsthesamepatterndescribed\n",
            "3\n",
            "abovefor∂C/∂b . Here’sthecomparisonofthetwoexpressions:\n",
            "1\n",
            "5\n",
            "Thetwoexpressionssharemanyterms. Butthegradient∂C/∂b includestwoextraterms\n",
            "1\n",
            "eachoftheformw\n",
            "j\n",
            "σ (cid:48)(z j). Aswe’veseen,suchtermsaretypicallylessthan1/4inmagnitude. Andsothegradient∂C/∂b willusuallybeafactorof16(ormore)smallerthan∂C/∂b . 1 3\n",
            "Thisistheessentialoriginofthevanishinggradientproblem.\n",
            "Ofcourse,thisisaninformalargument,notarigorousproofthatthevanishinggradient\n",
            "problemwilloccur. Thereareseveralpossibleescapeclauses.\n",
            "Inparticular,wemightwonder\n",
            "whethertheweightsw j couldgrowduringtraining. Iftheydo,it’spossiblethetermsw j σ (cid:48)(z j)\n",
            "intheproductwillnolongersatisfy w j σ (cid:48)(z j) <1/4. Indeed,ifthetermsgetlargeenough\n",
            "–greaterthan1–thenwewillnolo|ngerhav|eavanishinggradientproblem. Instead,the\n",
            "gradientwillactuallygrowexponentiallyaswemovebackwardthroughthelayers. Instead\n",
            "ofavanishinggradientproblem,we’llhaveanexplodinggradientproblem. Theexplodinggradientproblem: Let’slookatanexplicitexamplewhereexploding\n",
            "gradientsoccur. Theexampleissomewhatcontrived: I’mgoingtofixparametersinthe\n",
            "networkinjusttherightwaytoensurewegetanexplodinggradient. Buteventhoughthe\n",
            "exampleiscontrived,ithasthevirtueoffirmlyestablishingthatexplodinggradientsaren’t\n",
            "merelyahypotheticalpossibility,theyreallycanhappen. Therearetwostepstogettinganexplodinggradient. First,wechoosealltheweights\n",
            "inthenetworktobelarge,sayw 1=w 2=w 3=w 4=100. Second,we’llchoosethebiases\n",
            "sothattheσ (cid:48)(z j)termsarenottoosmall. That’sactuallyprettyeasytodo: allweneed\n",
            "doischoosethebiasestoensurethattheweightedinputtoeachneuronisz j=0(andso\n",
            "σ (cid:48)(z j)=1/4). So,forinstance,wewantz 1=w\n",
            "1\n",
            "a 0+b 1=0. Wecanachievethisbysetting\n",
            "b 1= 100 a 0 . Wecanusethesameideatoselecttheotherbiases. Whenwedothis,we\n",
            "seeth−atall×thetermsw\n",
            "j\n",
            "σ (cid:48)(z j)areequalto100 1/4=25.\n",
            "Withthesechoiceswegetan\n",
            "explodinggradient. ×\n",
            "The unstable gradient problem: The fundamental problem here isn’t so much the\n",
            "vanishinggradientproblemortheexplodinggradientproblem. It’sthatthegradientin\n",
            "earlylayersistheproductoftermsfromallthelaterlayers.\n",
            "Whentherearemanylayers,\n",
            "that’sanintrinsicallyunstablesituation. Theonlywayalllayerscanlearnatclosetothe\n",
            "samespeedisifallthoseproductsoftermscomeclosetobalancingout. Withoutsome\n",
            "mechanismorunderlyingreasonforthatbalancingtooccur,it’shighlyunlikelytohappen\n",
            "simplybychance. Inshort,therealproblemhereisthatneuralnetworkssufferfroman\n",
            "unstablegradientproblem. Asaresult,ifweusestandardgradient-basedlearningtechniques,\n",
            "differentlayersinthenetworkwilltendtolearnatwildlydifferentspeeds. Exercise\n",
            "\n",
            "(cid:12)\n",
            "5.3. Unstablegradientsinmorecomplexnetworks (cid:12) 163\n",
            "(cid:12)\n",
            "Inourdiscussionofthevanishinggradientproblem,wemadeuseofthefactthat\n",
            "• σ (cid:48)(z) <1/4. Supposeweusedadifferentactivationfunction,onewhosederivative\n",
            "|could|bemuchlarger. Wouldthathelpusavoidtheunstablegradientproblem? The prevalence of the vanishing gradient problem: We’ve seen that the gradient can\n",
            "eithervanishorexplodeintheearlylayersofadeepnetwork. Infact,whenusingsigmoid\n",
            "neuronsthegradientwillusuallyvanish. Toseewhy,consideragaintheexpression wσ (cid:48)(z). Toavoidthevanishinggradientproblemweneed wσ (cid:48)(z) 1. Youmightthinkth|iscould|\n",
            "happeneasilyif wisverylarge. However,it’smo|rediffic|u≥ltthanitlooks. Thereasonis\n",
            "thattheσ (cid:48)(z)termalsodependsonw: σ (cid:48)(z)= σ (cid:48)(wa+b),whereaistheinputactivation. Sowhenwemake wlarge,weneedtobecarefulthatwe’renotsimultaneouslymaking\n",
            "σ (cid:48)(wa+b)small. Thatturnsouttobeaconsiderableconstraint.\n",
            "Thereasonisthatwhen\n",
            "wemakewlargewetendtomakewa+bverylarge. Lookingatthegraphofσ\n",
            "(cid:48)\n",
            "youcansee\n",
            "thatthisputsusoffinthe“wings”oftheσ function,whereittakesverysmallvalues. The 5\n",
            "(cid:48)\n",
            "onlywaytoavoidthisisiftheinputactivationfallswithinafairlynarrowrangeofvalues\n",
            "(thisqualitativeexplanationismadequantitativeinthefirstproblembelow). Sometimes\n",
            "thatwillchancetohappen.\n",
            "Moreoften,though,itdoesnothappen.\n",
            "Andsointhegeneric\n",
            "casewehavevanishinggradients. Problems\n",
            "Considertheproduct wσ (cid:48)(wa+b). Suppose wσ (cid:48)(wa+b) 1. (1)Arguethat\n",
            "• thiscanonlyeveroccu|rif w 4. |(2)Supposi|ngthat w |4≥,considerthesetof\n",
            "inputactivationsaforwhich| w|≥σ (cid:48)(wa+b) 1. Showtha|t|th≥esetofasatisfyingthat\n",
            "constraintcanrangeoveran|intervalnogr|e≥aterinwidththan\n",
            "2 (cid:18) w(1+ (cid:112) 1 4/w) (cid:19)\n",
            "ln | | − | | 1 . (5.8)\n",
            "w 2 −\n",
            "| |\n",
            "(3)Shownumericallythattheaboveexpressionboundingthewidthoftherangeis\n",
            "greatestat w 6.9,whereittakesavalue 0.45. Andsoevengiventhateverything\n",
            "linesupjus|tp|e≈rfectly,westillhaveafairly≈narrowrangeofinputactivationswhich\n",
            "canavoidthevanishinggradientproblem. Identityneuron: Consideraneuronwithasingleinput, x,acorrespondingweight,\n",
            "• w ,abias b,andaweightw ontheoutput. Showthatbychoosingtheweightsand\n",
            "1 2\n",
            "biasappropriately,wecanensurew\n",
            "2\n",
            "σ (w\n",
            "1\n",
            "x+b) xforx [0,1]. Suchaneuroncan\n",
            "thusbeusedasakindofidentityneuron,thatis≈,aneuron∈whoseoutputisthesame\n",
            "(uptorescalingbyaweightfactor)asitsinput. Hint: Ithelpstorewrite x=1/2+ ∆,\n",
            "toassumew issmall,andtouseaTaylorseriesexpansioninw ∆. 1 1\n",
            "5.3 Unstable gradients in more complex networks\n",
            "We’vebeenstudyingtoynetworks,withjustoneneuronineachhiddenlayer. Whatabout\n",
            "morecomplexdeepnetworks,withmanyneuronsineachhiddenlayer?\n",
            "\n",
            "(cid:12)\n",
            "164 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "5\n",
            "Infact,muchthesamebehaviouroccursinsuchnetworks. Intheearlierchapteronback-\n",
            "propagationwesawthatthegradientinthel-thlayerofanLlayernetworkisgivenby:\n",
            "δl = Σ (cid:48)(zl )(wl+1 ) TΣ (cid:48)(zl+1 )(wl+2 ) T...Σ (cid:48)(zL ) a C (5.9)\n",
            "∇\n",
            "Here,Σ (cid:48)(zl )isadiagonalmatrixwhoseentriesaretheσ (cid:48)(z)valuesfortheweightedinputs\n",
            "tothel-thlayer. Thewl aretheweightmatricesforthedifferentlayers. And C isthe\n",
            "a\n",
            "vectorofpartialderivativesofC withrespecttotheoutputactivations. ∇\n",
            "Thisisamuchmorecomplicatedexpressionthaninthesingle-neuroncase. Still, if\n",
            "youlookclosely,theessentialformisverysimilar,withlotsofpairsoftheform(wj ) TΣ (cid:48)(zj ). What’smore,thematricesΣ (cid:48)(zj )havesmallentriesonthediagonal,nonelargerthan1/4. Providedtheweightmatriceswj aren’ttoolarge,eachadditionalterm(wj ) TΣ (cid:48)(zl )tendsto\n",
            "makethegradientvectorsmaller,leadingtoavanishinggradient. Moregenerally,thelarge\n",
            "numberoftermsintheproducttendstoleadtoanunstablegradient,justasinourearlier\n",
            "example. Inpractice,empiricallyitistypicallyfoundinsigmoidnetworksthatgradients\n",
            "vanishexponentiallyquicklyinearlierlayers. Asaresult,learningslowsdowninthoselayers. Thisslowdownisn’tmerelyanaccidentoraninconvenience: it’safundamentalconsequence\n",
            "oftheapproachwe’retakingtolearning. 5.4 Other obstacles to deep learning\n",
            "Inthischapterwe’vefocusedonvanishinggradients–and,moregenerally,unstablegra-\n",
            "dients–asanobstacletodeeplearning. Infact,unstablegradientsarejustoneobstacle\n",
            "todeeplearning,albeitanimportantfundamentalobstacle.\n",
            "Muchongoingresearchaims\n",
            "tobetterunderstandthechallengesthatcanoccurwhentrainingdeepnetworks. Iwon’t\n",
            "comprehensivelysummarizethatworkhere,butjustwanttobrieflymentionacoupleof\n",
            "papers,togiveyoutheflavorofsomeofthequestionspeopleareasking. Asafirstexample,in2010GlorotandBengio9foundevidencesuggestingthattheuseof\n",
            "9Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks,byXavierGlorotand\n",
            "YoshuaBengio(2010).SeealsotheearlierdiscussionoftheuseofsigmoidsinEfficientBackProp,by\n",
            "YannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller(1998). \n",
            "(cid:12)\n",
            "5.4. Otherobstaclestodeeplearning (cid:12) 165\n",
            "(cid:12)\n",
            "sigmoidactivationfunctionscancauseproblemstrainingdeepnetworks. Inparticular,they\n",
            "foundevidencethattheuseofsigmoidswillcausetheactivationsinthefinalhiddenlayerto\n",
            "saturatenear0earlyintraining,substantiallyslowingdownlearning. Theysuggestedsome\n",
            "alternativeactivationfunctions,whichappearnottosufferasmuchfromthissaturation\n",
            "problem. Asasecondexample,in2013Sutskever,Martens,DahlandHinton10studiedtheimpact\n",
            "ondeeplearningofboththerandomweightinitializationandthemomentumschedulein\n",
            "momentum-basedstochasticgradientdescent. Inbothcases,makinggoodchoicesmadea\n",
            "substantialdifferenceintheabilitytotraindeepnetworks.\n",
            "Theseexamplessuggestthat“Whatmakesdeepnetworkshardtotrain?” isacomplex\n",
            "question. Inthischapter,we’vefocusedontheinstabilitiesassociatedtogradient-based\n",
            "learning in deep networks. The results in the last two paragraphs suggest that there is\n",
            "alsoaroleplayedbythechoiceofactivationfunction,thewayweightsareinitialized,and\n",
            "5\n",
            "evendetailsofhowlearningbygradientdescentisimplemented. And,ofcourse,choiceof\n",
            "networkarchitectureandotherhyper-parametersisalsoimportant. Thus,manyfactorscan\n",
            "playaroleinmakingdeepnetworkshardtotrain,andunderstandingallthosefactorsis\n",
            "stillasubjectofongoingresearch. Thisallseemsratherdownbeatandpessimism-inducing. Butthegoodnewsisthatinthenextchapterwe’llturnthataround,anddevelopseveral\n",
            "approachestodeeplearningthattosomeextentmanagetoovercomeorroutearoundall\n",
            "thesechallenges. 10Ontheimportanceofinitializationandmomentumindeeplearning,byIlyaSutskever,James\n",
            "Martens,GeorgeDahlandGeoffreyHinton(2013).\n",
            "\n",
            "(cid:12)\n",
            "166 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 167\n",
            "(cid:12)\n",
            "6666\n",
            "Deep learning\n",
            "6\n",
            "Inthelastchapterwelearnedthatdeepneuralnetworksareoftenmuchhardertotrain\n",
            "thanshallowneuralnetworks. That’sunfortunate,sincewehavegoodreasontobelieve\n",
            "thatifwecouldtraindeepnetsthey’dbemuchmorepowerfulthanshallownets. Butwhile\n",
            "thenewsfromthelastchapterisdiscouraging,wewon’tletitstopus. Inthischapter,we’ll\n",
            "developtechniqueswhichcanbeusedtotraindeepnetworks,andapplytheminpractice. We’llalsolookatthebroaderpicture,brieflyreviewingrecentprogressonusingdeepnets\n",
            "forimagerecognition,speechrecognition,andotherapplications. Andwe’lltakeabrief,\n",
            "speculativelookatwhatthefuturemayholdforneuralnets,andforartificialintelligence.\n",
            "Thechapterisalongone.\n",
            "Tohelpyounavigate,let’stakeatour. Thesectionsareonly\n",
            "looselycoupled,soprovidedyouhavesomebasicfamiliaritywithneuralnets,youcanjump\n",
            "towhatevermostinterestsyou. Themainpartofthechapterisanintroductiontooneofthemostwidelyusedtypesof\n",
            "deepnetwork: deepconvolutionalnetworks. We’llworkthroughadetailedexample–code\n",
            "andall–ofusingconvolutionalnetstosolvetheproblemofclassifyinghandwrittendigits\n",
            "fromtheMNISTdataset:\n",
            "We’llstartouraccountofconvolutionalnetworkswiththeshallownetworksusedtoattackthis\n",
            "problemearlierinthebook. Throughmanyiterationswe’llbuildupmoreandmorepowerful\n",
            "networks. Aswegowe’llexploremanypowerfultechniques: convolutions,pooling,theuse\n",
            "ofGPUstodofarmoretrainingthanwedidwithourshallownetworks,thealgorithmic\n",
            "expansionofourtrainingdata(toreduceoverfitting),theuseofthedropouttechnique(also\n",
            "toreduceoverfitting),theuseofensemblesofnetworks,andothers. Theresultwillbea\n",
            "\n",
            "(cid:12)\n",
            "168 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "systemthatoffersnear-humanperformance. Ofthe10,000MNISTtestimages–images\n",
            "notseenduringtraining! –oursystemwillclassify9,967correctly.\n",
            "Here’sapeekatthe33\n",
            "imageswhicharemisclassified. Notethatthecorrectclassificationisinthetopright;our\n",
            "program’sclassificationisinthebottomright:\n",
            "6\n",
            "Manyofthesearetoughevenforahumantoclassify. Consider,forexample,thethirdimage\n",
            "inthetoprow. Tomeitlooksmorelikea“9”thanan“8”,whichistheofficialclassification. Ournetworkalsothinksit’sa“9”.\n",
            "Thiskindof“error”isattheveryleastunderstandable,\n",
            "andperhapsevencommendable. Weconcludeourdiscussionofimagerecognitionwitha\n",
            "surveyofsomeofthespectacularrecentprogressusingnetworks(particularlyconvolutional\n",
            "nets)todoimagerecognition. Theremainderofthechapterdiscussesdeeplearningfromabroaderandlessdetailed\n",
            "perspective. We’llbrieflysurveyothermodelsofneuralnetworks,suchasrecurrentneural\n",
            "netsandlongshort-termmemoryunits,andhowsuchmodelscanbeappliedtoproblemsin\n",
            "speechrecognition,naturallanguageprocessing,andotherareas. Andwe’llspeculateabout\n",
            "thefutureofneuralnetworksanddeeplearning,rangingfromideaslikeintention-driven\n",
            "userinterfaces,totheroleofdeeplearninginartificialintelligence. Thechapterbuildsontheearlierchaptersinthebook,makinguseofandintegrating\n",
            "ideassuchasbackpropagation,regularization,thesoftmaxfunction,andsoon. However,to\n",
            "readthechapteryoudon’tneedtohaveworkedindetailthroughalltheearlierchapters. It\n",
            "will,however,helptohavereadChapter1,onthebasicsofneuralnetworks. WhenIuse\n",
            "conceptsfromChapters2to5,Iprovidelinkssoyoucanfamiliarizeyourself,ifnecessary. It’sworthnotingwhatthechapterisnot. It’snotatutorialonthelatestandgreatest\n",
            "neuralnetworkslibraries. Norarewegoingtobetrainingdeepnetworkswithdozensof\n",
            "layerstosolveproblemsattheveryleadingedge. Rather,thefocusisonunderstanding\n",
            "someofthecoreprinciplesbehinddeepneuralnetworks,andapplyingtheminthesimple,\n",
            "easy-to-understandcontextoftheMNISTproblem. Putanotherway: thechapterisnot\n",
            "goingtobringyourightuptothefrontier. Rather,theintentofthisandearlierchaptersisto\n",
            "focusonfundamentals,andsotoprepareyoutounderstandawiderangeofcurrentwork.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 169\n",
            "(cid:12)\n",
            "6.1 Introducing convolutional networks\n",
            "Inearlierchapters,wetaughtourneuralnetworkstodoaprettygoodjobrecognizingimages\n",
            "ofhandwrittendigits:\n",
            "Wedidthisusingnetworksinwhichadjacentnetworklayersarefullyconnectedtoone\n",
            "another. Thatis, everyneuroninthenetworkisconnectedtoeveryneuroninadjacent\n",
            "layers:\n",
            "6\n",
            "Inparticular,foreachpixelintheinputimage,weencodedthepixel’sintensityasthevalue\n",
            "foracorrespondingneuronintheinputlayer. Forthe28 28pixelimageswe’vebeenusing,\n",
            "thismeansournetworkhas784(=28 28)inputneuro×ns.\n",
            "Wethentrainedthenetwork’s\n",
            "weightsandbiasessothatthenetwork×’soutputwould–wehope! –correctlyidentifythe\n",
            "inputimage: ‘0’,‘1’,‘2’,...,‘8’,or‘9’.\n",
            "Ourearliernetworksworkprettywell: we’veobtainedaclassificationaccuracybetter\n",
            "than98percent,usingtrainingandtestdatafromtheMNISThandwrittendigitdataset. But\n",
            "uponreflection,it’sstrangetousenetworkswithfully-connectedlayerstoclassifyimages. Thereasonisthatsuchanetworkarchitecturedoesnottakeintoaccountthespatialstructure\n",
            "oftheimages. Forinstance,ittreatsinputpixelswhicharefarapartandclosetogether\n",
            "onexactlythesamefooting. Suchconceptsofspatialstructuremustinsteadbeinferred\n",
            "fromthetrainingdata. Butwhatif,insteadofstartingwithanetworkarchitecturewhichis\n",
            "tabularasa,weusedanarchitecturewhichtriestotakeadvantageofthespatialstructure? \n",
            "(cid:12)\n",
            "170 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "In this section I describe convolutional neural networks1. These networks use a special\n",
            "architecturewhichisparticularlywell-adaptedtoclassifyimages. Usingthisarchitecture\n",
            "makesconvolutionalnetworksfasttotrain. This,inturn,helpsustraindeep,many-layer\n",
            "networks,whichareverygoodatclassifyingimages. Today,deepconvolutionalnetworksor\n",
            "someclosevariantareusedinmostneuralnetworksforimagerecognition. Convolutionalneuralnetworksusethreebasicideas: localreceptivefields,sharedweights,\n",
            "andpooling.\n",
            "Let’slookateachoftheseideasinturn. Local receptive fields: In the fully-connected layers shown earlier, the inputs were\n",
            "depictedasaverticallineofneurons. Inaconvolutionalnet,it’llhelptothinkinsteadof\n",
            "theinputsasa28 28squareofneurons,whosevaluescorrespondtothe28 28pixel\n",
            "intensitieswe’reus×ingasinputs: ×\n",
            "6\n",
            "As per usual, we’ll connect the input pixels to a layer of hidden neurons. But we won’t\n",
            "connecteveryinputpixeltoeveryhiddenneuron. Instead,weonlymakeconnectionsin\n",
            "small,localizedregionsoftheinputimage. Tobemoreprecise,eachneuroninthefirsthiddenlayerwillbeconnectedtoasmall\n",
            "regionoftheinputneurons,say,forexample,a5 5region,correspondingto25input\n",
            "pixels. So,foraparticularhiddenneuron,wemight×haveconnectionsthatlooklikethis:\n",
            "Thatregionintheinputimageiscalledthelocalreceptivefieldforthehiddenneuron. It’sa\n",
            "littlewindowontheinputpixels.\n",
            "Eachconnectionlearnsaweight. Andthehiddenneuron\n",
            "1Theoriginsofconvolutionalneuralnetworksgobacktothe1970s.Buttheseminalpaperestablishing\n",
            "themodernsubjectofconvolutionalnetworkswasa1998paper,Gradient-basedlearningappliedto\n",
            "documentrecognition,byYannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner.LeCunhas\n",
            "sincemadeaninterestingremarkontheterminologyforconvolutionalnets:“The[biological]neural\n",
            "inspirationinmodelslikeconvolutionalnetsisverytenuous. That’swhyIcallthem‘convolutional\n",
            "nets’not‘convolutionalneuralnets’,andwhywecallthenodes‘units’andnot‘neurons’”.Despitethis\n",
            "remark,convolutionalnetsusemanyofthesameideasastheneuralnetworkswe’vestudieduptonow:\n",
            "ideassuchasbackpropagation,gradientdescent,regularization,non-linearactivationfunctions,andso\n",
            "on.Andsowewillfollowcommonpractice,andconsiderthematypeofneuralnetwork.Iwillusethe\n",
            "terms“convolutionalneuralnetwork”and“convolutionalnet(work)”interchangeably.Iwillalsouse\n",
            "theterms“[artificial]neuron”and“unit”interchangeably. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 171\n",
            "(cid:12)\n",
            "learnsanoverallbiasaswell. Youcanthinkofthatparticularhiddenneuronaslearningto\n",
            "analyzeitsparticularlocalreceptivefield. We then slide the local receptive field across the entire input image. For each local\n",
            "receptivefield,thereisadifferenthiddenneuroninthefirsthiddenlayer. Toillustratethis\n",
            "concretely,let’sstartwithalocalreceptivefieldinthetop-leftcorner:\n",
            "Thenweslidethelocalreceptivefieldoverbyonepixeltotheright(i.e.,byoneneuron),to\n",
            "connecttoasecondhiddenneuron: 6\n",
            "Andsoon,buildingupthefirsthiddenlayer. Notethatifwehavea28 28inputimage,and\n",
            "5 5localreceptivefields,thentherewillbe24 24neuronsinthe×hiddenlayer. Thisis\n",
            "be×causewecanonlymovethelocalreceptivefield×23neuronsacross(or23neuronsdown),\n",
            "beforecollidingwiththeright-handside(orbottom)oftheinputimage. I’veshownthelocalreceptivefieldbeingmovedbyonepixelatatime. Infact,sometimes\n",
            "adifferentstridelengthisused. Forinstance,wemightmovethelocalreceptivefield2\n",
            "pixelstotheright(ordown),inwhichcasewe’dsayastridelengthof2isused. Inthis\n",
            "chapterwe’llmostlystickwithstridelength1,butit’sworthknowingthatpeoplesometimes\n",
            "experimentwithdifferentstridelengths2. Sharedweightsandbiases: I’vesaidthateachhiddenneuronhasabiasand5 5\n",
            "weightsconnectedtoitslocalreceptivefield. WhatIdidnotyetmentionisthatwe’rego×ing\n",
            "tousethesameweightsandbiasforeachofthe24 24hiddenneurons.\n",
            "Inotherwords,\n",
            "forthe j,k-thhiddenneuron,theoutputis: ×\n",
            "(cid:130) 4 4 (cid:140)\n",
            "(cid:88)(cid:88)\n",
            "σ b+ w\n",
            "l,m\n",
            "a\n",
            "j+l,k+m\n",
            ". (6.1)\n",
            "l=0m=0\n",
            "2Aswasdoneinearlierchapters,ifwe’reinterestedintryingdifferentstridelengthsthenwecanuse\n",
            "validationdatatopickoutthestridelengthwhichgivesthebestperformance.Formoredetails,seethe\n",
            "earlierdiscussionofhowtochoosehyper-parametersinaneuralnetwork.Thesameapproachmayalso\n",
            "beusedtochoosethesizeofthelocalreceptivefield–thereis,ofcourse,nothingspecialaboutusinga\n",
            "5 5localreceptivefield. Ingeneral,largerlocalreceptivefieldstendtobehelpfulwhentheinput\n",
            "im×agesaresignificantlylargerthanthe28 28pixelMNISTimages.\n",
            "×\n",
            "\n",
            "(cid:12)\n",
            "172 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Here,σistheneuralactivationfunction–perhapsthesigmoidfunctionweusedinearlier\n",
            "chapters. bisthesharedvalueforthebias.\n",
            "w isa5 5arrayofsharedweights. And,\n",
            "l,m\n",
            "finally,weusea todenotetheinputactivationatposit×ion x,y. x,y\n",
            "Thismeansthatalltheneuronsinthefirsthiddenlayerdetectexactlythesamefeature3,\n",
            "justatdifferentlocationsintheinputimage. Toseewhythismakessense,supposethe\n",
            "weightsandbiasaresuchthatthehiddenneuroncanpickout, say, averticaledgeina\n",
            "particularlocalreceptivefield. Thatabilityisalsolikelytobeusefulatotherplacesinthe\n",
            "image. Andsoitisusefultoapplythesamefeaturedetectoreverywhereintheimage. Toput\n",
            "itinslightlymoreabstractterms,convolutionalnetworksarewelladaptedtothetranslation\n",
            "invarianceofimages: moveapictureofacat(say)alittleways,andit’sstillanimageofa\n",
            "cat4. Forthisreason,wesometimescallthemapfromtheinputlayertothehiddenlayera\n",
            "featuremap. Wecalltheweightsdefiningthefeaturemapthesharedweights.\n",
            "Andwecall\n",
            "thebiasdefiningthefeaturemapinthiswaythesharedbias. Thesharedweightsandbias\n",
            "areoftensaidtodefineakernelorfilter. Intheliterature,peoplesometimesusetheseterms\n",
            "inslightlydifferentways,andforthatreasonI’mnotgoingtobemoreprecise;rather,ina\n",
            "6\n",
            "moment,we’lllookatsomeconcreteexamples. ThenetworkstructureI’vedescribedsofarcandetectjustasinglekindoflocalized\n",
            "feature. Todoimagerecognitionwe’llneedmorethanonefeaturemap. Andsoacomplete\n",
            "convolutionallayerconsistsofseveraldifferentfeaturemaps:\n",
            "Intheexampleshown,thereare3featuremaps. Eachfeaturemapisdefinedbyasetof\n",
            "5 5sharedweights,andasinglesharedbias. Theresultisthatthenetworkcandetect\n",
            "3×differentkindsoffeatures, witheachfeaturebeingdetectableacrosstheentireimage. I’veshownjust3featuremaps,tokeepthediagramabovesimple. However,inpractice\n",
            "convolutionalnetworksmayusemore(andperhapsmanymore)featuremaps. Oneofthe\n",
            "earlyconvolutionalnetworks,LeNet-5,used6featuremaps,eachassociatedtoa5 5local\n",
            "receptivefield,torecognizeMNISTdigits. Sotheexampleillustratedaboveisactual×lypretty\n",
            "closetoLeNet-5. Intheexampleswedeveloplaterinthechapterwe’lluseconvolutional\n",
            "layerswith20and40featuremaps. Let’stakeaquickpeekatsomeofthefeatureswhich\n",
            "arelearned. 3Ihaven’tpreciselydefinedthenotionofafeature. Informally,thinkofthefeaturedetectedbya\n",
            "hiddenneuronasthekindofinputpatternthatwillcausetheneurontoactivate:itmightbeanedgein\n",
            "theimage,forinstance,ormaybesomeothertypeofshape. 4Infact,fortheMNISTdigitclassificationproblemwe’vebeenstudying,theimagesarecenteredand\n",
            "size-normalized.SoMNISThaslesstranslationinvariancethanimagesfound“inthewild”,sotospeak. Still,featureslikeedgesandcornersarelikelytobeusefulacrossmuchoftheinputspace.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 173\n",
            "(cid:12)\n",
            "The20imagescorrespondto20differentfeaturemaps(orfilters,orkernels). Eachmapis\n",
            "representedasa5 5blockimage,correspondingtothe5 5weightsinthelocalreceptive\n",
            "field. Whiterblock×smeanasmaller(typically,morenega×tive)weight,sothefeaturemap 6\n",
            "respondslesstocorrespondinginputpixels. Darkerblocksmeanalargerweight,sothe\n",
            "featuremaprespondsmoretothecorrespondinginputpixels. Veryroughlyspeaking,the\n",
            "imagesaboveshowthetypeoffeaturestheconvolutionallayerrespondsto. Sowhatcanweconcludefromthesefeaturemaps? It’sclearthereisspatialstructure\n",
            "here beyond what we’d expect at random: many of the features have clear sub-regions\n",
            "oflightanddark. Thatshowsournetworkreallyislearningthingsrelatedtothespatial\n",
            "structure.\n",
            "However,beyondthat,it’sdifficulttoseewhatthesefeaturedetectorsarelearning. Certainly,we’renotlearning(say)theGaborfilterswhichhavebeenusedinmanytraditional\n",
            "approachestoimagerecognition. Infact,there’snowalotofworkonbetterunderstanding\n",
            "the features learnt by convolutional networks. If you’re interested in following up on\n",
            "thatwork,IsuggeststartingwiththepaperVisualizingandUnderstandingConvolutional\n",
            "NetworksbyMatthewZeilerandRobFergus(2013). Abigadvantageofsharingweightsandbiasesisthatitgreatlyreducesthenumberof\n",
            "parametersinvolvedinaconvolutionalnetwork. Foreachfeaturemapweneed25=5 5\n",
            "sharedweights,plusasinglesharedbias. Soeachfeaturemaprequires26parameters. If×we\n",
            "have20featuremapsthat’satotalof20 26=520parametersdefiningtheconvolutional\n",
            "layer. Bycomparison,supposewehadafu×llyconnectedfirstlayer,with784=28 28input\n",
            "neurons,andarelativelymodest30hiddenneurons,asweusedinmanyofthe×examples\n",
            "earlierinthebook. That’satotalof784 30weights,plusanextra30biases,foratotal\n",
            "of23,550parameters.\n",
            "Inotherwords,the×fully-connectedlayerwouldhavemorethan40\n",
            "timesasmanyparametersastheconvolutionallayer.\n",
            "Ofcourse,wecan’treallydoadirectcomparisonbetweenthenumberofparameters,\n",
            "sincethetwomodelsaredifferentinessentialways. But,intuitively,itseemslikelythatthe\n",
            "useoftranslationinvariancebytheconvolutionallayerwillreducethenumberofparameters\n",
            "it needs to get the same performance as the fully-connected model. That, in turn, will\n",
            "resultinfastertrainingfortheconvolutionalmodel,and,ultimately,willhelpusbuilddeep\n",
            "networksusingconvolutionallayers. Incidentally,thenameconvolutionalcomesfromthefactthattheoperationinEquation\n",
            "(125)issometimesknownasaconvolution. Alittlemoreprecisely,peoplesometimeswrite\n",
            "thatequationasa1 = σ (b+w a0 ),wherea1 denotesthesetofoutputactivationsfrom\n",
            "onefeaturemap,a0istheseto∗finputactivations,and iscalledaconvolutionoperation. ∗\n",
            "\n",
            "(cid:12)\n",
            "174 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "We’renotgoingtomakeanydeepuseofthemathematicsofconvolutions,soyoudon’tneed\n",
            "toworrytoomuchaboutthisconnection. Butit’sworthatleastknowingwherethename\n",
            "comesfrom. Poolinglayers: Inadditiontotheconvolutionallayersjustdescribed,convolutional\n",
            "neuralnetworksalsocontainpoolinglayers. Poolinglayersareusuallyusedimmediately\n",
            "afterconvolutionallayers. Whatthepoolinglayersdoissimplifytheinformationinthe\n",
            "outputfromtheconvolutionallayer. Indetail,apoolinglayertakeseachfeaturemap5outputfromtheconvolutionallayer\n",
            "andpreparesacondensedfeaturemap. Forinstance,eachunitinthepoolinglayermay\n",
            "summarizearegionof(say)2 2neuronsinthepreviouslayer. Asaconcreteexample,\n",
            "onecommonprocedureforpoo×lingisknownasmax-pooling. Inmax-pooling,apooling\n",
            "unitsimplyoutputsthemaximumactivationinthe2 2inputregion,asillustratedinthe\n",
            "followingdiagram: ×\n",
            "6\n",
            "Notethatsincewehave24 24neuronsoutputfromtheconvolutionallayer,afterpooling\n",
            "wehave12 12neurons. ×\n",
            "Asment×ionedabove,theconvolutionallayerusuallyinvolvesmorethanasinglefeature\n",
            "map. Weapplymax-poolingtoeachfeaturemapseparately. Soiftherewerethreefeature\n",
            "maps,thecombinedconvolutionalandmax-poolinglayerswouldlooklike:\n",
            "Wecanthinkofmax-poolingasawayforthenetworktoaskwhetheragivenfeatureisfound\n",
            "anywhereinaregionoftheimage. Itthenthrowsawaytheexactpositionalinformation.\n",
            "Theintuitionisthatonceafeaturehasbeenfound,itsexactlocationisn’tasimportantasits\n",
            "roughlocationrelativetootherfeatures. Abigbenefitisthattherearemanyfewerpooled\n",
            "features,andsothishelpsreducethenumberofparametersneededinlaterlayers.\n",
            "Max-poolingisn’ttheonlytechniqueusedforpooling. Anothercommonapproachis\n",
            "knownasL2pooling. Here,insteadoftakingthemaximumactivationofa2 2regionof\n",
            "×\n",
            "5Thenomenclatureisbeingusedlooselyhere.Inparticular,I’musing“featuremap”tomeannotthe\n",
            "functioncomputedbytheconvolutionallayer,butrathertheactivationofthehiddenneuronsoutput\n",
            "fromthelayer.Thiskindofmildabuseofnomenclatureisprettycommonintheresearchliterature. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 175\n",
            "(cid:12)\n",
            "neurons,wetakethesquarerootofthesumofthesquaresoftheactivationsinthe2 2\n",
            "region. Whilethedetailsaredifferent,theintuitionissimilartomax-pooling: L2pooling×is\n",
            "awayofcondensinginformationfromtheconvolutionallayer. Inpractice,bothtechniques\n",
            "havebeenwidelyused. Andsometimespeopleuseothertypesofpoolingoperation. If\n",
            "you’rereallytryingtooptimizeperformance,youmayusevalidationdatatocompareseveral\n",
            "differentapproachestopooling,andchoosetheapproachwhichworksbest. Butwe’renot\n",
            "goingtoworryaboutthatkindofdetailedoptimization.\n",
            "Puttingitalltogether: Wecannowputalltheseideastogethertoformacomplete\n",
            "convolutionalneuralnetwork. It’ssimilartothearchitecturewewerejustlookingat,but\n",
            "hastheadditionofalayerof10outputneurons,correspondingtothe10possiblevaluesfor\n",
            "MNISTdigits(‘0’,‘1’,‘2’,etc):\n",
            "6\n",
            "Thenetworkbeginswith28 28inputneurons,whichareusedtoencodethepixelintensities\n",
            "fortheMNISTimage. This×isthenfollowedbyaconvolutionallayerusinga5 5local\n",
            "receptivefieldand3featuremaps. Theresultisalayerof3 24 24hiddenfeature×neurons. Thenextstepisamax-poolinglayer,appliedto2 2regio×ns,×acrosseachofthe3feature\n",
            "maps. Theresultisalayerof3 12 12hiddenfe×atureneurons. × ×\n",
            "Thefinallayerofconnectionsinthenetworkisafully-connectedlayer. Thatis,thislayer\n",
            "connectseveryneuronfromthemax-pooledlayertoeveryoneofthe10outputneurons. Thisfully-connectedarchitectureisthesameasweusedinearlierchapters. Note,however,\n",
            "thatinthediagramabove,I’veusedasinglearrow,forsimplicity,ratherthanshowingall\n",
            "theconnections. Ofcourse,youcaneasilyimaginetheconnections. This convolutional architecture is quite different to the architectures used in earlier\n",
            "chapters. Buttheoverallpictureissimilar: anetworkmadeofmanysimpleunits,whose\n",
            "behaviorsaredeterminedbytheirweightsandbiases.\n",
            "Andtheoverallgoalisstillthesame:\n",
            "tousetrainingdatatotrainthenetwork’sweightsandbiasessothatthenetworkdoesa\n",
            "goodjobclassifyinginputdigits. In particular, just as earlier in the book, we will train our network using stochastic\n",
            "gradientdescentandbackpropagation. Thismostlyproceedsinexactlythesamewayasin\n",
            "earlierchapters.\n",
            "However,wedoneedtomakeafewmodificationstothebackpropagation\n",
            "procedure. Thereasonisthatourearlierderivationofbackpropagationwasfornetworks\n",
            "withfully-connectedlayers. Fortunately,it’sstraightforwardtomodifythederivationfor\n",
            "convolutionalandmax-poolinglayers. Ifyou’dliketounderstandthedetails,thenIinvite\n",
            "youtoworkthroughthefollowingproblem. Bewarnedthattheproblemwilltakesometime\n",
            "toworkthrough,unlessyou’vereallyinternalizedtheearlierderivationofbackpropagation\n",
            "(inwhichcaseit’seasy). Problem\n",
            "\n",
            "(cid:12)\n",
            "176 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "BackpropagationinaconvolutionalnetworkThecoreequationsofbackpropagationin\n",
            "• anetworkwithfully-connectedlayersare(BP1)–(BP4). Supposewehaveanetwork\n",
            "containingaconvolutionallayer,amax-poolinglayer,andafully-connectedoutput\n",
            "layer,asinthenetworkdiscussedabove. Howaretheequationsofbackpropagation\n",
            "modified? 6.2 Convolutional neural networks in practice\n",
            "We’venowseenthecoreideasbehindconvolutionalneuralnetworks. Let’slookathowthey\n",
            "workinpractice,byimplementingsomeconvolutionalnetworks,andapplyingthemtothe\n",
            "MNISTdigitclassificationproblem. Theprogramwe’llusetodothisiscallednetwork3.py,\n",
            "andit’sanimprovedversionoftheprogramsnetwork.pyandnetwork2.pydevelopedin\n",
            "earlierchapters6. Ifyouwishtofollowalong,thecodeisavailableonGitHub. Notethat\n",
            "we’llworkthroughthecodefornetwork3.pyitselfinthenextsection.\n",
            "Inthissection,we’ll\n",
            "usenetwork3.pyasalibrarytobuildconvolutionalnetworks. 6 The programs network.py and network2.py were implemented using Python and\n",
            "the matrix library Numpy. Those programs worked from first principles, and got right\n",
            "downintothedetailsofbackpropagation,stochasticgradientdescent,andsoon. Butnow\n",
            "thatweunderstandthosedetails,fornetwork3.pywe’regoingtouseamachinelearning\n",
            "libraryknownasTheano7. UsingTheanomakesiteasytoimplementbackpropagationfor\n",
            "convolutionalneuralnetworks,sinceitautomaticallycomputesallthemappingsinvolved. Theano is also quite a bit faster than our earlier code (which was written to be easy to\n",
            "understand, not fast), and this makes it practical to train more complex networks. In\n",
            "particular,onegreatfeatureofTheanoisthatitcanruncodeoneitheraCPUor,ifavailable,\n",
            "aGPU.RunningonaGPUprovidesasubstantialspeedupand,again,helpsmakeitpractical\n",
            "totrainmorecomplexnetworks.\n",
            "Ifyouwishtofollowalong, thenyou’llneedtogetTheanorunningonyoursystem. ToinstallTheano,followtheinstructionsattheproject’shomepage. Theexampleswhich\n",
            "followwererunusingTheano0.68. SomewererununderMacOSXYosemite,withnoGPU. SomewererunonUbuntu14.04,withanNVIDIAGPU.Andsomeoftheexperimentswere\n",
            "rununderboth. Togetnetwork3.pyrunningyou’llneedtosettheGPUflagtoeitherTrue\n",
            "orFalse(asappropriate)inthenetwork3.pysource.\n",
            "Beyondthat,togetTheanoupand\n",
            "runningonaGPUyoumayfindtheinstructionsherehelpful. Therearealsotutorialsonthe\n",
            "web,easilyfoundusingGoogle,whichcanhelpyougetthingsworking. Ifyoudon’thavea\n",
            "GPUavailablelocally,thenyoumaywishtolookintoAmazonWebServicesEC2G2spot\n",
            "instances. NotethatevenwithaGPUthecodewilltakesometimetoexecute.\n",
            "Manyofthe\n",
            "experimentstakefromminutestohourstorun. OnaCPUitmaytakedaystorunthemost\n",
            "complexoftheexperiments. Asinearlierchapters,Isuggestsettingthingsrunning,and\n",
            "continuingtoread,occasionallycomingbacktochecktheoutputfromthecode. Ifyou’re\n",
            "6Notealsothatnetwork3.pyincorporatesideasfromtheTheanolibrary’sdocumentationonconvo-\n",
            "lutionalneuralnets(notablytheimplementationofLeNet-5),fromMishaDenil’simplementationof\n",
            "dropout,andfromChrisOlah. 7SeeTheano: ACPUandGPUMathExpressionCompilerinPython,byJamesBergstra,Olivier\n",
            "Breuleux,FredericBastien,PascalLamblin,RavzanPascanu,GuillaumeDesjardins,JosephTurian,David\n",
            "Warde-Farley,andYoshuaBengio(2010).TheanoisalsothebasisforthepopularPylearn2andKeras\n",
            "neuralnetworkslibraries.OtherpopularneuralnetslibrariesatthetimeofthiswritingincludeCaffe\n",
            "andTorch. 8AsIreleasethischapter,thecurrentversionofTheanohaschangedtoversion0.7. I’veactually\n",
            "reruntheexamplesunderTheano0.7andgetextremelysimilarresultstothosereportedinthetext. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 177\n",
            "(cid:12)\n",
            "usingaCPU,youmaywishtoreducethenumberoftrainingepochsforthemorecomplex\n",
            "experiments,orperhapsomitthementirely. Togetabaseline,we’llstartwithashallowarchitectureusingjustasinglehiddenlayer,\n",
            "containing100hiddenneurons.\n",
            "We’lltrainfor60epochs,usingalearningrateofη =0.1,a\n",
            "mini-batchsizeof10,andnoregularization. Herewego9:\n",
            ">>> import network3\n",
            ">>> from network3 import Network\n",
            ">>> from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
            ">>> training_data, validation_data, test_data = network3.load_data_shared()\n",
            ">>> mini_batch_size = 10\n",
            ">>> net = Network([FullyConnectedLayer(n_in=784, n_out=100),SoftmaxLayer(n_in\n",
            "=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Iobtainedabestclassificationaccuracyof97.80percent. Thisistheclassificationaccuracy\n",
            "on the test_data, evaluated at the training epoch where we get the best classification\n",
            "accuracyonthevalidation_data.\n",
            "Usingthevalidationdatatodecidewhentoevaluate\n",
            "6\n",
            "thetestaccuracyhelpsavoidoverfittingtothetestdata(seethisearlierdiscussionoftheuse\n",
            "ofvalidationdata). Wewillfollowthispracticebelow. Yourresultsmayvaryslightly,since\n",
            "thenetwork’sweightsandbiasesarerandomlyinitialized10. This97.80percentaccuracyisclosetothe98.04percentaccuracyobtainedbackin\n",
            "Chapter3,usingasimilarnetworkarchitectureandlearninghyper-parameters. Inparticular,\n",
            "bothexamplesusedashallownetwork,withasinglehiddenlayercontaining100hidden\n",
            "neurons. Bothalsotrainedfor60epochs,usedamini-batchsizeof10,andalearningrate\n",
            "ofη =0.1. Therewere,however,twodifferencesintheearliernetwork. First,weregularizedthe\n",
            "earliernetwork,tohelpreducetheeffectsofoverfitting. Regularizingthecurrentnetwork\n",
            "doesimprovetheaccuracies,butthegainisonlysmall,andsowe’llholdoffworryingabout\n",
            "regularizationuntillater. Second,whilethefinallayerintheearliernetworkusedsigmoid\n",
            "activationsandthecross-entropycostfunction,thecurrentnetworkusesasoftmaxfinal\n",
            "layer,andthelog-likelihoodcostfunction. AsexplainedinChapter3thisisn’tabigchange. Ihaven’tmadethisswitchforanyparticularlydeepreason–mostly,I’vedoneitbecause\n",
            "softmaxpluslog-likelihoodcostismorecommoninmodernimageclassificationnetworks. Canwedobetterthantheseresultsusingadeepernetworkarchitecture? Let’sbeginbyinsertingaconvolutionallayer,rightatthebeginningofthenetwork. We’ll\n",
            "use5by5localreceptivefields,astridelengthof1,and20featuremaps. We’llalsoinsert\n",
            "amax-poolinglayer,whichcombinesthefeaturesusing2by2poolingwindows.\n",
            "Sothe\n",
            "overallnetworkarchitecturelooksmuchlikethearchitecturediscussedinthelastsection,\n",
            "butwithanextrafully-connectedlayer:\n",
            "9Codefortheexperimentsinthissectionmaybefoundinthisscript.Notethatthecodeinthescript\n",
            "simplyduplicatesandparallelsthediscussioninthissection. NotealsothatthroughoutthesectionI’veexplicitlyspecifiedthenumberoftrainingepochs.I’vedone\n",
            "thisforclarityabouthowwe’retraining.Inpractice,it’sworthusingearlystopping,thatis,tracking\n",
            "accuracyonthevalidationset,andstoppingtrainingwhenweareconfidentthevalidationaccuracyhas\n",
            "stoppedimproving. 10Infact,inthisexperimentIactuallydidthreeseparaterunstraininganetworkwiththisarchitecture. Ithenreportedthetestaccuracywhichcorrespondedtothebestvalidationaccuracyfromanyofthe\n",
            "threeruns.Usingmultiplerunshelpsreducevariationinresults,whichisusefulwhencomparingmany\n",
            "architectures,aswearedoing.I’vefollowedthisprocedurebelow,exceptwherenoted.Inpractice,it\n",
            "madelittledifferencetotheresultsobtained. \n",
            "(cid:12)\n",
            "178 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Inthisarchitecture,wecanthinkoftheconvolutionalandpoolinglayersaslearningabout\n",
            "localspatialstructureintheinputtrainingimage,whilethelater,fully-connectedlayerlearns\n",
            "atamoreabstractlevel,integratingglobalinformationfromacrosstheentireimage. Thisis\n",
            "acommonpatterninconvolutionalneuralnetworks. Let’strainsuchanetwork,andseehowitperforms11:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "6\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Thatgetsusto98.78percentaccuracy,whichisaconsiderableimprovementoveranyof\n",
            "ourpreviousresults. Indeed,we’vereducedourerrorratebybetterthanathird,whichisa\n",
            "greatimprovement.\n",
            "Inspecifyingthenetworkstructure,I’vetreatedtheconvolutionalandpoolinglayersas\n",
            "asinglelayer. Whetherthey’reregardedasseparatelayersorasasinglelayeristosome\n",
            "extentamatteroftaste. network3.pytreatsthemasasinglelayerbecauseitmakesthe\n",
            "codefornetwork3.pyalittlemorecompact. However,itiseasytomodifynetwork3.pyso\n",
            "thelayerscanbespecifiedseparately,ifdesired. Exercise\n",
            "Whatclassificationaccuracydoyougetifyouomitthefully-connectedlayer,and\n",
            "• justusetheconvolutional-poolinglayerandsoftmaxlayer? Doestheinclusionofthe\n",
            "fully-connectedlayerhelp? Canweimproveonthe98.78percentclassificationaccuracy? Let’stryinsertingasecondconvolutional-poolinglayer. We’llmaketheinsertionbetween\n",
            "theexistingconvolutional-poolinglayerandthefully-connectedhiddenlayer.\n",
            "Again,we’ll\n",
            "usea5 5localreceptivefield,andpoolover2 2regions. Let’sseewhathappenswhen\n",
            "wetrain×usingsimilarhyper-parameterstobefore×:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "ConvPoolLayer(\n",
            "11I’vecontinuedtouseamini-batchsizeof10here.Infact,aswediscussedearlieritmaybepossible\n",
            "tospeeduptrainingusinglargermini-batches.I’vecontinuedtousethesamemini-batchsizemostlyfor\n",
            "consistencywiththeexperimentsinearlierchapters. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 179\n",
            "(cid:12)\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Onceagain,wegetanimprovement: we’renowat99.06percentclassificationaccuracy! There’stwonaturalquestionstoaskatthispoint. Thefirstquestionis: whatdoesiteven\n",
            "meantoapplyasecondconvolutional-poolinglayer? Infact,youcanthinkofthesecond\n",
            "convolutional-poolinglayerashavingasinput12 12“images”,whose“pixels”represent\n",
            "thepresence(orabsence)ofparticularlocalizedfea×turesintheoriginalinputimage. Soyou\n",
            "canthinkofthislayerashavingasinputaversionoftheoriginalinputimage. Thatversion\n",
            "isabstractedandcondensed,butstillhasalotofspatialstructure,andsoitmakessenseto\n",
            "useasecondconvolutional-poolinglayer. That’sasatisfyingpointofview,butgivesrisetoasecondquestion. Theoutputfrom\n",
            "thepreviouslayerinvolves20separatefeaturemaps,andsothereare20 12 12inputs\n",
            "to the second convolutional-pooling layer. It’s as though we’ve got 20×separ×ate images 6\n",
            "inputtotheconvolutional-poolinglayer,notasingleimage,aswasthecaseforthefirst\n",
            "convolutional-poolinglayer. Howshouldneuronsinthesecondconvolutional-poolinglayer\n",
            "respondtothesemultipleinputimages? Infact,we’llalloweachneuroninthislayertolearn\n",
            "fromall20 5 5inputneuronsinitslocalreceptivefield. Moreinformally: thefeature\n",
            "detectorsin×the×secondconvolutional-poolinglayerhaveaccesstoallthefeaturesfromthe\n",
            "previouslayer,butonlywithintheirparticularlocalreceptivefield12. Problem\n",
            "UsingthetanhactivationfunctionSeveraltimesearlierinthebookI’vementioned\n",
            "• argumentsthatthetanhfunctionmaybeabetteractivationfunctionthanthesigmoid\n",
            "function. We’veneveractedonthosesuggestions, sincewewerealreadymaking\n",
            "plentyofprogresswiththesigmoid.\n",
            "Butnowlet’strysomeexperimentswithtanh\n",
            "as our activation function. Try training the network with tanh activations in the\n",
            "convolutionalandfully-connectedlayers13. Beginwiththesamehyper-parametersas\n",
            "forthesigmoidnetwork,buttrainfor20epochsinsteadof60. Howwelldoesyour\n",
            "networkperform? Whatifyoucontinueoutto60epochs? Tryplottingtheper-epoch\n",
            "validationaccuraciesforbothtanh-andsigmoid-basednetworks,allthewayoutto60\n",
            "epochs. Ifyourresultsaresimilartomine,you’llfindthetanhnetworkstrainalittle\n",
            "faster,butthefinalaccuraciesareverysimilar. Canyouexplainwhythetanhnetwork\n",
            "mighttrainfaster? Canyougetasimilartrainingspeedwiththesigmoid,perhaps\n",
            "bychangingthelearningrate,ordoingsomerescaling14? Tryahalf-dozeniterations\n",
            "onthelearninghyper-parametersornetworkarchitecture,searchingforwaysthat\n",
            "tanhmaybesuperiortothesigmoid. Note: Thisisanopen-endedproblem.\n",
            "Personally,\n",
            "Ididnotfindmuchadvantageinswitchingtotanh,althoughIhaven’texperimented\n",
            "exhaustively,andperhapsyoumayfindaway. Inanycase,inamomentwewillfindan\n",
            "advantageinswitchingtotherectifiedlinearactivationfunction,andsowewon’tgoany\n",
            "12Thisissuewouldhaveariseninthefirstlayeriftheinputimageswereincolor.Inthatcasewe’d\n",
            "have3inputfeaturesforeachpixel,correspondingtored,greenandbluechannelsintheinputimage. Sowe’dallowthefeaturedetectorstohaveaccesstoallcolorinformation,butonlywithinagivenlocal\n",
            "receptivefield. 13Note that you can pass activation_fn=tanh as a parameter to the ConvPoolLayer and\n",
            "FullyConnectedLayerclasses. 14Youmayperhapsfindinspirationinrecallingthatσ (z)=(1+tanh(z/2)) /2.\n",
            "\n",
            "(cid:12)\n",
            "180 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "deeperintotheuseoftanh. Usingrectifiedlinearunits: Thenetworkwe’vedevelopedatthispointisactuallyavariant\n",
            "ofoneofthenetworksusedintheseminal1998paper15introducingtheMNISTproblem,\n",
            "anetworkknownasLeNet-5. It’sagoodfoundationforfurtherexperimentation,andfor\n",
            "buildingupunderstandingandintuition. Inparticular,therearemanywayswecanvarythe\n",
            "networkinanattempttoimproveourresults. Asabeginning,let’schangeourneuronssothatinsteadofusingasigmoidactivation\n",
            "function, we use rectified linear units. That is, we’ll use the activation function f(z)\n",
            "max(0,z).\n",
            "We’lltrainfor60epochs,withalearningrateofη =0.03. Ialsofoundthat≡it\n",
            "helpsalittletousesomel2regularization,withregularizationparameterλ =0.1:\n",
            ">>> from network3 import ReLU\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5), poolsize=(2, 2), activation_fn=ReLU),\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12), filter_shape=(40, 20, 5, 5),\n",
            "6\n",
            "poolsize=(2, 2), activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4, n_out\n",
            "=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.03, validation_data, test_data,\n",
            "lmbda=0.1)\n",
            "Iobtainedaclassificationaccuracyof99.23percent. It’samodestimprovementoverthe\n",
            "sigmoidresults(99.06).\n",
            "However,acrossallmyexperimentsIfoundthatnetworksbased\n",
            "onrectifiedlinearunitsconsistentlyoutperformednetworksbasedonsigmoidactivation\n",
            "functions. Thereappearstobearealgaininmovingtorectifiedlinearunitsforthisproblem. What makes the rectified linear activation function better than the sigmoid or tanh\n",
            "functions? Atpresent,wehaveapoorunderstandingoftheanswertothisquestion.\n",
            "Indeed,\n",
            "rectifiedlinearunitshaveonlybeguntobewidelyusedinthepastfewyears. Thereason\n",
            "forthatrecentadoptionisempirical: afewpeopletriedrectifiedlinearunits,oftenonthe\n",
            "basisofhunchesorheuristicarguments16. Theygotgoodresultsclassifyingbenchmarkdata\n",
            "sets,andthepracticehasspread. Inanidealworldwe’dhaveatheorytellinguswhich\n",
            "activationfunctiontopickforwhichapplication. Butatpresentwe’realongwayfromsuch\n",
            "aworld. Ishouldnotbeatallsurprisediffurthermajorimprovementscanbeobtainedby\n",
            "anevenbetterchoiceofactivationfunction. AndIalsoexpectthatincomingdecadesa\n",
            "powerfultheoryofactivationfunctionswillbedeveloped. Today,westillhavetorelyon\n",
            "poorlyunderstoodrulesofthumbandexperience. Expandingthetrainingdata: Anotherwaywemayhopetoimproveourresultsisby\n",
            "algorithmicallyexpandingthetrainingdata. Asimplewayofexpandingthetrainingdatais\n",
            "todisplaceeachtrainingimagebyasinglepixel,eitheruponepixel,downonepixel,left\n",
            "onepixel,orrightonepixel. Wecandothisbyrunningtheprogramexpand_mnist.py\n",
            "fromtheshellprompt17:\n",
            "15Gradient-basedlearningappliedtodocumentrecognition,byYannLeCun,LéonBottou,Yoshua\n",
            "Bengio,andPatrickHaffner(1998). Therearemanydifferencesofdetail,butbroadlyspeakingour\n",
            "networkisquitesimilartothenetworksdescribedinthepaper. 16Acommonjustificationisthatmax(0,z)doesn’tsaturateinthelimitoflargez,unlikesigmoid\n",
            "neurons,andthishelpsrectifiedlinearunitscontinuelearning.Theargumentisfine,asfaritgoes,but\n",
            "it’shardlyadetailedjustification,moreofajust-sostory. Notethatwediscussedtheproblemswith\n",
            "saturationbackinChapter2. 17Thecodeforexpand_mnist.pyisavailablehere. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 181\n",
            "(cid:12)\n",
            "$ python expand_mnist.py\n",
            "Runningthisprogramtakesthe50,000MNISTtrainingimages,andpreparesanexpanded\n",
            "trainingset,with250,000trainingimages. Wecanthenusethosetrainingimagestotrain\n",
            "ournetwork. We’llusethesamenetworkasabove,withrectifiedlinearunits. Inmyinitial\n",
            "experimentsIreducedthenumberoftrainingepochs–thismadesense,sincewe’retraining\n",
            "with5timesasmuchdata. But, infact, expandingthedataturnedouttoconsiderably\n",
            "reducetheeffectofoverfitting.\n",
            "Andso,aftersomeexperimentation,Ieventuallywentback\n",
            "totrainingfor60epochs. Inanycase,let’strain:\n",
            ">>> expanded_training_data, _, _ = network3.load_data_shared(\"../data/\n",
            "mnist_expanded.pkl.gz\")\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU), 6\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "UsingtheexpandedtrainingdataIobtaineda99.37percenttrainingaccuracy. Sothisalmost\n",
            "trivial change gives a substantial improvement in classification accuracy.\n",
            "Indeed, as we\n",
            "discussedearlierthisideaofalgorithmicallyexpandingthedatacanbetakenfurther. Justto\n",
            "remindyouoftheflavourofsomeoftheresultsinthatearlierdiscussion: in2003Simard,\n",
            "SteinkrausandPlatt18improvedtheirMNISTperformanceto99.6percentusinganeural\n",
            "networkotherwiseverysimilartoours,usingtwoconvolutional-poolinglayers,followedby\n",
            "ahiddenfully-connectedlayerwith100neurons. Therewereafewdifferencesofdetailin\n",
            "theirarchitecture–theydidn’thavetheadvantageofusingrectifiedlinearunits,forinstance–\n",
            "butthekeytotheirimprovedperformancewasexpandingthetrainingdata.\n",
            "Theydidthisby\n",
            "rotating,translating,andskewingtheMNISTtrainingimages. Theyalsodevelopedaprocess\n",
            "of“elasticdistortion”,awayofemulatingtherandomoscillationshandmusclesundergo\n",
            "whenapersoniswriting. Bycombiningalltheseprocessestheysubstantiallyincreasedthe\n",
            "effectivesizeoftheirtrainingdata,andthat’showtheyachieved99.6percentaccuracy. Problem\n",
            "Theideaofconvolutionallayersistobehaveinaninvariantwayacrossimages. It\n",
            "• mayseemsurprising,then,thatournetworkcanlearnmorewhenallwe’vedoneis\n",
            "translatetheinputdata. Canyouexplainwhythisisactuallyquitereasonable? Insertinganextrafully-connectedlayer: Canwedoevenbetter? Onepossibilityistouse\n",
            "exactlythesameprocedureasabove,buttoexpandthesizeofthefully-connectedlayer. I\n",
            "triedwith300and1,000neurons,obtainingresultsof99.46and99.43percent,respectively. That’sinteresting,butnotreallyaconvincingwinovertheearlierresult(99.37percent)\n",
            "18BestPracticesforConvolutionalNeuralNetworksAppliedtoVisualDocumentAnalysis,byPatrice\n",
            "Simard,DaveSteinkraus,andJohnPlatt(2003). \n",
            "(cid:12)\n",
            "182 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Whataboutaddinganextrafully-connectedlayer? Let’stryinsertinganextrafully-\n",
            "connectedlayer,sothatwehavetwo100-hiddenneuronfully-connectedlayers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=100, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "Doingthis,Iobtainedatestaccuracyof99.43percent. Again,theexpandednetisn’thelping\n",
            "6\n",
            "somuch. Runningsimilarexperimentswithfully-connectedlayerscontaining300and1,000\n",
            "neuronsyieldsresultsof99.48and99.47percent. That’sencouraging,butstillfallsshortof\n",
            "areallydecisivewin. What’sgoingonhere? Isitthattheexpandedorextrafully-connectedlayersreally\n",
            "don’thelpwithMNIST?Ormightitbethatournetworkhasthecapacitytodobetter,but\n",
            "we’regoingaboutlearningthewrongway? Forinstance, maybewecouldusestronger\n",
            "regularizationtechniquestoreducethetendencytooverfit. Onepossibilityisthedropout\n",
            "techniqueintroducedbackinChapter3. Recallthatthebasicideaofdropoutistoremove\n",
            "individualactivationsatrandomwhiletrainingthenetwork.\n",
            "Thismakesthemodelmore\n",
            "robusttothelossofindividualpiecesofevidence,andthuslesslikelytorelyonparticular\n",
            "idiosyncraciesofthetrainingdata. Let’stryapplyingdropouttothefinalfully-connected\n",
            "layers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(\n",
            "n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "FullyConnectedLayer(\n",
            "n_in=1000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
            "mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 40, mini_batch_size, 0.03,\n",
            "validation_data, test_data)\n",
            "Usingthis,weobtainanaccuracyof99.60percent,whichisasubstantialimprovementover\n",
            "ourearlierresults,especiallyourmainbenchmark,thenetworkwith100hiddenneurons,\n",
            "whereweachieved99.37percent. Therearetwochangesworthnoting.\n",
            "\n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 183\n",
            "(cid:12)\n",
            "First,Ireducedthenumberoftrainingepochsto40: dropoutreducedoverfitting,and\n",
            "sowelearnedfaster. Second,thefully-connectedhiddenlayershave1,000neurons,notthe100usedearlier. Ofcourse,dropouteffectivelyomitsmanyoftheneuronswhiletraining,sosomeexpansion\n",
            "istobeexpected. Infact,Itriedexperimentswithboth300and1,000hiddenneurons,and\n",
            "obtained(veryslightly)bettervalidationperformancewith1,000hiddenneurons. Usinganensembleofnetworks: Aneasywaytoimproveperformancestillfurtheristo\n",
            "createseveralneuralnetworks,andthengetthemtovotetodeterminethebestclassification. Suppose,forexample,thatwetrained5differentneuralnetworksusingtheprescription\n",
            "above, with each achieving accuracies near to 99.6 percent. Even though the networks\n",
            "wouldallhavesimilaraccuracies,theymightwellmakedifferenterrors,duetothedifferent\n",
            "randominitializations. It’splausiblethattakingavoteamongstour5networksmightyield\n",
            "aclassificationbetterthananyindividualnetwork. Thissoundstoogoodtobetrue,butthiskindofensemblingisacommontrickwithboth\n",
            "neuralnetworksandothermachinelearningtechniques. Anditdoesinfactyieldfurther\n",
            "improvements: weendupwith99.67percentaccuracy. Inotherwords,ourensembleof\n",
            "networksclassifiesallbut33ofthe10,000testimagescorrectly. 6\n",
            "Theremainingerrorsinthetestsetareshownbelow. Thelabelinthetoprightisthe\n",
            "correctclassification,accordingtotheMNISTdata,whileinthebottomrightisthelabel\n",
            "outputbyourensembleofnets:\n",
            "It’sworthlookingthroughtheseindetail. Thefirsttwodigits,a6anda5,aregenuineerrors\n",
            "byourensemble.\n",
            "However, they’realsounderstandableerrors, thekindahumancould\n",
            "plausiblymake. That6reallydoeslookalotlikea0,andthe5looksalotlikea3. Thethird\n",
            "image,supposedlyan8,actuallylookstomemorelikea9. SoI’msidingwiththenetwork\n",
            "ensemblehere: Ithinkit’sdoneabetterjobthanwhoeveroriginallydrewthedigit. Onthe\n",
            "otherhand,thefourthimage,the6,reallydoesseemtobeclassifiedbadlybyournetworks.\n",
            "Andsoon. Inmostcasesournetworks’choicesseematleastplausible,andinsomecases\n",
            "they’vedoneabetterjobclassifyingthantheoriginalpersondidwritingthedigit. Overall,\n",
            "ournetworksofferexceptionalperformance,especiallywhenyouconsiderthattheycorrectly\n",
            "classified9,967imageswhicharen’tshown.\n",
            "Inthatcontext,thefewclearerrorshereseem\n",
            "quiteunderstandable. Evenacarefulhumanmakestheoccasionalmistake. AndsoIexpect\n",
            "\n",
            "(cid:12)\n",
            "184 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "thatonlyanextremelycarefulandmethodicalhumanwoulddomuchbetter. Ournetwork\n",
            "isgettingneartohumanperformance. Whyweonlyapplieddropouttothefully-connectedlayers: Ifyoulookcarefullyat\n",
            "thecodeabove,you’llnoticethatweapplieddropoutonlytothefully-connectedsectionof\n",
            "thenetwork,nottotheconvolutionallayers. Inprinciplewecouldapplyasimilarprocedure\n",
            "totheconvolutionallayers. But, infact, there’snoneed: theconvolutionallayershave\n",
            "considerableinbuiltresistancetooverfitting. Thereasonisthatthesharedweightsmean\n",
            "thatconvolutionalfiltersareforcedtolearnfromacrosstheentireimage. Thismakesthem\n",
            "lesslikelytopickuponlocalidiosyncraciesinthetrainingdata. Andsothereislessneedto\n",
            "applyotherregularizers,suchasdropout. Goingfurther: It’spossibletoimproveperformanceonMNISTstillfurther. Rodrigo\n",
            "Benensonhascompiledaninformativesummarypage,showingprogressovertheyears,with\n",
            "linkstopapers. Manyofthesepapersusedeepconvolutionalnetworksalonglinessimilarto\n",
            "thenetworkswe’vebeenusing. Ifyoudigthroughthepapersyou’llfindmanyinteresting\n",
            "techniques,andyoumayenjoyimplementingsomeofthem. Ifyoudosoit’swisetostart\n",
            "implementationwithasimplenetworkthatcanbetrainedquickly,whichwillhelpyoumore\n",
            "6\n",
            "rapidlyunderstandwhatisgoingon. Forthemostpart,Iwon’ttrytosurveythisrecentwork. ButIcan’tresistmakingone\n",
            "exception. It’sa2010paperbyCire¸san,Meier,Gambardella,andSchmidhuber19.\n",
            "WhatI\n",
            "likeaboutthispaperishowsimpleitis. Thenetworkisamany-layerneuralnetwork,using\n",
            "onlyfully-connectedlayers(noconvolutions). Theirmostsuccessfulnetworkhadhidden\n",
            "layerscontaining2,500,2,000,1,500,1,000,and500neurons,respectively. Theyusedideas\n",
            "similartoSimardetaltoexpandtheirtrainingdata. Butapartfromthat,theyusedfew\n",
            "othertricks,includingnoconvolutionallayers: itwasaplain,vanillanetwork,ofthekind\n",
            "that,withenoughpatience,couldhavebeentrainedinthe1980s(iftheMNISTdatasethad\n",
            "existed),givenenoughcomputingpower. Theyachievedaclassificationaccuracyof99.65\n",
            "percent,moreorlessthesameasours. Thekeywastouseaverylarge,verydeepnetwork,\n",
            "andtouseaGPUtospeeduptraining. Thisletthemtrainformanyepochs. Theyalsotook\n",
            "advantageoftheirlongtrainingtimestograduallydecreasethelearningratefrom10 3to\n",
            "−\n",
            "10 6. It’safunexercisetotrytomatchtheseresultsusinganarchitectureliketheirs. −\n",
            "Why are we able to train? We saw in the last chapter that there are fundamental\n",
            "obstructionstotrainingindeep,many-layerneuralnetworks. Inparticular,wesawthatthe\n",
            "gradienttendstobequiteunstable: aswemovefromtheoutputlayertoearlierlayersthe\n",
            "gradienttendstoeithervanish(thevanishinggradientproblem)orexplode(theexploding\n",
            "gradientproblem). Sincethegradientisthesignalweusetotrain,thiscausesproblems.\n",
            "Howhaveweavoidedthoseresults? Ofcourse, theansweristhatwehaven’tavoidedtheseresults.\n",
            "Instead, we’vedone\n",
            "afewthingsthathelpusproceedanyway. Inparticular: (1)Usingconvolutionallayers\n",
            "greatlyreducesthenumberofparametersinthoselayers, makingthelearningproblem\n",
            "much easier; (2) Using more powerful regularization techniques (notably dropout and\n",
            "convolutionallayers)toreduceoverfitting,whichisotherwisemoreofaprobleminmore\n",
            "complexnetworks;(3)Usingrectifiedlinearunitsinsteadofsigmoidneurons,tospeedup\n",
            "training–empirically,oftenbyafactorof3–5;(4)UsingGPUsandbeingwillingtotrainfor\n",
            "alongperiodoftime. Inparticular,inourfinalexperimentswetrainedfor40epochsusing\n",
            "adataset5timeslargerthantherawMNISTtrainingdata. Earlierinthebookwemostly\n",
            "19Deep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCire¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJürgenSchmidhuber(2010).\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 185\n",
            "(cid:12)\n",
            "trainedfor30epochsusingjusttherawtrainingdata. Combiningfactors(3)and(4)it’sas\n",
            "thoughwe’vetrainedafactorperhaps30timeslongerthanbefore. Yourresponsemaybe“Isthatit?\n",
            "Isthatallwehadtodototraindeepnetworks? What’s\n",
            "allthefussabout?”\n",
            "Ofcourse,we’veusedotherideas,too: makinguseofsufficientlylargedatasets(to\n",
            "helpavoidoverfitting);usingtherightcostfunction(toavoidalearningslowdown);using\n",
            "goodweightinitializations(alsotoavoidalearningslowdown,duetoneuronsaturation);\n",
            "algorithmicallyexpandingthetrainingdata. Wediscussedtheseandotherideasinearlier\n",
            "chapters,andhaveforthemostpartbeenabletoreusetheseideaswithlittlecommentin\n",
            "thischapter. Withthatsaid,thisreallyisarathersimplesetofideas. Simple,butpowerful,when\n",
            "usedinconcert. Gettingstartedwithdeeplearninghasturnedouttobeprettyeasy!\n",
            "Howdeeparethesenetworks,anyway? Countingtheconvolutional-poolinglayers\n",
            "as single layers, our final architecture has 4 hidden layers. Does such a network really\n",
            "deserve to be called a deep network? Of course, 4 hidden layers is many more than in\n",
            "6\n",
            "theshallownetworkswestudiedearlier. Mostofthosenetworksonlyhadasinglehidden\n",
            "layer,oroccasionally2hiddenlayers. Ontheotherhand,asof2015state-of-the-artdeep\n",
            "networkssometimeshavedozensofhiddenlayers. I’veoccasionallyheardpeopleadopta\n",
            "deeper-than-thouattitude,holdingthatifyou’renotkeeping-up-with-the-Jonesesinterms\n",
            "ofnumberofhiddenlayers,thenyou’renotreallydoingdeeplearning. I’mnotsympathetic\n",
            "tothisattitude, inpartbecauseitmakesthedefinitionofdeeplearningintosomething\n",
            "whichdependsupontheresult-of-the-moment. Therealbreakthroughindeeplearningwas\n",
            "torealizethatit’spracticaltogobeyondtheshallow1-and2-hiddenlayernetworksthat\n",
            "dominatedworkuntilthemid-2000s. Thatreallywasasignificantbreakthrough,opening\n",
            "uptheexplorationofmuchmoreexpressivemodels. Butbeyondthat,thenumberoflayers\n",
            "isnotofprimaryfundamentalinterest. Rather,theuseofdeepernetworksisatooltouseto\n",
            "helpachieveothergoals–likebetterclassificationaccuracies. Awordonprocedure: Inthissection,we’vesmoothlymovedfromsinglehidden-layer\n",
            "shallownetworkstomany-layerconvolutionalnetworks. Itallseemedsoeasy!\n",
            "Wemakea\n",
            "changeand,forthemostpart,wegetanimprovement. Ifyoustartexperimenting,Ican\n",
            "guaranteethingswon’talwaysbesosmooth. ThereasonisthatI’vepresentedacleaned-up\n",
            "narrative,omittingmanyexperiments–includingmanyfailedexperiments. Thiscleaned-up\n",
            "narrativewillhopefullyhelpyougetclearonthebasicideas. Butitalsorunstheriskof\n",
            "conveyinganincompleteimpression. Gettingagood,workingnetworkcaninvolvealotof\n",
            "trialanderror,andoccasionalfrustration. Inpractice,youshouldexpecttoengageinquite\n",
            "abitofexperimentation.\n",
            "TospeedthatprocessupyoumayfindithelpfultorevisitChapter\n",
            "3’sdiscussionofhowtochooseaneuralnetwork’shyper-parameters,andperhapsalsoto\n",
            "lookatsomeofthefurtherreadingsuggestedinthatsection. 6.3 The code for our convolutional networks\n",
            "Alright,let’stakealookatthecodeforourprogram,network3.py. Structurally,it’ssimilar\n",
            "tonetwork2.py,theprogramwedevelopedinChapter3,althoughthedetailsdiffer,due\n",
            "totheuseofTheano. We’llstartbylookingattheFullyConnectedLayerclass,whichis\n",
            "\n",
            "(cid:12)\n",
            "186 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "similartothelayersstudiedearlierinthebook. Here’sthecode(discussionbelow)20:\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "6 self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "Muchofthe__init__methodisself-explanatory,butafewremarksmayhelpclarifythe\n",
            "code. Asperusual,werandomlyinitializetheweightsandbiasesasnormalrandomvariables\n",
            "withsuitablestandarddeviations.\n",
            "Thelinesdoingthislookalittleforbidding. However,\n",
            "mostofthecomplicationisjustloadingtheweightsandbiasesintowhatTheanocallsshared\n",
            "variables. ThisensuresthatthesevariablescanbeprocessedontheGPU,ifoneisavailable. Wewon’tgettoomuchintothedetailsofthis. Ifyou’reinterested,youcandigintothe\n",
            "Theanodocumentation. Notealsothatthisweightandbiasinitializationisdesignedforthe\n",
            "sigmoidactivationfunction(asdiscussedearlier). Ideally,we’dinitializetheweightsand\n",
            "biasessomewhatdifferentlyforactivationfunctionssuchasthetanhandrectifiedlinear\n",
            "function. Thisisdiscussedfurtherinproblemsbelow.\n",
            "The__init__methodfinisheswith\n",
            "self.params = [self.w, self.b]. Thisisahandywaytobundleupallthelearnable\n",
            "parametersassociatedtothelayer. Lateron, theNetwork.SGDmethodwilluseparams\n",
            "attributestofigureoutwhatvariablesinaNetworkinstancecanlearn. Theset_inptmethodisusedtosettheinputtothelayer,andtocomputethecorre-\n",
            "spondingoutput. Iusethenameinptratherthaninputbecauseinputisabuilt-infunction\n",
            "20NoteaddedNovember2016:severalreadershavenotedthatinthelineinitializingself.w,Iset\n",
            "scale=np.sqrt(1.0/n_out),whentheargumentsofChapter3suggestabetterinitializationmaybe\n",
            "scale=np.sqrt(1.0/n_in).Thiswassimplyamistakeonmypart.InanidealworldI’drerunallthe\n",
            "examplesinthischapterwiththecorrectcode.Still,I’vemovedontootherprojects,soamgoingtolet\n",
            "theerrorgo. \n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 187\n",
            "(cid:12)\n",
            "inPython,andmessingwithbuilt-instendstocauseunpredictablebehavioranddifficult-to-\n",
            "diagnosebugs. Notethatweactuallysettheinputintwoseparateways: asself.inptand\n",
            "self.inpt_dropout. Thisisdonebecauseduringtrainingwemaywanttousedropout. If\n",
            "that’sthecasethenwewanttoremoveafractionself.p_dropoutoftheneurons. That’s\n",
            "whatthefunctiondropout_layerinthesecond-lastlineoftheset_inptmethodisdo-\n",
            "ing. Soself.inpt_dropoutandself.output_dropoutareusedduringtraining,while\n",
            "self.inptandself.outputareusedforallotherpurposes,e.g.,evaluatingaccuracyonthe\n",
            "validationandtestdata. TheConvPoolLayerandSoftmaxLayerclassdefinitionsaresimilartoFullyConnectedLayer\n",
            ". Indeed,they’resoclosethatIwon’texcerptthecodehere.\n",
            "Ifyou’reinterestedyoucanlook\n",
            "atthefulllistingfornetwork3.py,laterinthissection. However,acoupleofminordifferencesofdetailareworthmentioning. Mostobviously,\n",
            "inbothConvPoolLayerandSoftmaxLayerwecomputetheoutputactivationsintheway\n",
            "appropriate to that layer type. Fortunately, Theano makes that easy, providing built-in\n",
            "operationstocomputeconvolutions,max-pooling,andthesoftmaxfunction. Less obviously, when we introduced the softmax layer, we never discussed how to\n",
            "initializetheweightsandbiases. Elsewherewe’vearguedthatforsigmoidlayersweshould 6\n",
            "initialize the weights using suitably parameterized normal random variables. But that\n",
            "heuristicargumentwasspecifictosigmoidneurons(and,withsomeamendment,totanh\n",
            "neurons). However,there’snoparticularreasontheargumentshouldapplytosoftmaxlayers. Sothere’snoapriorireasontoapplythatinitializationagain. Ratherthandothat,Ishall\n",
            "initializealltheweightsandbiasestobe0. Thisisaratheradhocprocedure,butworks\n",
            "wellenoughinpractice. Okay,we’velookedatallthelayerclasses.\n",
            "WhatabouttheNetworkclass? Let’sstartby\n",
            "lookingatthe__init__method:\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of ‘layers‘, describing the network architecture, and\n",
            "a value for the ‘mini_batch_size‘ to be used during training\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "Mostofthisisself-explanatory,ornearlyso. Thelineself.params = [param for layer\n",
            "in ...]bundlesuptheparametersforeachlayerintoasinglelist.\n",
            "Asanticipatedabove,\n",
            "theNetwork.SGDmethodwilluseself.paramstofigureoutwhatvariablesintheNetwork\n",
            "canlearn. Thelinesself.x = T.matrix(\"x\")andself.y = T.ivector(\"y\")define\n",
            "Theanosymbolicvariablesnamedxandy. Thesewillbeusedtorepresenttheinputand\n",
            "\n",
            "(cid:12)\n",
            "188 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "desiredoutputfromthenetwork. Now,thisisn’taTheanotutorial,andsowewon’tgettoodeeplyintowhatitmeans\n",
            "thatthesearesymbolicvariables21. Buttheroughideaisthattheserepresentmathematical\n",
            "variables,notexplicitvalues. Wecandoalltheusualthingsonewoulddowithsuchvariables:\n",
            "add, subtract, andmultiplythem, applyfunctions, andsoon. Indeed, Theanoprovides\n",
            "manywaysofmanipulatingsuchsymbolicvariables,doingthingslikeconvolutions,max-\n",
            "pooling,andsoon. Butthebigwinistheabilitytodofastsymbolicdifferentiation,usinga\n",
            "verygeneralformofthebackpropagationalgorithm. Thisisextremelyusefulforapplying\n",
            "stochasticgradientdescenttoawidevarietyofnetworkarchitectures. Inparticular,thenext\n",
            "fewlinesofcodedefinesymbolicoutputsfromthenetwork. Westartbysettingtheinputto\n",
            "theinitiallayer,withtheline\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "Notethattheinputsaresetonemini-batchatatime,whichiswhythemini-batchsizeis\n",
            "there. Notealsothatwepasstheinputself.xintwice: thisisbecausewemayusethe\n",
            "networkintwodifferentways(withorwithoutdropout). Theforloopthenpropagatesthe\n",
            "6\n",
            "symbolicvariableself.xforwardthroughthelayersoftheNetwork. Thisallowsustodefine\n",
            "thefinaloutputandoutput_dropoutattributes,whichsymbolicallyrepresenttheoutput\n",
            "fromtheNetwork. Nowthatwe’veunderstoodhowaNetworkisinitialized,let’slookathowitistrained,\n",
            "usingtheSGDmethod. Thecodelookslengthy,butitsstructureisactuallyrathersimple.\n",
            "Explanatorycommentsafterthecode. def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches. i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "21TheTheanodocumentationprovidesagoodintroductiontoTheano.Andifyougetstuck,youmay\n",
            "findithelpfultolookatoneoftheothertutorialsavailableonline. Forinstance,thistutorialcovers\n",
            "manybasics.\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 189\n",
            "(cid:12)\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out, 6\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(’The corresponding test accuracy is {0:.2%}’.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "Thefirstfewlinesarestraightforward,separatingthedatasetsinto x and y components,\n",
            "andcomputingthenumberofmini-batchesusedineachdataset. Thenextfewlinesare\n",
            "moreinteresting,andshowsomeofwhatmakesTheanofuntoworkwith.\n",
            "Let’sexplicitly\n",
            "excerptthelineshere:\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "\n",
            "(cid:12)\n",
            "190 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)]\n",
            "Intheselineswesymbolicallysetuptheregularizedlog-likelihoodcostfunction,computethe\n",
            "correspondingderivativesinthegradientfunction,aswellasthecorrespondingparameter\n",
            "updates. Theanoletsusachieveallofthisinjustthesefewlines.\n",
            "Theonlythinghiddenis\n",
            "thatcomputingthecostinvolvesacalltothecostmethodfortheoutputlayer;thatcodeis\n",
            "elsewhereinnetwork3.py. Butthatcodeisshortandsimple,anyway. Withallthesethings\n",
            "defined,thestageissettodefinethetrain_mbfunction,aTheanosymbolicfunctionwhich\n",
            "usestheupdatestoupdatetheNetworkparameters,givenamini-batchindex. Similarly,\n",
            "validate_mb_accuracyandtest_mb_accuracycomputetheaccuracyoftheNetworkon\n",
            "anygivenmini-batchofvalidationortestdata. Byaveragingoverthesefunctions,wewill\n",
            "beabletocomputeaccuraciesontheentirevalidationandtestdatasets. The remainder of the SGD method is self-explanatory – we simply iterate over the\n",
            "epochs,repeatedlytrainingthenetworkonmini-batchesoftrainingdata,andcomputing\n",
            "thevalidationandtestaccuracies. 6\n",
            "Okay,we’venowunderstoodthemostimportantpiecesofcodeinnetwork3.py.\n",
            "Let’s\n",
            "takeabrieflookattheentireprogram. Youdon’tneedtoreadthroughthisindetail,butyou\n",
            "mayenjoyglancingoverit,andperhapsdivingdownintoanypiecesthatstrikeyourfancy. Thebestwaytoreallyunderstanditis,ofcourse,bymodifyingit,addingextrafeatures,or\n",
            "refactoringanythingyouthinkcouldbedonemoreelegantly. Afterthecode,therearesome\n",
            "problemswhichcontainafewstartersuggestionsforthingstodo. Here’sthecode22:\n",
            "\"\"\"network3.py\n",
            "~~~~~~~~~~~~~~\n",
            "A Theano-based program for training and running simple neural\n",
            "networks. Supports several layer types (fully connected, convolutional, max\n",
            "pooling, softmax), and activation functions (sigmoid, tanh, and\n",
            "rectified linear units, with more easily added). When run on a CPU, this program is much faster than network.py and\n",
            "network2.py. However, unlike network.py and network2.py it can also\n",
            "be run on a GPU, which makes it faster still. Because the code is based on Theano, the code is different in many\n",
            "ways from network.py and network2.py. However, where possible I have\n",
            "tried to maintain consistency with the earlier programs. In\n",
            "particular, the API is similar to network2.py. Note that I have\n",
            "focused on making the code simple, easily readable, and easily\n",
            "modifiable.\n",
            "It is not optimized, and omits many desirable features. This program incorporates ideas from the Theano documentation on\n",
            "convolutional neural nets (notably,\n",
            "http://deeplearning.net/tutorial/lenet.html ), from Misha Denil’s\n",
            "implementation of dropout (https://github.com/mdenil/dropout ), and\n",
            "from Chris Olah (http://colah.github.io ). 22UsingTheanoonaGPUcanbealittletricky.Inparticular,it’seasytomakethemistakeofpulling\n",
            "dataofftheGPU,whichcanslowthingsdownalot.I’vetriedtoavoidthis.Withthatsaid,thiscode\n",
            "cancertainlybespedupquiteabitfurtherwithcarefuloptimizationofTheano’sconfiguration.Seethe\n",
            "Theanodocumentationformoredetails. \n",
            "(cid:12)\n",
            "6.3.\n",
            "Thecodeforourconvolutionalnetworks (cid:12) 191\n",
            "(cid:12)\n",
            "Written for Theano 0.6 and 0.7, needs some changes for more recent\n",
            "versions of Theano. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import cPickle\n",
            "import gzip\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "import theano\n",
            "import theano.tensor as T\n",
            "from theano.tensor.nnet import conv\n",
            "from theano.tensor.nnet import softmax\n",
            "from theano.tensor import shared_randomstreams\n",
            "from theano.tensor.signal import downsample\n",
            "# Activation functions for neurons\n",
            "def linear(z): return z 6\n",
            "def ReLU(z): return T.maximum(0.0, z)\n",
            "from theano.tensor.nnet import sigmoid\n",
            "from theano.tensor import tanh\n",
            "#### Constants\n",
            "GPU = True\n",
            "if GPU:\n",
            "print \"Trying to run under a GPU. If this is not desired, then modify \"+\\\n",
            "\"network3.py\\nto set the GPU flag to False.\"\n",
            "try: theano.config.device = ’gpu’\n",
            "except: pass # it’s already set\n",
            "theano.config.floatX = ’float32’\n",
            "else:\n",
            "print \"Running with a CPU. If this is not desired, then the modify \"+\\\n",
            "\"network3.py to set\\nthe GPU flag to True.\"\n",
            "#### Load the MNIST data\n",
            "def load_data_shared(filename=\"../data/mnist.pkl.gz\"):\n",
            "f = gzip.open(filename, ’rb’)\n",
            "training_data, validation_data, test_data = cPickle.load(f)\n",
            "f.close()\n",
            "def shared(data):\n",
            "\"\"\"Place the data into shared variables. This allows Theano to copy\n",
            "the data to the GPU, if one is available. \"\"\"\n",
            "shared_x = theano.shared(\n",
            "np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
            "shared_y = theano.shared(\n",
            "np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
            "return shared_x, T.cast(shared_y, \"int32\")\n",
            "return [shared(training_data), shared(validation_data), shared(test_data)]\n",
            "#### Main class used to construct and train networks\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of ‘layers‘, describing the network architecture, and\n",
            "a value for the ‘mini_batch_size‘ to be used during training\n",
            "\n",
            "(cid:12)\n",
            "192 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "6 training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches.\n",
            "i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 193\n",
            "(cid:12)\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out,\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format( 6\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(’The corresponding test accuracy is {0:.2%}’.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "#### Define layer types\n",
            "class ConvPoolLayer(object):\n",
            "\"\"\"Used to create a combination of a convolutional and a max-pooling\n",
            "layer. A more sophisticated implementation would separate the\n",
            "two, but for our purposes we’ll always use them together, and it\n",
            "simplifies the code, so it makes sense to combine them.\n",
            "\"\"\"\n",
            "def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
            "activation_fn=sigmoid):\n",
            "\"\"\"‘filter_shape‘ is a tuple of length 4, whose entries are the number\n",
            "of filters, the number of input feature maps, the filter height, and the\n",
            "filter width. ‘image_shape‘ is a tuple of length 4, whose entries are the\n",
            "mini-batch size, the number of input feature maps, the image\n",
            "height, and the image width. ‘poolsize‘ is a tuple of length 2, whose entries are the y and\n",
            "x pooling sizes. \"\"\"\n",
            "self.filter_shape = filter_shape\n",
            "\n",
            "(cid:12)\n",
            "194 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "self.image_shape = image_shape\n",
            "self.poolsize = poolsize\n",
            "self.activation_fn=activation_fn\n",
            "# initialize weights and biases\n",
            "n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape(self.image_shape)\n",
            "conv_out = conv.conv2d(\n",
            "6 input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
            "image_shape=self.image_shape)\n",
            "pooled_out = downsample.max_pool_2d(\n",
            "input=conv_out, ds=self.poolsize, ignore_border=True)\n",
            "self.output = self.activation_fn(\n",
            "pooled_out + self.b.dimshuffle(’x’, 0, ’x’, ’x’))\n",
            "self.output_dropout = self.output # no dropout in the convolutional layers\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 195\n",
            "(cid:12)\n",
            "class SoftmaxLayer(object):\n",
            "def __init__(self, n_in, n_out, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.zeros((n_out,), dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer( 6\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def cost(self, net):\n",
            "\"Return the log-likelihood cost.\"\n",
            "return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "#### Miscellanea\n",
            "def size(data):\n",
            "\"Return the size of the dataset ‘data‘.\"\n",
            "return data[0].get_value(borrow=True).shape[0]\n",
            "def dropout_layer(layer, p_dropout):\n",
            "srng = shared_randomstreams.RandomStreams(\n",
            "np.random.RandomState(0).randint(999999))\n",
            "mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
            "return layer*T.cast(mask, theano.config.floatX)\n",
            "Problems\n",
            "At present, the SGD method requires the user to manually choose the number of\n",
            "• epochstotrainfor. Earlierinthebookwediscussedanautomatedwayofselecting\n",
            "thenumberofepochstotrainfor,knownasearlystopping.\n",
            "Modifynetwork3.pyto\n",
            "implementearlystopping. AddaNetworkmethodtoreturntheaccuracyonanarbitrarydataset. • ModifytheSGDmethodtoallowthelearningrateηtobeafunctionoftheepoch\n",
            "• number. Hint: Afterworkingonthisproblemforawhile,youmayfinditusefultosee\n",
            "thediscussionatthislink. EarlierinthechapterIdescribedatechniqueforexpandingthetrainingdatabyapply-\n",
            "• ing(small)rotations,skewing,andtranslation. Modifynetwork3.pytoincorporate\n",
            "allthesetechniques. Note: Unlessyouhaveatremendousamountofmemory, itis\n",
            "notpracticaltoexplicitlygeneratetheentireexpandeddataset. Soyoushouldconsider\n",
            "\n",
            "(cid:12)\n",
            "196 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "alternateapproaches. Addtheabilitytoloadandsavenetworkstonetwork3.py. • Ashortcomingofthecurrentcodeisthatitprovidesfewdiagnostictools. Canyou\n",
            "• thinkofanydiagnosticstoaddthatwouldmakeiteasiertounderstandtowhatextent\n",
            "anetworkisoverfitting? Addthem. We’veusedthesameinitializationprocedureforrectifiedlinearunitsasforsigmoid\n",
            "• (andtanh)neurons. Ourargumentforthatinitializationwasspecifictothesigmoid\n",
            "function.Consideranetworkmadeentirelyofrectifiedlinearunits(includingoutputs). Showthatrescalingalltheweightsinthenetworkbyaconstantfactorc>0simply\n",
            "rescalestheoutputsbyafactor cL 1, where L isthenumberoflayers. Howdoes\n",
            "−\n",
            "thischangeifthefinallayerisasoftmax? Whatdoyouthinkofusingthesigmoid\n",
            "initialization procedure for the rectified linear units? Can you think of a better\n",
            "initialization procedure?\n",
            "Note: This is a very open-ended problem, not something\n",
            "withasimpleself-containedanswer.\n",
            "Still,consideringtheproblemwillhelpyoubetter\n",
            "understandnetworkscontainingrectifiedlinearunits. Ouranalysisoftheunstablegradientproblemwasforsigmoidneurons. Howdoes\n",
            "6 • theanalysischangefornetworksmadeupofrectifiedlinearunits? Canyouthinkofa\n",
            "goodwayofmodifyingsuchanetworksoitdoesn’tsufferfromtheunstablegradient\n",
            "problem? Note: Thewordgoodinthesecondpartofthismakestheproblemaresearch\n",
            "problem. It’sactuallyeasytothinkofwaysofmakingsuchmodifications.\n",
            "ButIhaven’t\n",
            "investigatedinenoughdepthtoknowofareallygoodtechnique. 6.4 Recent progress in image recognition\n",
            "In1998,theyearMNISTwasintroduced,ittookweekstotrainastate-of-the-artworkstation\n",
            "toachieveaccuraciessubstantiallyworsethanthosewecanachieveusingaGPUandless\n",
            "thananhouroftraining. Thus, MNISTisnolongeraproblemthatpushesthelimitsof\n",
            "availabletechnique;rather,thespeedoftrainingmeansthatitisaproblemgoodforteaching\n",
            "andlearningpurposes. Meanwhile,thefocusofresearchhasmovedon,andmodernwork\n",
            "involvesmuchmorechallengingimagerecognitionproblems.Inthissection,Ibrieflydescribe\n",
            "somerecentworkonimagerecognitionusingneuralnetworks. Thesectionisdifferenttomostofthebook. ThroughthebookI’vefocusedonideaslikely\n",
            "tobeoflastinginterest–ideassuchasbackpropagation,regularization,andconvolutional\n",
            "networks. I’vetriedtoavoidresultswhicharefashionableasIwrite,butwhoselong-term\n",
            "valueisunknown. Inscience,suchresultsaremoreoftenthannotephemerawhichfadeand\n",
            "havelittlelastingimpact. Giventhis,askepticmightsay: “well,surelytherecentprogress\n",
            "inimagerecognitionisanexampleofsuchephemera? Inanothertwoorthreeyears,things\n",
            "willhavemovedon. Sosurelytheseresultsareonlyofinteresttoafewspecialistswhowant\n",
            "tocompeteattheabsolutefrontier? Whybotherdiscussingit?”\n",
            "Such a skeptic is right that some of the finer details of recent papers will gradually\n",
            "diminishinperceivedimportance. Withthatsaid,thepastfewyearshaveseenextraordinary\n",
            "improvementsusingdeepnetstoattackextremelydifficultimagerecognitiontasks.\n",
            "Imagine\n",
            "ahistorianofsciencewritingaboutcomputervisionintheyear2100. Theywillidentifythe\n",
            "years2011to2015(andprobablyafewyearsbeyond)asatimeofhugebreakthroughs,\n",
            "drivenbydeepconvolutionalnets. Thatdoesn’tmeandeepconvolutionalnetswillstillbe\n",
            "usedin2100,muchlessdetailedideassuchasdropout,rectifiedlinearunits,andsoon. But\n",
            "itdoesmeanthatanimportanttransitionistakingplace,rightnow,inthehistoryofideas. \n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 197\n",
            "(cid:12)\n",
            "It’sabitlikewatchingthediscoveryoftheatom,ortheinventionofantibiotics: invention\n",
            "anddiscoveryonahistoricscale. Andsowhilewewon’tdigdowndeepintodetails,it’s\n",
            "worthgettingsomeideaoftheexcitingdiscoveriescurrentlybeingmade.\n",
            "The2012LRMDpaper: Letmestartwitha2012paper23fromagroupofresearchers\n",
            "from Stanford and Google. I’ll refer to this paper as LRMD, after the last names of the\n",
            "firstfourauthors. LRMDusedaneuralnetworktoclassifyimagesfromImageNet,avery\n",
            "challengingimagerecognitionproblem. The2011ImageNetdatathattheyusedincluded\n",
            "16millionfullcolorimages,in20thousandcategories. Theimageswerecrawledfromthe\n",
            "opennet,andclassifiedbyworkersfromAmazon’sMechanicalTurkservice. Here’safew\n",
            "ImageNetimages24:\n",
            "6\n",
            "Theseare,respectively,inthecategoriesforbeadingplane,brownrootrotfungus,scalded\n",
            "milk,andthecommonroundworm. Ifyou’relookingforachallenge,Iencourageyouto\n",
            "visitImageNet’slistofhandtools,whichdistinguishesbetweenbeadingplanes,blockplanes,\n",
            "chamferplanes,andaboutadozenothertypesofplane,amongstothercategories. Idon’t\n",
            "knowaboutyou,butIcannotconfidentlydistinguishbetweenallthesetooltypes. Thisis\n",
            "obviouslyamuchmorechallengingimagerecognitiontaskthanMNIST!LRMD’snetwork\n",
            "obtainedarespectable15.8percentaccuracyforcorrectlyclassifyingImageNetimages. That\n",
            "maynotsoundimpressive,butitwasahugeimprovementoverthepreviousbestresult\n",
            "of9.3percentaccuracy. Thatjumpsuggestedthatneuralnetworksmightofferapowerful\n",
            "approachtoverychallengingimagerecognitiontasks,suchasImageNet. The2012KSHpaper: TheworkofLRMDwasfollowedbya2012paperofKrizhevsky,\n",
            "SutskeverandHinton(KSH)25.KSHtrainedandtestedadeepconvolutionalneuralnetwork\n",
            "usingarestrictedsubsetoftheImageNetdata. Thesubsettheyusedcamefromapopular\n",
            "machinelearningcompetition–theImageNetLarge-ScaleVisualRecognitionChallenge\n",
            "(ILSVRC).Usingacompetitiondatasetgavethemagoodwayofcomparingtheirapproach\n",
            "tootherleadingtechniques. TheILSVRC-2012trainingsetcontainedabout1.2million\n",
            "ImageNet images, drawn from 1,000 categories. The validation and test sets contained\n",
            "50,000and150,000images,respectively,drawnfromthesame1,000categories. OnedifficultyinrunningtheILSVRCcompetitionisthatmanyImageNetimagescontain\n",
            "multipleobjects. Supposeanimageshowsalabradorretrieverchasingasoccerball. The\n",
            "so-called“correct”ImageNetclassificationoftheimagemightbeasalabradorretriever. 23Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning,byQuocLe,Marc’Aurelio\n",
            "Ranzato,RajatMonga,MatthieuDevin,KaiChen,GregCorrado,JeffDean,andAndrewNg(2012).\n",
            "Notethatthedetailedarchitectureofthenetworkusedinthepaperdifferedinmanydetailsfromthe\n",
            "deepconvolutionalnetworkswe’vebeenstudying.Broadlyspeaking,however,LRMDisbasedonmany\n",
            "similarideas. 24Thesearefromthe2014dataset,whichissomewhatchangedfrom2011.Qualitatively,however,\n",
            "thedatasetisextremelysimilar.DetailsaboutImageNetareavailableintheoriginalImageNetpaper,\n",
            "ImageNet:alarge-scalehierarchicalimagedatabase,byJiaDeng,WeiDong,RichardSocher,Li-JiaLi,\n",
            "KaiLi,andLiFei-Fei(2009). 25ImageNetclassificationwithdeepconvolutionalneuralnetworks,byAlexKrizhevsky,IlyaSutskever,\n",
            "andGeoffreyE.Hinton(2012).\n",
            "\n",
            "(cid:12)\n",
            "198 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Shouldanalgorithmbepenalizedifitlabelstheimageasasoccerball? Becauseofthis\n",
            "ambiguity,analgorithmwasconsideredcorrectiftheactualImageNetclassificationwas\n",
            "amongthe5classificationsthealgorithmconsideredmostlikely. Bythistop-5criterion,\n",
            "KSH’sdeepconvolutionalnetworkachievedanaccuracyof84.7percent,vastlybetterthan\n",
            "thenext-bestcontestentry,whichachievedanaccuracyof73.8percent. Usingthemore\n",
            "restrictivemetricofgettingthelabelexactlyright,KSH’snetworkachievedanaccuracyof\n",
            "63.3percent. It’sworthbrieflydescribingKSH’snetwork,sinceithasinspiredmuchsubsequentwork. It’salso,asweshallsee,closelyrelatedtothenetworkswetrainedearlierinthischapter,\n",
            "albeitmoreelaborate. KSHusedadeepconvolutionalneuralnetwork,trainedontwoGPUs. TheyusedtwoGPUsbecausetheparticulartypeofGPUtheywereusing(anNVIDIAGeForce\n",
            "GTX580)didn’thaveenoughon-chipmemorytostoretheirentirenetwork. Sotheysplit\n",
            "thenetworkintotwoparts,partitionedacrossthetwoGPUs.\n",
            "TheKSHnetworkhas7layersofhiddenneurons. Thefirst5hiddenlayersareconvolu-\n",
            "tionallayers(somewithmax-pooling),whilethenext2layersarefully-connectedlayers. Theoutputlayerisa1,000-unitsoftmaxlayer,correspondingtothe1,000imageclasses. 6 Here’sasketchofthenetwork,takenfromtheKSHpaper26.\n",
            "Thedetailsareexplainedbelow. Notethatmanylayersaresplitinto2parts,correspondingtothe2GPUs. Theinputlayercontains3 224 224neurons,representingtheRGBvaluesfora224 224\n",
            "image. Recallthat,asmen×tione×dearlier,ImageNetcontainsimagesofvaryingresol×ution. Thisposesaproblem,sinceaneuralnetwork’sinputlayerisusuallyofafixedsize. KSHdealt\n",
            "withthisbyrescalingeachimagesotheshortersidehadlength256. Theythencropped\n",
            "outa256 256areainthecenteroftherescaledimage. Finally,KSHextractedrandom\n",
            "224 224×subimages(andhorizontalreflections)fromthe256 256images. Theydidthis\n",
            "rand×omcroppingasawayofexpandingthetrainingdata,an×dthusreducingoverfitting. ThisisparticularlyhelpfulinalargenetworksuchasKSH’s. Itwasthese224 224images\n",
            "whichwereusedasinputstothenetwork. Inmostcasesthecroppedimagestil×lcontainsthe\n",
            "mainobjectfromtheuncroppedimage. MovingontothehiddenlayersinKSH’snetwork,thefirsthiddenlayerisaconvolutional\n",
            "layer,withamax-poolingstep. Ituseslocalreceptivefieldsofsize11 11,andastride\n",
            "lengthof4pixels. Thereareatotalof96featuremaps. Thefeaturemaps×aresplitintotwo\n",
            "groupsof48each,withthefirst48featuremapsresidingononeGPU,andthesecond48\n",
            "featuremapsresidingontheotherGPU.Themax-poolinginthisandlaterlayersisdonein\n",
            "3 3regions,butthepoolingregionsareallowedtooverlap,andarejust2pixelsapart. ×\n",
            "26ThankstoIlyaSutskever.\n",
            "\n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 199\n",
            "(cid:12)\n",
            "Thesecondhiddenlayerisalsoaconvolutionallayer,withamax-poolingstep. Ituses\n",
            "5 5localreceptivefields,andthere’satotalof256featuremaps,splitinto128oneach\n",
            "GP×U.Notethatthefeaturemapsonlyuse48inputchannels,notthefull96outputfromthe\n",
            "previouslayer(aswouldusuallybethecase). Thisisbecauseanysinglefeaturemaponly\n",
            "usesinputsfromthesameGPU.Inthissensethenetworkdepartsfromtheconvolutional\n",
            "architecturewedescribedearlierinthechapter,thoughobviouslythebasicideaisstillthe\n",
            "same. Thethird,fourthandfifthhiddenlayersareconvolutionallayers,butunliketheprevious\n",
            "layers,theydonotinvolvemax-pooling. Theirrespectivesparametersare: (3)384feature\n",
            "maps,with3 3localreceptivefields,and256inputchannels;(4)384featuremaps,with\n",
            "3 3localrece×ptivefields,and192inputchannels;and(5)256featuremaps,with3 3local\n",
            "re×ceptivefields,and192inputchannels. Notethatthethirdlayerinvolvessomein×ter-GPU\n",
            "communication(asdepictedinthefigure)inorderthatthefeaturemapsuseall256input\n",
            "channels. Thesixthandseventhhiddenlayersarefully-connectedlayers,with4,096neuronsin\n",
            "eachlayer. Theoutputlayerisa1,000-unitsoftmaxlayer. 6\n",
            "TheKSHnetworktakesadvantageofmanytechniques. Insteadofusingthesigmoidor\n",
            "tanhactivationfunctions,KSHuserectifiedlinearunits,whichspeduptrainingsignificantly. KSH’snetworkhadroughly60millionlearnedparameters,andwasthus,evenwiththelarge\n",
            "trainingset,susceptibletooverfitting. Toovercomethis,theyexpandedthetrainingsetusing\n",
            "therandomcroppingstrategywediscussedabove. Theyalsofurtheraddressedoverfitting\n",
            "byusingavariantofl2regularization,anddropout. Thenetworkitselfwastrainedusing\n",
            "momentum-basedmini-batchstochasticgradientdescent. That’s an overview of many of the core ideas in the KSH paper.\n",
            "I’ve omitted some\n",
            "details, for which you should look at the paper.\n",
            "You can also look at Alex Krizhevsky’s\n",
            "cuda-convnet(andsuccessors),whichcontainscodeimplementingmanyoftheideas. A\n",
            "Theano-basedimplementationhasalsobeendeveloped27,withthecodeavailablehere. The\n",
            "codeisrecognizablyalongsimilarlinestothatdevelopedinthischapter,althoughtheuseof\n",
            "multipleGPUscomplicatesthingssomewhat. TheCaffeneuralnetsframeworkalsoincludes\n",
            "aversionoftheKSHnetwork,seetheirModelZoofordetails. The 2014 ILSVRC competition: Since 2012, rapid progress continues to be made. Considerthe2014ILSVRCcompetition. Asin2012,itinvolvedatrainingsetof1.2million\n",
            "images, in 1,000 categories, and the figure of merit was whether the top 5 predictions\n",
            "includedthecorrectcategory. Thewinningteam,basedprimarilyatGoogle28,usedadeep\n",
            "convolutionalnetworkwith22layersofneurons. TheycalledtheirnetworkGoogLeNet,\n",
            "asahomagetoLeNet-5.\n",
            "GoogLeNetachievedatop-5accuracyof93.33percent,agiant\n",
            "improvementoverthe2013winner(Clarifai,with88.3percent),andthe2012winner(KSH,\n",
            "with84.7percent). JusthowgoodisGoogLeNet’s93.33percentaccuracy?\n",
            "In2014ateamofresearchers\n",
            "wroteasurveypaperabouttheILSVRCcompetition29. Oneofthequestionstheyaddressis\n",
            "howwellhumansperformonILSVRC.Todothis,theybuiltasystemwhichletshumans\n",
            "27Theano-basedlarge-scalevisualrecognitionwithmultipleGPUs,byWeiguangDing,RuoyanWang,\n",
            "FeiMao,andGrahamTaylor(2014). 28Goingdeeperwithconvolutions,byChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,Scott\n",
            "Reed,DragomirAnguelov,DumitruErhan,VincentVanhoucke,andAndrewRabinovich(2014). 29ImageNetlargescalevisualrecognitionchallenge,byOlgaRussakovsky,JiaDeng,HaoSu,Jonathan\n",
            "Krause,SanjeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,\n",
            "AlexanderC.Berg,andLiFei-Fei(2014). \n",
            "(cid:12)\n",
            "200 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "classifyILSVRCimages. Asoneoftheauthors,AndrejKarpathy,explainsinaninformative\n",
            "blogpost,itwasalotoftroubletogetthehumansuptoGoogLeNet’sperformance:\n",
            "...thetaskoflabelingimageswith5outof1000categoriesquicklyturned\n",
            "outtobeextremelychallenging,evenforsomefriendsinthelabwhohave\n",
            "beenworkingonILSVRCanditsclassesforawhile. Firstwethought\n",
            "we would put it up on [Amazon Mechanical Turk]. Then we thought\n",
            "wecouldrecruitpaidundergrads. ThenIorganizedalabelingpartyof\n",
            "intenselabelingeffortonlyamongthe(expertlabelers)inourlab. ThenI\n",
            "developedamodifiedinterfacethatusedGoogLeNetpredictionstoprune\n",
            "thenumberofcategoriesfrom1000toonlyabout100. Itwasstilltoo\n",
            "hard–peoplekeptmissingcategoriesandgettinguptorangesof13–15%\n",
            "errorrates. IntheendIrealizedthattogetanywherecompetitivelyclose\n",
            "toGoogLeNet,itwasmostefficientifIsatdownandwentthroughthe\n",
            "painfullylongtrainingprocessandthesubsequentcarefulannotation\n",
            "processmyself... Thelabelinghappenedatarateofabout1perminute,\n",
            "butthisdecreasedovertime... Someimagesareeasilyrecognized,while\n",
            "6\n",
            "some images (such as those of fine-grained breeds of dogs, birds, or\n",
            "monkeys)canrequiremultipleminutesofconcentratedeffort. Ibecame\n",
            "verygoodatidentifyingbreedsofdogs...\n",
            "Basedonthesampleofimages\n",
            "Iworkedon,theGoogLeNetclassificationerrorturnedouttobe6.8%... My own error in the end turned out to be 5.1%, approximately 1.7%\n",
            "better. In other words, an expert human, working painstakingly, was with great effort able to\n",
            "narrowlybeatthedeepneuralnetwork. Infact, Karpathyreportsthatasecondhuman\n",
            "expert,trainedonasmallersampleofimages,wasonlyabletoattaina12.0percenttop-5\n",
            "errorrate,significantlybelowGoogLeNet’sperformance. Abouthalftheerrorsweredueto\n",
            "theexpert“failingtospotandconsiderthegroundtruthlabelasanoption”.\n",
            "These are astonishing results. Indeed, since this work, several teams have reported\n",
            "systems whose top-5 error rate is actually better than 5.1%. This has sometimes been\n",
            "reportedinthemediaasthesystemshavingbetter-than-humanvision.\n",
            "Whiletheresultsare\n",
            "genuinelyexciting,therearemanycaveatsthatmakeitmisleadingtothinkofthesystems\n",
            "ashavingbetter-than-humanvision. TheILSVRCchallengeisinmanywaysaratherlimited\n",
            "problem – a crawl of the open web is not necessarily representative of images found in\n",
            "applications! And,ofcourse,thetop-5criterionisquiteartificial. Wearestillalongway\n",
            "fromsolvingtheproblemofimagerecognitionor,morebroadly,computervision. Still,it’s\n",
            "extremelyencouragingtoseesomuchprogressmadeonsuchachallengingproblem,over\n",
            "justafewyears. Otheractivity: I’vefocusedonImageNet,butthere’saconsiderableamountofother\n",
            "activityusingneuralnetstodoimagerecognition. Letmebrieflydescribeafewinteresting\n",
            "recentresults,justtogivetheflavourofsomecurrentwork. OneencouragingpracticalsetofresultscomesfromateamatGoogle,whoapplieddeep\n",
            "convolutionalnetworkstotheproblemofrecognizingstreetnumbersinGoogle’sStreetView\n",
            "imagery30. Intheirpaper,theyreportdetectingandautomaticallytranscribingnearly100\n",
            "millionstreetnumbersatanaccuracysimilartothatofahumanoperator. Thesystemisfast:\n",
            "theirsystemtranscribedallofStreetView’simagesofstreetnumbersinFranceinlessthan\n",
            "30Multi-digitNumberRecognitionfromStreetViewImageryusingDeepConvolutionalNeuralNet-\n",
            "works,byIanJ.Goodfellow,YaroslavBulatov,JulianIbarz,SachaArnoud,andVinayShet(2013). \n",
            "(cid:12)\n",
            "6.4.\n",
            "Recentprogressinimagerecognition (cid:12) 201\n",
            "(cid:12)\n",
            "anhour! Theysay: “Havingthisnewdatasetsignificantlyincreasedthegeocodingqualityof\n",
            "GoogleMapsinseveralcountriesespeciallytheonesthatdidnotalreadyhaveothersources\n",
            "ofgoodgeocoding.” Andtheygoontomakethebroaderclaim: “Webelievewiththismodel\n",
            "wehavesolved[opticalcharacterrecognition]forshortsequences[ofcharacters]formany\n",
            "applications.”\n",
            "I’veperhapsgiventheimpressionthatit’sallaparadeofencouragingresults. Ofcourse,\n",
            "someofthemostinterestingworkreportsonfundamentalthingswedon’tyetunderstand. Forinstance,a2013paper31showedthatdeepnetworksmaysufferfromwhatareeffectively\n",
            "blindspots. Considerthelinesofimagesbelow. OntheleftisanImageNetimageclassified\n",
            "correctlybytheirnetwork. Ontherightisaslightlyperturbedimage(theperturbationisin\n",
            "themiddle)whichisclassifiedincorrectlybythenetwork. Theauthorsfoundthatthereare\n",
            "such“adversarial”imagesforeverysampleimage,notjustafewspecialones. 6\n",
            "Thisisadisturbingresult.ThepaperusedanetworkbasedonthesamecodeasKSH’snetwork\n",
            "–thatis,justthetypeofnetworkthatisbeingincreasinglywidelyused. Whilesuchneural\n",
            "networkscomputefunctionswhichare,inprinciple,continuous,resultslikethissuggestthat\n",
            "inpracticethey’relikelytocomputefunctionswhichareverynearlydiscontinuous. Worse,\n",
            "they’llbediscontinuousinwaysthatviolateourintuitionaboutwhatisreasonablebehavior. That’sconcerning. Furthermore,it’snotyetwellunderstoodwhat’scausingthediscontinuity:\n",
            "isitsomethingaboutthelossfunction? Theactivationfunctionsused? Thearchitectureof\n",
            "thenetwork?\n",
            "Somethingelse? Wedon’tyetknow.\n",
            "Now,theseresultsarenotquiteasbadastheysound. Althoughsuchadversarialimages\n",
            "arecommon,they’realsounlikelyinpractice. Asthepapernotes:\n",
            "Theexistenceoftheadversarialnegativesappearstobeincontradiction\n",
            "withthenetwork’sabilitytoachievehighgeneralizationperformance. Indeed,ifthenetworkcangeneralizewell,howcanitbeconfusedby\n",
            "theseadversarialnegatives,whichareindistinguishablefromtheregular\n",
            "examples? Theexplanationisthatthesetofadversarialnegativesisof\n",
            "extremelylowprobability,andthusisnever(orrarely)observedinthe\n",
            "31Intriguingpropertiesofneuralnetworks,byChristianSzegedy,WojciechZaremba,IlyaSutskever,\n",
            "JoanBruna,DumitruErhan,IanGoodfellow,andRobFergus(2013)\n",
            "\n",
            "(cid:12)\n",
            "202 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "testset,yetitisdense(muchliketherationalnumbers),andsoitisfound\n",
            "nearvirtuallyeverytestcase. Nonetheless, it is distressing that we understand neural nets so poorly that this kind of\n",
            "resultshouldbearecentdiscovery. Ofcourse,amajorbenefitoftheresultsisthatthey\n",
            "havestimulatedmuchfollowupwork. Forexample,onerecentpaper32 showsthatgiven\n",
            "atrainednetworkit’spossibletogenerateimageswhichlooktoahumanlikewhitenoise,\n",
            "butwhichthenetworkclassifiesasbeinginaknowncategorywithaveryhighdegreeof\n",
            "confidence. Thisisanotherdemonstrationthatwehavealongwaytogoinunderstanding\n",
            "neuralnetworksandtheiruseinimagerecognition. Despiteresultslikethis,theoverallpictureisencouraging.\n",
            "We’reseeingrapidprogress\n",
            "onextremelydifficultbenchmarks,likeImageNet. We’realsoseeingrapidprogressinthe\n",
            "solutionofreal-worldproblems,likerecognizingstreetnumbersinStreetView. Butwhilethis\n",
            "isencouragingit’snotenoughjusttoseeimprovementsonbenchmarks,orevenreal-world\n",
            "applications. Therearefundamentalphenomenawhichwestillunderstandpoorly,such\n",
            "astheexistenceofadversarialimages. Whensuchfundamentalproblemsarestillbeing\n",
            "discovered(nevermindsolved),itisprematuretosaythatwe’renearsolvingtheproblemof\n",
            "6\n",
            "imagerecognition. Atthesametimesuchproblemsareanexcitingstimulustofurtherwork. 6.5 Other approaches to deep neural nets\n",
            "Throughthisbook,we’veconcentratedonasingleproblem: classifyingtheMNISTdigits. It’sajuicyproblemwhichforcedustounderstandmanypowerfulideas: stochasticgradient\n",
            "descent,backpropagation,convolutionalnets,regularization,andmore.Butit’salsoanarrow\n",
            "problem. Ifyoureadtheneuralnetworksliterature,you’llrunintomanyideaswehaven’t\n",
            "discussed: recurrentneuralnetworks,Boltzmannmachines,generativemodels,transfer\n",
            "learning,reinforcementlearning,andsoon,onandonâA˘˛eandon! Neuralnetworksisa\n",
            "vastfield. However,manyimportantideasarevariationsonideaswe’vealreadydiscussed,\n",
            "andcanbeunderstoodwithalittleeffort.\n",
            "InthissectionIprovideaglimpseoftheseasyet\n",
            "unseenvistas. Thediscussionisn’tdetailed,norcomprehensive–thatwouldgreatlyexpand\n",
            "thebook. Rather,it’simpressionistic,anattempttoevoketheconceptualrichnessofthe\n",
            "field,andtorelatesomeofthoserichestowhatwe’vealreadyseen. Throughthesection,I’ll\n",
            "provideafewlinkstoothersources,asentreestolearnmore. Ofcourse,manyoftheselinks\n",
            "willsoonbesuperseded,andyoumaywishtosearchoutmorerecentliterature.\n",
            "Thatpoint\n",
            "notwithstanding,Iexpectmanyoftheunderlyingideastobeoflastinginterest. Recurrentneuralnetworks(RNNs): Inthefeedforwardnetswe’vebeenusingthere\n",
            "isasingleinputwhichcompletelydeterminestheactivationsofalltheneuronsthrough\n",
            "theremaininglayers. It’saverystaticpicture: everythinginthenetworkisfixed,witha\n",
            "frozen,crystallinequalitytoit. Butsupposeweallowtheelementsinthenetworktokeep\n",
            "changinginadynamicway. Forinstance,thebehaviourofhiddenneuronsmightnotjustbe\n",
            "determinedbytheactivationsinprevioushiddenlayers,butalsobytheactivationsatearlier\n",
            "times. Indeed,aneuron’sactivationmightbedeterminedinpartbyitsownactivationatan\n",
            "earliertime. That’scertainlynotwhathappensinafeedforwardnetwork. Orperhapsthe\n",
            "activationsofhiddenandoutputneuronswon’tbedeterminedjustbythecurrentinputto\n",
            "thenetwork,butalsobyearlierinputs. 32DeepNeuralNetworksareEasilyFooled:HighConfidencePredictionsforUnrecognizableImages,\n",
            "byAnhNguyen,JasonYosinski,andJeffClune(2014). \n",
            "(cid:12)\n",
            "6.5. Otherapproachestodeepneuralnets (cid:12) 203\n",
            "(cid:12)\n",
            "Neuralnetworkswiththiskindoftime-varyingbehaviourareknownasrecurrentneural\n",
            "networksorRNNs. Therearemanydifferentwaysofmathematicallyformalizingtheinformal\n",
            "descriptionofrecurrentnetsgiveninthelastparagraph.\n",
            "Youcangettheflavourofsomeof\n",
            "thesemathematicalmodelsbyglancingattheWikipediaarticleonRNNs. AsIwrite,that\n",
            "pagelistsnofewerthan13differentmodels. Butmathematicaldetailsaside,thebroadidea\n",
            "isthatRNNsareneuralnetworksinwhichthereissomenotionofdynamicchangeovertime. And,notsurprisingly,they’reparticularlyusefulinanalysingdataorprocessesthatchange\n",
            "overtime. Suchdataandprocessesarisenaturallyinproblemssuchasspeechornatural\n",
            "language,forexample. OnewayRNNsarecurrentlybeingusedistoconnectneuralnetworksmorecloselyto\n",
            "traditionalwaysofthinkingaboutalgorithms, waysofthinkingbasedonconceptssuch\n",
            "asTuringmachinesand(conventional)programminglanguages. A2014paperdeveloped\n",
            "an RNN which could take as input a character-by-character description of a (very, very\n",
            "simple!) Pythonprogram,andusethatdescriptiontopredicttheoutput. Informally,the\n",
            "networkislearningto“understand”certainPythonprograms. Asecondpaper,alsofrom\n",
            "2014,usedRNNsasastartingpointtodevelopwhattheycalledaneuralTuringmachine\n",
            "6\n",
            "(NTM).Thisisauniversalcomputerwhoseentirestructurecanbetrainedusinggradient\n",
            "descent. TheytrainedtheirNTMtoinferalgorithmsforseveralsimpleproblems,suchas\n",
            "sortingandcopying. Asitstands,theseareextremelysimpletoymodels. LearningtoexecutethePythonpro-\n",
            "gramprint(398345+42598)doesn’tmakeanetworkintoafull-fledgedPythoninterpreter! It’snotclearhowmuchfurtheritwillbepossibletopushtheideas.\n",
            "Still,theresultsare\n",
            "intriguing. Historically,neuralnetworkshavedonewellatpatternrecognitionproblems\n",
            "whereconventionalalgorithmicapproacheshavetrouble. Viceversa,conventionalalgorith-\n",
            "micapproachesaregoodatsolvingproblemsthatneuralnetsaren’tsogoodat. No-one\n",
            "todayimplementsawebserveroradatabaseprogramusinganeuralnetwork! It’dbegreat\n",
            "todevelopunifiedmodelsthatintegratethestrengthsofbothneuralnetworksandmore\n",
            "traditionalapproachestoalgorithms. RNNsandideasinspiredbyRNNsmayhelpusdothat. RNNshavealsobeenusedinrecentyearstoattackmanyotherproblems. They’vebeen\n",
            "particularlyusefulinspeechrecognition. ApproachesbasedonRNNshave,forexample,\n",
            "setrecordsfortheaccuracyofphonemerecognition. They’vealsobeenusedtodevelop\n",
            "improvedmodelsofthelanguagepeopleusewhilespeaking. Betterlanguagemodelshelp\n",
            "disambiguateutterancesthatotherwisesoundalike.Agoodlanguagemodelwill,forexample,\n",
            "tellusthat“toinfinityandbeyond”ismuchmorelikelythan“twoinfinityandbeyond”,\n",
            "despitethefactthatthephrasessoundidentical. RNNshavebeenusedtosetnewrecords\n",
            "forcertainlanguagebenchmarks. Thisworkis,incidentally,partofabroaderuseofdeepneuralnetsofalltypes,notjust\n",
            "RNNs,inspeechrecognition.\n",
            "Forexample,anapproachbasedondeepnetshasachieved\n",
            "outstandingresultsonlargevocabularycontinuousspeechrecognition. Andanothersystem\n",
            "basedondeepnetshasbeendeployedinGoogle’sAndroidoperatingsystem(forrelated\n",
            "technicalwork,seeVincentVanhoucke’s2012–2015papers). I’vesaidalittleaboutwhatRNNscando,butnotsomuchabouthowtheywork.\n",
            "It\n",
            "perhapswon’tsurpriseyoutolearnthatmanyoftheideasusedinfeedforwardnetworkscan\n",
            "alsobeusedinRNNs. Inparticular,wecantrainRNNsusingstraightforwardmodificationsto\n",
            "gradientdescentandbackpropagation. Manyotherideasusedinfeedforwardnets,ranging\n",
            "fromregularizationtechniquestoconvolutionstotheactivationandcostfunctionsused,are\n",
            "alsousefulinrecurrentnets. Andsomanyofthetechniqueswe’vedevelopedinthebook\n",
            "\n",
            "(cid:12)\n",
            "204 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "canbeadaptedforusewithRNNs. Longshort-termmemoryunits(LSTMs): OnechallengeaffectingRNNsisthatearly\n",
            "modelsturnedouttobeverydifficulttotrain,hardereventhandeepfeedforwardnetworks. ThereasonistheunstablegradientproblemdiscussedinChapter5. Recallthattheusual\n",
            "manifestationofthisproblemisthatthegradientgetssmallerandsmallerasitispropagated\n",
            "back through layers. This makes learning in early layers extremely slow. The problem\n",
            "actuallygetsworseinRNNs,sincegradientsaren’tjustpropagatedbackwardthroughlayers,\n",
            "they’repropagatedbackwardthroughtime. Ifthenetworkrunsforalongtimethatcan\n",
            "makethegradientextremelyunstableandhardtolearnfrom.\n",
            "Fortunately,it’spossibleto\n",
            "incorporateanideaknownaslongshort-termmemoryunits(LSTMs)intoRNNs. Theunits\n",
            "wereintroducedbyHochreiterandSchmidhuberin1997withtheexplicitpurposeofhelping\n",
            "addresstheunstablegradientproblem.\n",
            "LSTMsmakeitmucheasiertogetgoodresultswhen\n",
            "trainingRNNs,andmanyrecentpapers(includingmanythatIlinkedabove)makeuseof\n",
            "LSTMsorrelatedideas. Deepbeliefnets,generativemodels,andBoltzmannmachines: Moderninterestin\n",
            "deeplearningbeganin2006,withpapersexplaininghowtotrainatypeofneuralnetwork\n",
            "6 knownasadeepbeliefnetwork(DBN)33.DBNswereinfluentialforseveralyears,buthave\n",
            "sincelessenedinpopularity,whilemodelssuchasfeedforwardnetworksandrecurrentneural\n",
            "netshavebecomefashionable. Despitethis,DBNshaveseveralpropertiesthatmakethem\n",
            "interesting. OnereasonDBNsareinterestingisthatthey’reanexampleofwhat’scalledagenerative\n",
            "model. Inafeedforwardnetwork,wespecifytheinputactivations,andtheydeterminethe\n",
            "activationsofthefeatureneuronslaterinthenetwork. AgenerativemodellikeaDBNcan\n",
            "beusedinasimilarway,butit’salsopossibletospecifythevaluesofsomeofthefeature\n",
            "neuronsandthen“runthenetworkbackward”,generatingvaluesfortheinputactivations. Moreconcretely,aDBNtrainedonimagesofhandwrittendigitscan(potentially,andwith\n",
            "somecare)alsobeusedtogenerateimagesthatlooklikehandwrittendigits. Inotherwords,\n",
            "theDBNwouldinsomesensebelearningtowrite. Inthis,agenerativemodelismuchlike\n",
            "thehumanbrain: notonlycanitreaddigits,itcanalsowritethem. InGeoffreyHinton’s\n",
            "memorablephrase,torecognizeshapes,firstlearntogenerateimages. A second reason DBNs are interesting is that they can do unsupervised and semi-\n",
            "supervisedlearning. Forinstance,whentrainedwithimagedata,DBNscanlearnuseful\n",
            "featuresforunderstandingotherimages,evenifthetrainingimagesareunlabelled. Andthe\n",
            "abilitytodounsupervisedlearningisextremelyinterestingbothforfundamentalscientific\n",
            "reasons,and–ifitcanbemadetoworkwellenough–forpracticalapplications. Giventheseattractivefeatures,whyhaveDBNslessenedinpopularityasmodelsfor\n",
            "deeplearning? Partofthereasonisthatmodelssuchasfeedforwardandrecurrentnets\n",
            "haveachievedmanyspectacularresults,suchastheirbreakthroughsonimageandspeech\n",
            "recognitionbenchmarks. It’snotsurprisingandquiterightthatthere’snowlotsofattention\n",
            "beingpaidtothesemodels. There’sanunfortunatecorollary,however. Themarketplace\n",
            "ofideasoftenfunctionsinawinner-take-allfashion,withnearlyallattentiongoingtothe\n",
            "currentfashion-of-the-momentinanygivenarea. Itcanbecomeextremelydifficultforpeople\n",
            "toworkonmomentarilyunfashionableideas,evenwhenthoseideasareobviouslyofreal\n",
            "long-terminterest. MypersonalopinionisthatDBNsandothergenerativemodelslikely\n",
            "deservemoreattentionthantheyarecurrentlyreceiving. AndIwon’tbesurprisedifDBNs\n",
            "33SeeAfastlearningalgorithmfordeepbeliefnets,byGeoffreyHinton,SimonOsindero,andYee-Whye\n",
            "Teh(2006),aswellastherelatedworkinReducingthedimensionalityofdatawithneuralnetworks,by\n",
            "GeoffreyHintonandRuslanSalakhutdinov(2006).\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 205\n",
            "(cid:12)\n",
            "orarelatedmodelonedaysurpassthecurrentlyfashionablemodels. Foranintroductionto\n",
            "DBNs,seethisoverview. I’vealsofoundthisarticlehelpful. Itisn’tprimarilyaboutdeep\n",
            "beliefnets,perse,butdoescontainmuchusefulinformationaboutrestrictedBoltzmann\n",
            "machines,whichareakeycomponentofDBNs. Otherideas: Whatelseisgoingoninneuralnetworksanddeeplearning?\n",
            "Well,there’s\n",
            "ahugeamountofotherfascinatingwork. Activeareasofresearchincludeusingneural\n",
            "networkstodonaturallanguageprocessing(seealsothisinformativereviewpaper),machine\n",
            "translation,aswellasperhapsmoresurprisingapplicationssuchasmusicinformatics. There\n",
            "are,ofcourse,manyotherareastoo. Inmanycases,havingreadthisbookyoushouldbeable\n",
            "tobeginfollowingrecentwork,although(ofcourse)you’llneedtofillingapsinpresumed\n",
            "backgroundknowledge.\n",
            "Let me finish this section by mentioning a particularly fun paper. It combines deep\n",
            "convolutionalnetworkswithatechniqueknownasreinforcementlearninginordertolearn\n",
            "to play video games well (see also this followup). The idea is to use the convolutional\n",
            "networktosimplifythepixeldatafromthegamescreen,turningitintoasimplersetof\n",
            "features,whichcanbeusedtodecidewhichactiontotake: “goleft”,“godown”,“fire”,and\n",
            "6\n",
            "soon. Whatisparticularlyinterestingisthatasinglenetworklearnedtoplaysevendifferent\n",
            "classicvideogamesprettywell,outperforminghumanexpertsonthreeofthegames. Now,\n",
            "thisallsoundslikeastunt,andthere’snodoubtthepaperwaswellmarketed,withthe\n",
            "title“PlayingAtariwithreinforcementlearning”.\n",
            "Butlookingpastthesurfacegloss,consider\n",
            "thatthissystemistakingrawpixeldata–itdoesn’tevenknowthegamerules! –andfrom\n",
            "that data learning to do high-quality decision-making in several very different and very\n",
            "adversarialenvironments,eachwithitsowncomplexsetofrules. That’sprettyneat. 6.6 On the future of neural networks\n",
            "Intention-drivenuserinterfaces: There’sanoldjokeinwhichanimpatientprofessortellsa\n",
            "confusedstudent: “don’tlistentowhatIsay;listentowhatImean”. Historically,computers\n",
            "haveoftenbeen,liketheconfusedstudent,inthedarkaboutwhattheirusersmean. Butthis\n",
            "ischanging.IstillremembermysurprisethefirsttimeImisspelledaGooglesearchquery,only\n",
            "tohaveGooglesay“Didyoumean[correctedquery]?” andtoofferthecorrespondingsearch\n",
            "results. GoogleCEOLarryPageoncedescribedtheperfectsearchengineasunderstanding\n",
            "exactlywhat[yourqueries]meanandgivingyoubackexactlywhatyouwant. Thisisavisionofanintention-drivenuserinterface. Inthisvision,insteadofresponding\n",
            "tousers’literalqueries,searchwillusemachinelearningtotakevagueuserinput,discern\n",
            "preciselywhatwasmeant,andtakeactiononthebasisofthoseinsights. Theideaofintention-driveninterfacescanbeappliedfarmorebroadlythansearch. Overthenextfewdecades,thousandsofcompanieswillbuildproductswhichusemachine\n",
            "learningtomakeuserinterfacesthatcantolerateimprecision,whilediscerningandactingon\n",
            "theuser’strueintent. We’realreadyseeingearlyexamplesofsuchintention-driveninterfaces:\n",
            "Apple’sSiri;WolframAlpha;IBM’sWatson;systemswhichcanannotatephotosandvideos;\n",
            "andmuchmore. Mostoftheseproductswillfail. Inspireduserinterfacedesignishard, andIexpect\n",
            "manycompanieswilltakepowerfulmachinelearningtechnologyanduseittobuildinsipid\n",
            "userinterfaces. Thebestmachinelearningintheworldwon’thelpifyouruserinterface\n",
            "conceptstinks. Buttherewillbearesidueofproductswhichsucceed.\n",
            "Overtimethatwill\n",
            "causeaprofoundchangeinhowwerelatetocomputers. Notsolongago–let’ssay,2005\n",
            "\n",
            "(cid:12)\n",
            "206 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "–userstookitforgrantedthattheyneededprecisioninmostinteractionswithcomputers. Indeed,computerliteracytoagreatextentmeantinternalizingtheideathatcomputersare\n",
            "extremelyliteral;asinglemisplacedsemi-colonmaycompletelychangethenatureofan\n",
            "interactionwithacomputer. ButoverthenextfewdecadesIexpectwe’lldevelopmany\n",
            "successfulintention-drivenuserinterfaces,andthatwilldramaticallychangewhatweexpect\n",
            "wheninteractingwithcomputers. Machine learning, data science, and the virtuous circle of innovation: Of course,\n",
            "machinelearningisn’tjustbeingusedtobuildintention-driveninterfaces. Anothernotable\n",
            "applicationisindatascience,wheremachinelearningisusedtofindthe“knownunknowns”\n",
            "hiddenindata. Thisisalreadyafashionablearea,andmuchhasbeenwrittenaboutit,so\n",
            "Iwon’tsaymuch. ButIdowanttomentiononeconsequenceofthisfashionthatisnot\n",
            "so often remarked: over the long run it’s possible the biggest breakthrough in machine\n",
            "learningwon’tbeanysingleconceptualbreakthrough. Rather,thebiggestbreakthroughwill\n",
            "bethatmachinelearningresearchbecomesprofitable,throughapplicationstodatascience\n",
            "andotherareas. Ifacompanycaninvest1dollarinmachinelearningresearchandget1\n",
            "dollarand10centsbackreasonablyrapidly,thenalotofmoneywillendupinmachine\n",
            "6\n",
            "learningresearch. Putanotherway,machinelearningisanenginedrivingthecreationof\n",
            "severalmajornewmarketsandareasofgrowthintechnology. Theresultwillbelargeteams\n",
            "ofpeoplewithdeepsubjectexpertise, andwithaccesstoextraordinaryresources.\n",
            "That\n",
            "willpropelmachinelearningfurtherforward,creatingmoremarketsandopportunities,a\n",
            "virtuouscircleofinnovation. The role of neural networks and deep learning: I’ve been talking broadly about\n",
            "machinelearningasacreatorofnewopportunitiesfortechnology. Whatwillbethespecific\n",
            "roleofneuralnetworksanddeeplearninginallthis?\n",
            "Toanswerthequestion,ithelpstolookathistory. Backinthe1980stherewasagreat\n",
            "dealofexcitementandoptimismaboutneuralnetworks,especiallyafterbackpropagation\n",
            "becamewidelyknown. Thatexcitementfaded,andinthe1990sthemachinelearningbaton\n",
            "passedtoothertechniques,suchassupportvectormachines. Today,neuralnetworksare\n",
            "againridinghigh,settingallsortsofrecords,defeatingallcomersonmanyproblems. But\n",
            "whoistosaythattomorrowsomenewapproachwon’tbedevelopedthatsweepsneural\n",
            "networksawayagain? Orperhapsprogresswithneuralnetworkswillstagnate,andnothing\n",
            "willimmediatelyarisetotaketheirplace? Forthisreason,it’smucheasiertothinkbroadlyaboutthefutureofmachinelearning\n",
            "thanaboutneuralnetworksspecifically. Partoftheproblemisthatweunderstandneural\n",
            "networkssopoorly. Whyisitthatneuralnetworkscangeneralizesowell? Howisitthat\n",
            "theyavoidoverfittingaswellastheydo,giventheverylargenumberofparametersthey\n",
            "learn? Whyisitthatstochasticgradientdescentworksaswellasitdoes? Howwellwill\n",
            "neuralnetworksperformasdatasetsarescaled? Forinstance,ifImageNetwasexpanded\n",
            "byafactorof10,wouldneuralnetworks’performanceimprovemoreorlessthanother\n",
            "machinelearningtechniques? Theseareallsimple,fundamentalquestions.\n",
            "And,atpresent,\n",
            "weunderstandtheanswerstothesequestionsverypoorly. Whilethat’sthecase,it’sdifficult\n",
            "tosaywhatroleneuralnetworkswillplayinthefutureofmachinelearning. Iwillmakeoneprediction: Ibelievedeeplearningisheretostay.\n",
            "Theabilitytolearn\n",
            "hierarchiesofconcepts,buildingupmultiplelayersofabstraction,seemstobefundamental\n",
            "tomakingsenseoftheworld. Thisdoesn’tmeantomorrow’sdeeplearnerswon’tberadically\n",
            "differentthantoday’s. Wecouldseemajorchangesintheconstituentunitsused,inthe\n",
            "architectures,orinthelearningalgorithms. Thosechangesmaybedramaticenoughthatwe\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 207\n",
            "(cid:12)\n",
            "nolongerthinkoftheresultingsystemsasneuralnetworks. Butthey’dstillbedoingdeep\n",
            "learning. Willneuralnetworksanddeeplearningsoonleadtoartificialintelligence? Inthis\n",
            "bookwe’vefocusedonusingneuralnetstodospecifictasks, suchasclassifyingimages. Let’sbroadenourambitions, andask: whataboutgeneral-purposethinkingcomputers? Canneuralnetworksanddeeplearninghelpussolvetheproblemof(general)artificial\n",
            "intelligence(AI)?And,ifso,giventherapidrecentprogressofdeeplearning,canweexpect\n",
            "generalAIanytimesoon? Addressingthesequestionscomprehensivelywouldtakeaseparatebook.\n",
            "Instead,let\n",
            "meofferoneobservation. It’sbasedonanideaknownasConway’slaw:\n",
            "Anyorganizationthatdesignsasystem... willinevitablyproduceadesign\n",
            "whosestructureisacopyoftheorganization’scommunicationstructure. So,forexample,Conway’slawsuggeststhatthedesignofaBoeing747aircraftwillmirror\n",
            "theextendedorganizationalstructureofBoeinganditscontractorsatthetimethe747was\n",
            "designed. Orforasimple,specificexample,consideracompanybuildingacomplexsoftware\n",
            "application. Iftheapplication’sdashboardissupposedtobeintegratedwithsomemachine 6\n",
            "learningalgorithm,thepersonbuildingthedashboardbetterbetalkingtothecompany’s\n",
            "machinelearningexpert. Conway’slawismerelythatobservation,writlarge. UponfirsthearingConway’slaw, manypeoplerespondeither“Well, isn’tthatbanal\n",
            "andobvious?” or“Isn’tthatwrong?” Letmestartwiththeobjectionthatit’swrong. Asan\n",
            "instanceofthisobjection,considerthequestion: wheredoesBoeing’saccountingdepartment\n",
            "showupinthedesignofthe747?\n",
            "Whatabouttheirjanitorialdepartment?\n",
            "Theirinternal\n",
            "catering? Andtheansweristhatthesepartsoftheorganizationprobablydon’tshowup\n",
            "explicitlyanywhereinthe747. SoweshouldunderstandConway’slawasreferringonlyto\n",
            "thosepartsofanorganizationconcernedexplicitlywithdesignandengineering. Whatabouttheotherobjection,thatConway’slawisbanalandobvious? Thismayper-\n",
            "hapsbetrue,butIdon’tthinkso,fororganizationstoooftenactwithdisregardforConway’s\n",
            "law. Teamsbuildingnewproductsareoftenbloatedwithlegacyhiresor,contrariwise,lacka\n",
            "personwithsomecrucialexpertise. Thinkofalltheproductswhichhaveuselesscomplicating\n",
            "features. Orthinkofalltheproductswhichhaveobviousmajordeficiencies–e.g.,aterrible\n",
            "userinterface. Problemsinbothclassesareoftencausedbyamismatchbetweentheteam\n",
            "that was needed to produce a good product, and the team that was actually assembled. Conway’slawmaybeobvious,butthatdoesn’tmeanpeopledon’troutinelyignoreit. Conway’slawappliestothedesignandengineeringofsystemswherewestartoutwith\n",
            "aprettygoodunderstandingofthelikelyconstituentparts,andhowtobuildthem. Itcan’t\n",
            "beapplieddirectlytothedevelopmentofartificialintelligence,becauseAIisn’t(yet)sucha\n",
            "problem: wedon’tknowwhattheconstituentpartsare. Indeed,we’renotevensurewhat\n",
            "basicquestionstobeasking. Inotherswords,atthispointAIismoreaproblemofscience\n",
            "thanofengineering. Imaginebeginningthedesignofthe747withoutknowingaboutjet\n",
            "enginesortheprinciplesofaerodynamics. Youwouldn’tknowwhatkindsofexpertstohire\n",
            "intoyourorganization. AsWernhervonBraunputit,“basicresearchiswhatI’mdoingwhen\n",
            "Idon’tknowwhatI’mdoing”. IsthereaversionofConway’slawthatappliestoproblems\n",
            "whicharemoresciencethanengineering? Togaininsightintothisquestion,considerthehistoryofmedicine. Intheearlydays,\n",
            "medicine was the domain of practitioners like Galen and Hippocrates, who studied the\n",
            "entirebody. Butasourknowledgegrew,peoplewereforcedtospecialize.\n",
            "Wediscovered\n",
            "\n",
            "(cid:12)\n",
            "208 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "manydeepnewideas34: thinkofthingslikethegermtheoryofdisease,forinstance,orthe\n",
            "understandingofhowantibodieswork,ortheunderstandingthattheheart,lungs,veins\n",
            "andarteriesformacompletecardiovascularsystem. Suchdeepinsightsformedthebasisfor\n",
            "subfieldssuchasepidemiology,immunology,andtheclusterofinter-linkedfieldsaroundthe\n",
            "cardiovascularsystem. Andsothestructureofourknowledgehasshapedthesocialstructure\n",
            "ofmedicine. Thisisparticularlystrikinginthecaseofimmunology: realizingtheimmune\n",
            "systemexistsandisasystemworthyofstudyisanextremelynon-trivialinsight. Sowehave\n",
            "anentirefieldofmedicine–withspecialists,conferences,evenprizes,andsoon–organized\n",
            "aroundsomethingwhichisnotjustinvisible,it’sarguablynotadistinctthingatall. Thisisacommonpatternthathasbeenrepeatedinmanywell-establishedsciences:\n",
            "not just medicine, but physics, mathematics, chemistry, and others. The fields start out\n",
            "monolithic,withjustafewdeepideas.\n",
            "Earlyexpertscanmasterallthoseideas.\n",
            "Butastime\n",
            "passesthatmonolithiccharacterchanges. Wediscovermanydeepnewideas,toomanyfor\n",
            "anyonepersontoreallymaster. Asaresult,thesocialstructureofthefieldre-organizes\n",
            "anddividesaroundthoseideas. Insteadofamonolith,wehavefieldswithinfieldswithin\n",
            "fields,acomplex,recursive,self-referentialsocialstructure,whoseorganizationmirrorsthe\n",
            "6\n",
            "connectionsbetweenourdeepestinsights. Andsothestructureofourknowledgeshapesthe\n",
            "socialorganizationofscience.\n",
            "Butthatsocialshapeinturnconstrainsandhelpsdetermine\n",
            "whatwecandiscover. ThisisthescientificanalogueofConway’slaw. Sowhat’sthisgottodowithdeeplearningorAI? Well,sincetheearlydaysofAItherehavebeenargumentsaboutitthatgo,ononeside,\n",
            "“Hey,it’snotgoingtobesohard,we’vegot[super-specialweapon]onourside”,counteredby\n",
            "“[super-specialweapon]won’tbeenough”. Deeplearningisthelatestsuper-specialweapon\n",
            "I’veheardusedinsucharguments35;earlierversionsoftheargumentusedlogic,orProlog,\n",
            "orexpertsystems,orwhateverthemostpowerfultechniqueofthedaywas. Theproblem\n",
            "withsuchargumentsisthattheydon’tgiveyouanygoodwayofsayingjusthowpowerful\n",
            "anygivencandidatesuper-specialweaponis. Ofcourse,we’vejustspentachapterreviewing\n",
            "evidencethatdeeplearningcansolveextremelychallengingproblems. Itcertainlylooksvery\n",
            "excitingandpromising. ButthatwasalsotrueofsystemslikePrologorEuriskoorexpert\n",
            "systemsintheirday. Andsothemerefactthatasetofideaslooksverypromisingdoesn’t\n",
            "meanmuch. Howcanwetellifdeeplearningistrulydifferentfromtheseearlierideas? Is\n",
            "theresomewayofmeasuringhowpowerfulandpromisingasetofideasis? Conway’slaw\n",
            "suggeststhatasaroughandheuristicproxymetricwecanevaluatethecomplexityofthe\n",
            "socialstructureassociatedtothoseideas. So,therearetwoquestionstoask. First,howpowerfulasetofideasareassociatedto\n",
            "deeplearning,accordingtothismetricofsocialcomplexity? Second,howpowerfulatheory\n",
            "willweneed,inordertobeabletobuildageneralartificialintelligence? Astothefirstquestion: whenwelookatdeeplearningtoday,it’sanexcitingandfast-\n",
            "paced but also relatively monolithic field. There are a few deep ideas, and a few main\n",
            "conferences,withsubstantialoverlapbetweenseveraloftheconferences. Andthereispaper\n",
            "afterpaperleveragingthesamebasicsetofideas: usingstochasticgradientdescent(ora\n",
            "closevariation)tooptimizeacostfunction. It’sfantasticthoseideasaresosuccessful. But\n",
            "34Myapologiesforoverloading“deep”.Iwon’tdefine“deepideas”precisely,butlooselyImeanthe\n",
            "kindofideawhichisthebasisforarichfieldofenquiry.Thebackpropagationalgorithmandthegerm\n",
            "theoryofdiseasearebothgoodexamples. 35Interestingly,oftennotbyleadingexpertsindeeplearning,whohavebeenquiterestrained.See,\n",
            "forexample,thisthoughtfulpostbyYannLeCun.Thisisadifferencefrommanyearlierincarnationsof\n",
            "theargument.\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 209\n",
            "(cid:12)\n",
            "whatwedon’tyetseeislotsofwell-developedsubfields,eachexploringtheirownsetsof\n",
            "deepideas,pushingdeeplearninginmanydirections. Andso,accordingtothemetricof\n",
            "socialcomplexity,deeplearningis,ifyou’llforgivetheplayonwords,stillarathershallow\n",
            "field. It’sstillpossibleforonepersontomastermostofthedeepestideasinthefield. On the second question: how complex and powerful a set of ideas will be needed\n",
            "to obtain AI? Of course, the answer to this question is: no-one knows for sure.\n",
            "But in\n",
            "theappendixIexaminesomeoftheexistingevidenceonthisquestion. Iconcludethat,\n",
            "evenratheroptimistically,it’sgoingtotakemany,manydeepideastobuildanAI.Andso\n",
            "Conway’slawsuggeststhattogettosuchapointwewillnecessarilyseetheemergence\n",
            "ofmanyinterrelatingdisciplines, withacomplexandsurprisingstructuremirroringthe\n",
            "structureinourdeepestinsights. Wedon’tyetseethisrichsocialstructureintheuseof\n",
            "neuralnetworksanddeeplearning. Andso,Ibelievethatweareseveraldecades(atleast)\n",
            "fromusingdeeplearningtodevelopgeneralAI. I’vegonetoalotoftroubletoconstructanargumentwhichistentative,perhapsseems\n",
            "ratherobvious,andwhichhasanindefiniteconclusion. Thiswillnodoubtfrustratepeople\n",
            "who crave certainty. Reading around online, I see many people who loudly assert very\n",
            "definite,verystronglyheldopinionsaboutAI,oftenonthebasisofflimsyreasoningand\n",
            "non-existentevidence. Myfrankopinionisthis: it’stooearlytosay. Astheoldjokegoes,if\n",
            "youaskascientisthowfarawaysomediscoveryisandtheysay“10years”(ormore),what\n",
            "theymeanis“I’vegotnoidea”. AI,likecontrolledfusionandafewothertechnologies,has\n",
            "been10yearsawayfor60plusyears. Ontheflipside,whatwedefinitelydohaveindeep\n",
            "learningisapowerfultechniquewhoselimitshavenotyetbeenfound,andmanywide-open\n",
            "fundamentalproblems. That’sanexcitingcreativeopportunity.\n",
            "\n",
            "(cid:12)\n",
            "210 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 211\n",
            "(cid:12)\n",
            "AAAA\n",
            "Is there a simple algorithm for\n",
            "intelligence? A\n",
            "Inthisbook,we’vefocusedonthenutsandboltsofneuralnetworks: howtheywork,and\n",
            "howtheycanbeusedtosolvepatternrecognitionproblems.\n",
            "Thisismaterialwithmany\n",
            "immediatepracticalapplications. But,ofcourse,onereasonforinterestinneuralnetsisthe\n",
            "hopethatonedaytheywillgofarbeyondsuchbasicpatternrecognitionproblems. Perhaps\n",
            "they,orsomeotherapproachbasedondigitalcomputers,willeventuallybeusedtobuild\n",
            "thinkingmachines,machinesthatmatchorsurpasshumanintelligence? Thisnotionfar\n",
            "exceedsthematerialdiscussedinthebook–orwhatanyoneintheworldknowshowtodo. Butit’sfuntospeculate. Therehasbeenmuchdebateaboutwhetherit’sevenpossibleforcomputerstomatch\n",
            "humanintelligence. I’mnotgoingtoengagewiththatquestion. Despiteongoingdispute,I\n",
            "believeit’snotinseriousdoubtthatanintelligentcomputerispossible–althoughitmaybe\n",
            "extremelycomplicated,andperhapsfarbeyondcurrenttechnology–andcurrentnaysayers\n",
            "willonedayseemmuchlikethevitalists. Rather,thequestionIexplorehereiswhetherthereisasimplesetofprincipleswhich\n",
            "canbeusedtoexplainintelligence? Inparticular,andmoreconcretely,isthereasimple\n",
            "algorithmforintelligence? Theideathatthereisatrulysimplealgorithmforintelligenceisaboldidea.\n",
            "Itperhaps\n",
            "soundstoooptimistictobetrue. Manypeoplehaveastrongintuitivesensethatintelligence\n",
            "hasconsiderableirreduciblecomplexity. They’resoimpressedbytheamazingvarietyand\n",
            "flexibilityofhumanthoughtthattheyconcludethatasimplealgorithmforintelligencemust\n",
            "beimpossible. Despitethisintuition,Idon’tthinkit’swisetorushtojudgement. Thehistory\n",
            "ofscienceisfilledwithinstanceswhereaphenomenoninitiallyappearedextremelycomplex,\n",
            "butwaslaterexplainedbysomesimplebutpowerfulsetofideas. Consider,forexample,theearlydaysofastronomy. Humanshaveknownsinceancient\n",
            "timesthatthereisamenagerieofobjectsinthesky: thesun,themoon,theplanets,the\n",
            "comets,andthestars. Theseobjectsbehaveinverydifferentways–starsmoveinastately,\n",
            "\n",
            "(cid:12)\n",
            "212 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A regularwayacrossthesky,forexample,whilecometsappearasifoutofnowhere,streak\n",
            "acrossthesky,andthendisappear.\n",
            "Inthe16thcenturyonlyafoolishoptimistcouldhave\n",
            "imaginedthatalltheseobjects’motionscouldbeexplainedbyasimplesetofprinciples. Butinthe17thcenturyNewtonformulatedhistheoryofuniversalgravitation,whichnot\n",
            "onlyexplainedallthesemotions,butalsoexplainedterrestrialphenomenasuchasthetides\n",
            "andthebehaviourofEarth-boundprojecticles. The16thcentury’sfoolishoptimistseemsin\n",
            "retrospectlikeapessimist,askingfortoolittle.\n",
            "Ofcourse,sciencecontainsmanymoresuchexamples. Considerthemyriadchemical\n",
            "substancesmakingupourworld,sobeautifullyexplainedbyMendeleev’speriodictable,\n",
            "whichis,inturn,explainedbyafewsimpleruleswhichmaybeobtainedfromquantum\n",
            "mechanics. Orthepuzzleofhowthereissomuchcomplexityanddiversityinthebiological\n",
            "world,whoseoriginturnsouttolieintheprincipleofevolutionbynaturalselection. These\n",
            "andmanyotherexamplessuggestthatitwouldnotbewisetoruleoutasimpleexplanation\n",
            "ofintelligencemerelyonthegroundsthatwhatourbrains–currentlythebestexamplesof\n",
            "intelligence–aredoingappearstobeverycomplicated1. Contrariwise, anddespitetheseoptimisticexamples, itisalsologicallypossiblethat\n",
            "intelligencecanonlybeexplainedbyalargenumberoffundamentallydistinctmechanisms. Inthecaseofourbrains,thosemanymechanismsmayperhapshaveevolvedinresponse\n",
            "tomanydifferentselectionpressuresinourspecies’evolutionaryhistory. Ifthispointof\n",
            "viewiscorrect,thenintelligenceinvolvesconsiderableirreduciblecomplexity,andnosimple\n",
            "algorithmforintelligenceispossible. Whichofthesetwopointsofviewiscorrect? Togetinsightintothisquestion,let’saskacloselyrelatedquestion,whichiswhether\n",
            "there’sasimpleexplanationofhowhumanbrainswork. Inparticular,let’slookatsomeways\n",
            "ofquantifyingthecomplexityofthebrain. Ourfirstapproachistheviewofthebrainfrom\n",
            "connectomics. Thisisallabouttherawwiring: howmanyneuronsthereareinthebrain,\n",
            "howmanyglialcells,andhowmanyconnectionstherearebetweentheneurons. You’ve\n",
            "probablyheardthenumbersbefore–thebraincontainsontheorderof100billionneurons,\n",
            "100billionglialcells,and100trillionconnectionsbetweenneurons. Thosenumbersare\n",
            "staggering. They’re also intimidating. If we need to understand the details of all those\n",
            "connections(nottomentiontheneuronsandglialcells)inordertounderstandhowthe\n",
            "brainworks,thenwe’recertainlynotgoingtoendupwithasimplealgorithmforintelligence. There’sasecond,moreoptimisticpointofview,theviewofthebrainfrommolecular\n",
            "biology. Theideaistoaskhowmuchgeneticinformationisneededtodescribethebrain’s\n",
            "architecture.Togetahandleonthisquestion,we’llstartbyconsideringthegeneticdifferences\n",
            "betweenhumansandchimpanzees. You’veprobablyheardthesoundbitethat“humanbeings\n",
            "are98percentchimpanzee”. Thissayingissometimesvaried–popularvariationsalsogive\n",
            "thenumberas95or99percent. Thevariationsoccurbecausethenumberswereoriginally\n",
            "estimatedbycomparingsamplesofthehumanandchimpgenomes,nottheentiregenomes. However,in2007theentirechimpanzeegenomewassequenced(seealsohere),andwe\n",
            "nowknowthathumanandchimpDNAdifferatroughly125millionDNAbasepairs. That’s\n",
            "outofatotalofroughly3billionDNAbasepairsineachgenome.\n",
            "Soit’snotrighttosay\n",
            "humanbeingsare98percentchimpanzee–we’remorelike96percentchimpanzee. 1ThroughthisappendixIassumethatforacomputertobeconsideredintelligentitscapabilitiesmust\n",
            "matchorexceedhumanthinkingability.AndsoI’llregardthequestion“Isthereasimplealgorithmfor\n",
            "intelligence?”asequivalentto“Isthereasimplealgorithmwhichcan‘think’alongessentiallythesame\n",
            "linesasthehumanbrain?”It’sworthnoting,however,thattheremaywellbeformsofintelligencethat\n",
            "don’tsubsumehumanthought,butnonethelessgobeyonditininterestingways. \n",
            "(cid:12)\n",
            "(cid:12) 213\n",
            "(cid:12)\n",
            "Howmuchinformationisinthat125millionbasepairs? Eachbasepaircanbelabelled A\n",
            "byoneoffourpossibilities–the“letters”ofthegeneticcode,thebasesadenine,cytosine,\n",
            "guanine,andthymine. Soeachbasepaircanbedescribedusingtwobitsofinformation\n",
            "–justenoughinformationtospecifyoneofthefourlabels.\n",
            "So125millionbasepairsis\n",
            "equivalentto250millionbitsofinformation. That’sthegeneticdifferencebetweenhumans\n",
            "andchimps! Ofcourse,that250millionbitsaccountsforallthegeneticdifferencesbetweenhumans\n",
            "andchimps. We’reonlyinterestedinthedifferenceassociatedtothebrain. Unfortunately,\n",
            "no-oneknowswhatfractionofthetotalgeneticdifferenceisneededtoexplainthedifference\n",
            "between the brains. But let’s assume for the sake of argument that about half that 250\n",
            "millionbitsaccountsforthebraindifferences. That’satotalof125millionbits.\n",
            "125millionbitsisanimpressivelylargenumber. Let’sgetasenseforhowlargeitis\n",
            "bytranslatingitintomorehumanterms. Inparticular,howmuchwouldbeanequivalent\n",
            "amountofEnglishtext? ItturnsoutthattheinformationcontentofEnglishtextisabout\n",
            "1 bit per letter. That sounds low – after all, the alphabet has 26 letters – but there is a\n",
            "tremendousamountofredundancyinEnglishtext. Ofcourse,youmightarguethatour\n",
            "genomesareredundant,too,sotwobitsperbasepairisanoverestimate. Butwe’llignore\n",
            "that,sinceatworstitmeansthatwe’reoverestimatingourbrain’sgeneticcomplexity. With\n",
            "theseassumptions,weseethatthegeneticdifferencebetweenourbrainsandchimpbrains\n",
            "isequivalenttoabout125millionletters,orabout25millionEnglishwords. That’sabout\n",
            "30timesasmuchastheKingJamesBible. That’salotofinformation. Butit’snotincomprehensiblylarge.\n",
            "It’sonahumanscale. Maybenosinglehumancouldeverunderstandallthat’swritteninthatcode,butagroup\n",
            "ofpeoplecouldperhapsunderstanditcollectively,throughappropriatespecialization. And\n",
            "althoughit’salotofinformation,it’sminusculewhencomparedtotheinformationrequired\n",
            "todescribethe100billionneurons,100billionglialcells,and100trillionconnectionsin\n",
            "ourbrains. Evenifweuseasimple,coarsedescription–say,10floatingpointnumbersto\n",
            "characterizeeachconnection–thatwouldrequireabout70quadrillionbits. Thatmeansthe\n",
            "geneticdescriptionisafactorofabouthalfabillionlesscomplexthanthefullconnectome\n",
            "forthehumanbrain. Whatwelearnfromthisisthatourgenomecannotpossiblycontainadetaileddescription\n",
            "ofallourneuralconnections. Rather,itmustspecifyjustthebroadarchitectureandbasic\n",
            "principlesunderlyingthebrain. Butthatarchitectureandthoseprinciplesseemtobeenough\n",
            "toguaranteethatwehumanswillgrowuptobeintelligent. Ofcourse,therearecaveats\n",
            "–growingchildrenneedahealthy,stimulatingenvironmentandgoodnutritiontoachieve\n",
            "theirintellectualpotential. Butprovidedwegrowupinareasonableenvironment,ahealthy\n",
            "human will have remarkable intelligence. In some sense, the information in our genes\n",
            "containstheessenceofhowwethink. Andfurthermore,theprinciplescontainedinthat\n",
            "geneticinformationseemlikelytobewithinourabilitytocollectivelygrasp. Allthenumbersaboveareveryroughestimates. It’spossiblethat125millionbitsis\n",
            "atremendousoverestimate,thatthereissomemuchmorecompactsetofcoreprinciples\n",
            "underlyinghumanthought.Maybemostofthat125millionbitsisjustfine-tuningofrelatively\n",
            "minordetails. Ormaybewewereoverlyconservativeinhowwecomputedthenumbers.\n",
            "Obviously,that’dbegreatifitweretrue! Forourcurrentpurposes,thekeypointisthis: the\n",
            "architectureofthebrainiscomplicated,butit’snotnearlyascomplicatedasyoumightthink\n",
            "basedonthenumberofconnectionsinthebrain. Theviewofthebrainfrommolecular\n",
            "biologysuggestswehumansoughttoonedaybeabletounderstandthebasicprinciples\n",
            "\n",
            "(cid:12)\n",
            "214 (cid:12) Isthereasimplealgorithmforintelligence?\n",
            "(cid:12)\n",
            "A behindthebrain’sarchitecture. InthelastfewparagraphsI’veignoredthefactthat125millionbitsmerelyquantifies\n",
            "the genetic difference between human and chimp brains. Not all our brain function is\n",
            "duetothose125millionbits. Chimpsareremarkablethinkersintheirownright. Maybe\n",
            "the key to intelligence lies mostly in the mental abilities (and genetic information) that\n",
            "chimpsandhumanshaveincommon. Ifthisiscorrect,thenhumanbrainsmightbejusta\n",
            "minorupgradetochimpanzeebrains,atleastintermsofthecomplexityoftheunderlying\n",
            "principles.\n",
            "Despitetheconventionalhumanchauvinismaboutouruniquecapabilities,this\n",
            "isn’tinconceivable: thechimpanzeeandhumangeneticlinesdivergedjust5millionyears\n",
            "ago, a blink in evolutionary timescales. However, in the absence of a more compelling\n",
            "argument,I’msympathetictotheconventionalhumanchauvinism: myguessisthatthe\n",
            "mostinterestingprinciplesunderlyinghumanthoughtlieinthat125millionbits,notinthe\n",
            "partofthegenomewesharewithchimpanzees. Adoptingtheviewofthebrainfrommolecularbiologygaveusareductionofroughly\n",
            "nineordersofmagnitudeinthecomplexityofourdescription. Whileencouraging,itdoesn’t\n",
            "telluswhetherornotatrulysimplealgorithmforintelligenceispossible.\n",
            "Canwegetany\n",
            "furtherreductionsincomplexity? And, moretothepoint, canwesettlethequestionof\n",
            "whetherasimplealgorithmforintelligenceispossible? Unfortunately,thereisn’tyetanyevidencestrongenoughtodecisivelysettlethisques-\n",
            "tion. Letmedescribesomeoftheavailableevidence, withthecaveatthatthisisavery\n",
            "briefandincompleteoverview,meanttoconveytheflavourofsomerecentwork,notto\n",
            "comprehensivelysurveywhatisknown. Amongtheevidencesuggestingthattheremaybeasimplealgorithmforintelligence\n",
            "isanexperimentreportedinApril2000inthejournalNature. Ateamofscientistsledby\n",
            "MrigankaSur“rewired”thebrainsofnewbornferrets. Usually,thesignalfromaferret’s\n",
            "eyesistransmittedtoapartofthebrainknownasthevisualcortex. Butfortheseferrets\n",
            "thescientiststookthesignalfromtheeyesandrerouteditsoitinsteadwenttotheauditory\n",
            "cortex,i.e,thebrainregionthat’susuallyusedforhearing. Tounderstandwhathappenedwhentheydidthis,weneedtoknowabitaboutthevisual\n",
            "cortex. Thevisualcortexcontainsmanyorientationcolumns. Thesearelittleslabsofneurons,\n",
            "eachofwhichrespondstovisualstimulifromsomeparticulardirection. Youcanthinkofthe\n",
            "orientationcolumnsastinydirectionalsensors: whensomeoneshinesabrightlightfrom\n",
            "someparticulardirection,acorrespondingorientationcolumnisactivated. Ifthelightis\n",
            "moved,adifferentorientationcolumnisactivated. Oneofthemostimportanthigh-level\n",
            "structures in the visual cortex is the orientation map, which charts how the orientation\n",
            "columnsarelaidout. Whatthescientistsfoundisthatwhenthevisualsignalfromtheferrets’eyeswasrerouted\n",
            "totheauditorycortex,theauditorycortexchanged. Orientationcolumnsandanorientation\n",
            "mapbegantoemergeintheauditorycortex. Itwasmoredisorderlythantheorientationmap\n",
            "usuallyfoundinthevisualcortex,butunmistakablysimilar. Furthermore,thescientistsdid\n",
            "somesimpletestsofhowtheferretsrespondedtovisualstimuli,trainingthemtorespond\n",
            "differently when lights flashed from different directions. These tests suggested that the\n",
            "ferretscouldstilllearnto“see”,atleastinarudimentaryfashion,usingtheauditorycortex. Thisisanastonishingresult.\n",
            "Itsuggeststhattherearecommonprinciplesunderlying\n",
            "howdifferentpartsofthebrainlearntorespondtosensorydata. Thatcommonalitypro-\n",
            "videsatleastsomesupportfortheideathatthereisasetofsimpleprinciplesunderlying\n",
            "intelligence. However,weshouldn’tkidourselvesabouthowgoodtheferrets’visionwasin\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 215\n",
            "(cid:12)\n",
            "theseexperiments. Thebehaviouralteststestedonlyverygrossaspectsofvision. And,of A\n",
            "course,wecan’tasktheferretsifthey’ve“learnedtosee”. Sotheexperimentsdon’tprove\n",
            "thattherewiredauditorycortexwasgivingtheferretsahigh-fidelityvisualexperience. And\n",
            "sotheyprovideonlylimitedevidenceinfavouroftheideathatcommonprinciplesunderlie\n",
            "howdifferentpartsofthebrainlearn. Whatevidenceisthereagainsttheideaofasimplealgorithmforintelligence? Some\n",
            "evidencecomesfromthefieldsofevolutionarypsychologyandneuroanatomy. Sincethe\n",
            "1960sevolutionarypsychologistshavediscoveredawiderangeofhumanuniversals,complex\n",
            "behaviourscommontoallhumans,acrossculturesandupbringing. Thesehumanuniversals\n",
            "include the incest taboo between mother and son, the use of music and dance, as well\n",
            "asmuchcomplexlinguisticstructure,suchastheuseofswearwords(i.e.,taboowords),\n",
            "pronouns,andevenstructuresasbasicastheverb. Complementingtheseresults,agreat\n",
            "dealofevidencefromneuroanatomyshowsthatmanyhumanbehavioursarecontrolled\n",
            "byparticularlocalizedareasofthebrain,andthoseareasseemtobesimilarinallpeople.\n",
            "Takentogether,thesefindingssuggestthatmanyveryspecializedbehavioursarehardwired\n",
            "intoparticularpartsofourbrains. Somepeopleconcludefromtheseresultsthatseparateexplanationsmustberequiredfor\n",
            "thesemanybrainfunctions,andthatasaconsequencethereisanirreduciblecomplexityto\n",
            "thebrain’sfunction,acomplexitythatmakesasimpleexplanationforthebrain’soperation\n",
            "(and, perhaps, a simple algorithm for intelligence) impossible. For example, one well-\n",
            "knownartificialintelligenceresearcherwiththispointofviewisMarvinMinsky. Inthe\n",
            "1970sand1980sMinskydevelopedhis“SocietyofMind”theory,basedontheideathat\n",
            "humanintelligenceistheresultofalargesocietyofindividuallysimple(butverydifferent)\n",
            "computationalprocesseswhichMinskycallsagents. Inhisbookdescribingthetheory,Minsky\n",
            "sumsupwhatheseesasthepowerofthispointofview:\n",
            "Whatmagicaltrickmakesusintelligent? Thetrickisthatthereisnotrick.\n",
            "Thepower\n",
            "ofintelligencestemsfromourvastdiversity,notfromanysingle,perfectprinciple. Ina\n",
            "response 2 to reviews of his book, Minsky elaborated on the motivation for the Society\n",
            "of Mind, giving an argument similar to that stated above, based on neuroanatomy and\n",
            "evolutionarypsychology:\n",
            "Wenowknowthatthebrainitselfiscomposedofhundredsofdifferent\n",
            "regionsandnuclei,eachwithsignificantlydifferentarchitecturalelements\n",
            "andarrangements,andthatmanyofthemareinvolvedwithdemonstrably\n",
            "differentaspectsofourmentalactivities. Thismodernmassofknowledge\n",
            "showsthatmanyphenomenatraditionallydescribedbycommonsense\n",
            "terms like “intelligence” or “understanding” actually involve complex\n",
            "assembliesofmachinery. Minsky is, of course, not the only person to hold a point of view along these lines; I’m\n",
            "merelygivinghimasanexampleofasupporterofthislineofargument. Ifindtheargument\n",
            "interesting, but don’t believe the evidence is compelling. While it’s true that the brain\n",
            "iscomposedofalargenumberofdifferentregions, withdifferentfunctions, itdoesnot\n",
            "thereforefollowthatasimpleexplanationforthebrain’sfunctionisimpossible. Perhaps\n",
            "those architectural differences arise out of common underlying principles, much as the\n",
            "motionofcomets,theplanets,thesunandthestarsallarisefromasinglegravitationalforce. NeitherMinskynoranyoneelsehasarguedconvincinglyagainstsuchunderlyingprinciples.\n",
            "2InContemplatingMinds:AForumforArtificialIntelligence,editedbyWilliamJ.Clancey,Stephen\n",
            "W.Smoliar,andMarkStefik(MITPress,1994). \n",
            "(cid:12)\n",
            "216 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A Myownprejudiceisinfavouroftherebeingasimplealgorithmforintelligence. And\n",
            "themainreasonIliketheidea,aboveandbeyondthe(inconclusive)argumentsabove,is\n",
            "thatit’sanoptimisticidea. Whenitcomestoresearch,anunjustifiedoptimismisoftenmore\n",
            "productivethanaseeminglybetterjustifiedpessimism,foranoptimisthasthecourageto\n",
            "setoutandtrynewthings. That’sthepathtodiscovery,evenifwhatisdiscoveredisperhaps\n",
            "notwhatwasoriginallyhoped. Apessimistmaybemore“correct”insomenarrowsense,\n",
            "butwilldiscoverlessthantheoptimist. Thispointofviewisinstarkcontrasttothewayweusuallyjudgeideas: byattempting\n",
            "tofigureoutwhethertheyarerightorwrong. That’sasensiblestrategyfordealingwith\n",
            "theroutineminutiaeofday-to-dayresearch. Butitcanbethewrongwayofjudgingabig,\n",
            "boldidea,thesortofideathatdefinesanentireresearchprogram. Sometimes,wehaveonly\n",
            "weakevidenceaboutwhethersuchanideaiscorrectornot. Wecanmeeklyrefusetofollow\n",
            "theidea,insteadspendingallourtimesquintingattheavailableevidence,tryingtodiscern\n",
            "what’strue. Orwecanacceptthatno-oneyetknows,andinsteadworkhardondeveloping\n",
            "thebig,boldidea,intheunderstandingthatwhilewehavenoguaranteeofsuccess,itis\n",
            "onlythusthatourunderstandingadvances. Withallthatsaid,initsmostoptimisticform,Idon’tbelievewe’lleverfindasimple\n",
            "algorithmforintelligence. Tobemoreconcrete,Idon’tbelievewe’lleverfindareallyshort\n",
            "Python(orCorLisp,orwhatever)program–let’ssay,anywhereuptoathousandlines\n",
            "ofcode–whichimplementsartificialintelligence. NordoIthinkwe’lleverfindareally\n",
            "easily-describedneuralnetworkthatcanimplementartificialintelligence.\n",
            "ButIdobelieve\n",
            "it’sworthactingasthoughwecouldfindsuchaprogramornetwork. That’sthepathto\n",
            "insight,andbypursuingthatpathwemayonedayunderstandenoughtowritealonger\n",
            "programorbuildamoresophisticatednetworkwhichdoesexhibitintelligence. Andsoit’s\n",
            "worthactingasthoughanextremelysimplealgorithmforintelligenceexists. Inthe1980s,theeminentmathematicianandcomputerscientistJackSchwartzwas\n",
            "invited to a debate between artificial intelligence proponents and artificial intelligence\n",
            "skeptics. Thedebatebecameunruly,withtheproponentsmakingover-the-topclaimsabout\n",
            "theamazingthingsjustroundthecorner,andtheskepticsdoublingdownontheirpessimism,\n",
            "claimingartificialintelligencewasoutrightimpossible.\n",
            "Schwartzwasanoutsidertothe\n",
            "debate,andremainedsilentasthediscussionheatedup. Duringalull,hewasaskedto\n",
            "speakupandstatehisthoughtsontheissuesunderdiscussion. Hesaid: “Well,someof\n",
            "thesedevelopmentsmaylieonehundredNobelprizesaway”(ref,page22). Itseemstome\n",
            "aperfectresponse.\n",
            "Thekeytoartificialintelligenceissimple,powerfulideas,andwecan\n",
            "andshouldsearchoptimisticallyforthoseideas. Butwe’regoingtoneedmanysuchideas,\n",
            "andwe’vestillgotalongwaytogo! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "# result = qa(\"What is the cost function?\")\n",
        "# print(\"Answer:\", result[\"result\"])\n",
        "\n",
        "chain.invoke({\"user_input\": \"What is the cost function?\"})\n"
      ],
      "metadata": {
        "id": "TyHM4ujyGw7a",
        "outputId": "a55f1f8e-72f0-4756-f0ff-949de0038c62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_input': 'What is the cost function?',\n",
              " 'text': \" The cost function, also known as the loss function or objective function, is a fundamental concept in optimization and machine learning. It quantifies the discrepancy between the predictions made by a model and the actual (ground truth) values. The primary goal of training a model is to minimize this cost function, thereby improving the model's accuracy and performance.\\n\\n### **Key Aspects of the Cost Function:**\\n1. **Purpose:**\\n   - The cost function measures how well a model performs by evaluating the difference between predicted and actual outcomes.\\n   - It serves as a guide for optimization algorithms (e.g., gradient descent) to adjust model parameters (weights and biases) to reduce errors.\\n\\n2. **Types of Cost Functions:**\\n   - **Mean Squared Error (MSE):** Commonly used in regression problems. It calculates the average squared difference between predicted and actual values.\\n     \\\\[\\n     \\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2\\n     \\\\]\\n   - **Cross-Entropy Loss:** Used in classification tasks, especially for multi-class problems. It measures the difference between predicted probabilities and true labels.\\n     \\\\[\\n     \\\\text{Cross-Entropy} = -\\\\sum_{i=1}^{n} y_i \\\\log(\\\\hat{y}_i)\\n     \\\\]\\n   - **Hinge Loss:** Used in support vector machines (SVMs) for classification, emphasizing correct classification with a margin of separation.\\n   - **Log Loss (Logarithmic Loss):** A probabilistic measure of classification accuracy, penalizing incorrect predictions more heavily.\\n\\n3. **Role in Model Training:**\\n   - During training, the cost function is evaluated after each iteration of the optimization process.\\n   - Optimization algorithms (e.g., gradient descent) compute the gradient of the cost function with respect to model parameters and update them to minimize the loss.\\n\\n4. **Regularization:**\\n   - Sometimes, additional terms (e.g., L1/L2 regularization) are added to the cost function to prevent overfitting by penalizing overly complex models.\\n\\n5. **Applications Beyond Machine Learning:**\\n   - In economics, cost functions model production costs.\\n   - In physics, they represent energy or potential functions in optimization problems.\\n\\n### **Example: Linear Regression**\\nFor a simple linear regression model \\\\( \\\\hat{y} = w x + b \\\\), the cost function (MSE) becomes:\\n\\\\[\\nJ(w, b) = \\\\frac{1}{2n} \\\\sum_{i=1}^{n} (y_i - (w x_i + b))^2\\n\\\\]\\nHere, the goal is to find optimal values of \\\\( w \\\\) and \\\\( b \\\\) that minimize \\\\( J(w, b) \\\\).\\n\\n### **Conclusion**\\nThe cost function is the cornerstone of model training, shaping how well a model learns from data. Choosing the right cost function for a given problem (e.g., regression vs. classification) is crucial for achieving accurate and generalized predictions.\\n\\nWould you like a deeper explanation of a specific cost function or its implementation in a particular algorithm?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}